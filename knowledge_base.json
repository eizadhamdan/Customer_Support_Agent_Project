[
  {
    "text": "atlan atlan documentation skip to main content discover, trust, and govern your data ai ecosystem everything you need to get started with atlan. set up snowflake set up databricks set up power bi atlan architecture browser extension get started quick start guide step by step onboarding secure agent enterprise grade deployment options playbooks automation rule based metadata updates at scale core features find understand data search, discover, and profile assets govern manage create data contracts policies integrate automation, collaboration other integrations developer hub introductory walkthrough play with apis in minutes client sdks java, python more packages developer built utilities and integrations atlan university get started with atlan by building the right strategy and setting a strong foundation. atlan security a comprehensive look at atlan s security philosophy, core values, and rigorous security procedures help and support find answers or contact our team for personalized assistance",
    "source": "https://docs.atlan.com/"
  },
  {
    "text": "submit support request atlan documentation skip to main content submit request aim to include as much information and detail in your request as possible to reduce delays between replies. name email subject atlan url the url of your atlan tenant severity sev0 (s0) system down or critical issue sev1 (s1) major functionality affected sev2 (s2) general question or minor issue sev3 (s3) feature request or enhancement refer to severity and response sla guidelines as outlined here how is this impacting you? i m unable to use the product a major feature stopped working an issue is slowing me down i have a non urgent question i have a suggestion that will help me with my use case i need help with something description attachments add file or drop files here submit",
    "source": "https://docs.atlan.com/support/submit-request"
  },
  {
    "text": "what is atlan? atlan documentation skip to main content on this page we are a modern data workspace that makes collaboration among diverse users like business, analysts, and engineers easier, increasing efficiency and agility in data projects. we started out as a data team, solving social good problems using data science. we built atlan for ourselves over the course of 200 data projects, which included india s national data platform used by the prime minister and monitoring the sustainable development goals with the united nations. atlan helped us build india s national data platform with an 8 member team, making it the fastest project of its kind to go live in just 12 months instead of the projected three years. why we built atlan data teams can be diverse: analysts, scientists, engineers, and business users. diverse people with diverse tools and skillsets mean diverse dnas. all of it led to",
    "source": "https://docs.atlan.com/get-started/what-is-atlan"
  },
  {
    "text": "chaos, which made our slack channels look like this... we call this collaboration overheard we knew we couldn t scale like this, there had to be a better way. we borrowed the principles of agile from product teams, devops from engineering teams, and lean manufacturing from supply chain teams. we then experimented for two years and across 200 data projects to create our own idea of what makes data teams successful. we call this dataops . how atlan helps data teams with atlan, analyst teams at unilever have shipped 100 additional data projects per quarter, while data science teams at samsung have saved 50 of their time. create self service ecosystems by reducing dependencies atlan makes all your data assets easily discoverable. no more slack messages like where s that dataset? or long email threads for approvals. with atlan, you can simply cmd k your way to the right data",
    "source": "https://docs.atlan.com/get-started/what-is-atlan"
  },
  {
    "text": "asset. key capabilities : discovery and search , visual query builder , saved queries , readmes improve the agility of your data team data practitioners spend 30 50 of their time finding and understanding data. atlan cuts that time by 95 . your data team will be shipping 2 3 times more projects in no time. key capabilities : visibility of data quality tests and observability alerts, automated lineage , atlan ai promote governance and a sustainable data culture don t lose sleep trying to figure out if your sensitive data is secure. build ecosystems of trust, make your team happy, and let atlan manage governance and security behind the scenes. key capabilities : tag sensitive data , granular access control , data products why we built atlan how atlan helps data teams",
    "source": "https://docs.atlan.com/get-started/what-is-atlan"
  },
  {
    "text": "set up snowflake atlan documentation skip to main content on this page who can do this? you need your snowflake administrator to run these commands you may not have access yourself. create user and role in snowflake create a role and user in snowflake using the following commands: create role create a role in snowflake using the following commands: create or replace role atlan user role ; grant operate , usage on warehouse to role atlan user role ; replace with the default warehouse to use when running the snowflake crawler . atlan requires the following privileges to: operate enables atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. usage enables atlan to show or list metadata from snowflake. this in turn enables the snowflake crawler to run the show query. create a user create a separate user to integrate into atlan, using one of",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "the following 3 options: with a public key in snowflake see snowflake s official guide for details on generating an rsa key pair . to create a user with a key pair, replace the value for rsa public key with the public key and run the following: create user atlan user rsa public key miibijanbgkqh... default role atlan user role default warehouse display name atlan type service learn more about the service type property in snowflake documentation . did you know? atlan only supports encrypted private keys with a non empty passphrase generally recommended as more secure. an empty passphrase results in workflow failures. to generate an encrypted private key, omit the nocrypt option. refer to snowflake documentation to learn more. with a password in snowflake did you know? snowflake recommends transitioning away from basic authentication using username and password. change to key pair authentication for enhanced security. for any",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "existing snowflake workflows, you can modify the crawler configuration to update the authentication method. to create a user with a password, replace and run the following: create user atlan user password default role atlan user role default warehouse display name atlan type legacy service learn more about the legacy service type property in snowflake documentation . managed through your identity provider (idp) private preview this method is currently only available if okta is your idp (snowflake supports) authenticating natively through okta : create a user in your identity provider (idp) and use federated authentication in snowflake . the password for this user must be maintained solely in the idp and multi factor authentication (mfa) must be disabled. grant role to user to grant the atlan user role to the new user: grant role atlan user role to user atlan user ; configure oauth (client credentials flow) with microsoft entra id",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "to configure oauth authentication using microsoft entra id (formerly azure ad) with the client credentials flow: follow snowflake s documentation to: register a new application in microsoft entra id collect the client id , tenant id , and client secret add the required api permissions in snowflake, create a security integration using the following: create security integration external oauth azure ad type external oauth enabled true external oauth type azure external oauth issuer external oauth jws keys url external oauth audience list ( ) external oauth token user mapping claim sub external oauth snowflake user mapping attribute login name ; replace the placeholders with actual values from your azure ad app: your tenant s oauth 2.0 issuer url azure jwks uri application id uri of the azure app create a snowflake user with a login name that exactly matches the azure ad client object id: create user oauth svc user",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "with login name use azure client object id default role default warehouse ; grant the configured role to this user: grant role to user oauth svc user ; choose metadata fetching method atlan supports two methods for fetching metadata from snowflake account usage and information schema. you should choose one of these two methods to set up snowflake: account usage information schema overview simplified grants but some limitations in functionality most comprehensive approach, more grant management required method views in the snowflake database that display object metadata and usage metrics for your account system defined views and table functions that provide extensive metadata for objects created in your account permissions user role and account, single grant for snowflake database user role and account, multiple grants per database data latency 45 minutes to 3 hours (varies by view) none historical data retention 1 year 7 days to 6 months (varies by",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "view or table function) asset extraction account usage schema information schema schema view lineage account usage schema information schema schema table lineage account usage schema account usage schema tag import account usage schema account usage schema usage and popularity account usage schema account usage schema metadata extraction time varies by warehouse size. for example, 8 minutes for 10 million assets (recommended for extracting a large number of assets) varies by warehouse size. for example, 2 hours for 10 million assets extraction limitations external table location data, procedures, and primary and foreign keys none grant permissions for account usage method danger if you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. once the workflow has succeeded, adjust the value to be twice the extraction time. this method uses the views in snowflake.account usage (or a copied version of this schema)",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "to fetch the metadata from snowflake into atlan. you can be more granular with permissions using this method, but there are limitations with this approach . to crawl assets, generate lineage, and import tags if you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. snowflake stores all tag objects in the account usage schema. if you re using the account usage method to crawl metadata in atlan or you have configured the snowflake miner , you need to grant the same permissions to import tags as required for crawling snowflake assets. note that object tagging in snowflake currently requires enterprise edition or higher . to use the default snowflake database and account usage schema and also mine snowflake s query history (for lineage), grant these permissions: use role accountadmin ; grant imported privileges on database snowflake",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "to role atlan user role ; the accountadmin role is required to grant privileges on the snowflake database due to the following reasons: by default, only the accountadmin role can access the snowflake database. to enable other roles to access the database and schemas and query the views, a user with the accountadmin role needs to grant imported privileges on the snowflake database to the desired roles. to use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: grant usage on database to role atlan user role ; grant usage on schema in database to role atlan user role ; grant references on all views in database to role atlan user role ; replace with the copied snowflake database name. replace with the copied snowflake account usage schema name. the grants for the copied version can",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "t be used on the original snowflake database. this is because snowflake produces an error that granular grants can t be given to imported databases. when using a cloned or copied version, verify that the table or view definition remains unchanged as in your snowflake database. if the format is different. for example, a column is missing and it no longer qualifies as a clone. to crawl streams to crawl streams, provide the following permissions: to crawl current streams: grant usage on all schemas in database to role atlan user role ; grant references on all tables in database to role atlan user role ; grant select on all streams in database to role atlan user role ; replace with the snowflake database name. to crawl future streams: grant usage on future schemas in database to role atlan user role ; grant references on future tables in database to role",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "atlan user role ; grant select on future streams in database to role atlan user role ; replace with the snowflake database name. (optional) to preview and query existing assets to query and preview data within assets that already exist in snowflake, add these permissions: grant usage on database to role atlan user role ; grant usage on all schemas in database to role atlan user role ; grant select on all tables in database to role atlan user role ; grant select on all external tables in database to role atlan user role ; grant select on all views in database to role atlan user role ; grant select on all materialized views in database to role atlan user role ; grant select on all streams in database to role atlan user role ; grant monitor on pipe to role atlan user role ; replace with the database you",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "want to be able to preview and query in atlan. (repeat the statements for every database you wish to preview and query in atlan.) (optional) to preview and query future assets to query and preview data within assets that may be created in the future in snowflake, add these permissions. again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. grant usage on future schemas in database to role atlan user role ; grant select on future tables in database to role atlan user role ; grant select on future external tables in database to role atlan user role ; grant select on future views in database to role atlan user role ; grant select on future materialized views in database to role atlan user role ; grant select on future streams",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "in database to role atlan user role ; grant monitor on future pipes in database to role atlan user role ; replace with the database you want to be able to preview and query in atlan. (repeat the statements for every database you want to preview and query in atlan.) danger verify that all the assets you d like to crawl are present in these grants by checking the grants on the user role defined for the crawler. grant permissions for information schema method this method uses views in the information schema schema in snowflake databases to fetch metadata. you still need to grant specific permissions to enable atlan to crawl metadata, preview data, and query data with this method. to crawl existing assets grant these permissions to crawl assets that already exist in snowflake. if you also want to be able to preview and query the data, you can",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "use the preview and query existing assets permissions instead. grant permissions to crawl existing assets: grant usage on database to role atlan user role ; grant usage on all schemas in database to role atlan user role ; grant references on all tables in database to role atlan user role ; grant references on all external tables in database to role atlan user role ; grant references on all views in database to role atlan user role ; grant references on all materialized views in database to role atlan user role ; grant select on all streams in database to role atlan user role ; grant monitor on pipe to role atlan user role ; replace with the database you want to be available in atlan. (repeat the statements for every database you wish to integrate into atlan.) grant permissions to crawl functions: grant usage on all functions in database",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "to role atlan user role ; replace with the database you want to be available in atlan. (repeat the statements for every database you wish to integrate into atlan.) for secure user defined functions (udfs), grant ownership permissions to retrieve metadata: grant ownership on function . to role ; replace the placeholders with the appropriate values: : the name of the schema that contains the user defined function (udf). : the name of the secure udf that requires ownership permissions. : the role that gets assigned ownership of the secure udf. did you know? the statements given on this page apply to all schemas, tables, and views in a database in snowflake. if you want to limit access to only certain objects, you can instead specify the exact objects individually as well. to crawl future assets to crawl assets that may be created in the future in snowflake, add these",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "permissions. again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. to grant permissions at a database level: grant usage on future schemas in database to role atlan user role ; grant references on future tables in database to role atlan user role ; grant references on future external tables in database to role atlan user role ; grant references on future views in database to role atlan user role ; grant references on future materialized views in database to role atlan user role ; grant select on future streams in database to role atlan user role ; grant monitor on future pipes in database to role atlan user role ; grant usage on future functions in database to role atlan user role ; replace with the database you want to crawl",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "in atlan. (repeat the statements for every database you want to integrate into atlan.) danger for any future grants defined at a schema level to a different role, the schema level grants take precedence over the database level grants and any future grants defined at a database level to the atlan role get ignored. to learn more, refer to snowflake documentation . to grant permissions at a schema level: grant references on future tables in schema . to role atlan user role ; grant references on future external tables in schema . to role atlan user role ; grant references on future views in schema . to role atlan user role ; grant references on future materialized views in schema . to role atlan user role ; grant select on future streams in schema . to role atlan user role ; grant monitor on future pipes in schema . to",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "role atlan user role ; replace with the database and with the schema you want to crawl in atlan. (repeat the statements for every database and schema you want to integrate into atlan.) to mine query history for lineage to also mine snowflake s query history (for lineage), add these permissions. you can use either option: to mine query history direct from snowflake s internal tables: use role accountadmin ; grant imported privileges on database snowflake to role atlan user role ; to mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: grant usage on database to role atlan user role ; grant usage on schema . to role atlan user role ; grant select on all tables in schema . to role atlan user role ; grant select on all views in schema . to role atlan user role",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "; replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. when using a cloned or copied version, verify that the table or view definition remains unchanged as in your snowflake database. if the format is different. for example, a column is missing and it no longer qualifies as a clone. (optional) to preview and query existing assets to query and preview data within assets that already exist in snowflake, add these permissions: grant usage on database to role atlan user role ; grant usage on all schemas in database to role atlan user role ; grant select on all tables in database to role atlan user role ; grant select on all external tables in database to role atlan user role ; grant select on all views in database to role atlan user role ; grant select on all",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "materialized views in database to role atlan user role ; grant select on all streams in database to role atlan user role ; grant monitor on pipe to role atlan user role ; replace with the database you want to be able to preview and query in atlan. (repeat the statements for every database you wish to preview and query in atlan.) (optional) to preview and query future assets to query and preview data within assets that may be created in the future in snowflake, add these permissions. again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. grant usage on future schemas in database to role atlan user role ; grant select on future tables in database to role atlan user role ; grant select on future external tables in database",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "to role atlan user role ; grant select on future views in database to role atlan user role ; grant select on future materialized views in database to role atlan user role ; grant select on future streams in database to role atlan user role ; grant monitor on future pipes in database to role atlan user role ; replace with the database you want to be able to preview and query in atlan. (repeat the statements for every database you want to preview and query in atlan.) danger for any future grants defined at a schema level to a different role, the schema level grants take precedence over the database level grants and any future grants defined at a database level to the atlan role get ignored. to learn more, refer to snowflake documentation . to grant permissions at a schema level: grant select on future tables in schema",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": ". to role atlan user role ; grant select on future external tables in schema . to role atlan user role ; grant select on future views in schema . to role atlan user role ; grant select on future materialized views in schema . to role atlan user role ; grant select on future streams in schema . to role atlan user role ; grant monitor on future pipes in schema . to role atlan user role ; replace with the database and with the schema you want to be able to preview and query in atlan. (repeat the statements for every database and schema you want to preview and query in atlan.) danger verify that all the assets you d like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (optional) to import snowflake tags snowflake stores all",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "tag objects in the account usage schema. note that object tagging in snowflake currently requires enterprise edition or higher . to import tags from snowflake , grant these permissions: to use the default snowflake database and account usage schema and also mine snowflake s query history (for lineage), grant these permissions: use role accountadmin ; grant imported privileges on database snowflake to role atlan user role ; the accountadmin role is required to grant privileges on the snowflake database due to the following reasons: by default, only the accountadmin role can access the snowflake database. to enable other roles to access the database and schemas and query the views, a user with the accountadmin role needs to grant imported privileges on the snowflake database to the desired roles. to use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes,",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "grant these permissions: grant usage on database to role atlan user role ; grant usage on schema in database to role atlan user role ; grant references on all views in database to role atlan user role ; replace with the copied snowflake database name. replace with the copied snowflake account usage schema name. the grants for the copied version can t be used on the original snowflake database. this is because snowflake produces an error that granular grants can t be given to imported databases. (optional) to push updated tags to snowflake to push tags updated for assets in atlan to snowflake , grant these permissions: grant apply tag on account to role ; you can learn more about tag privileges from snowflake documentation . (optional) to crawl dynamic tables atlan currently supports fetching metadata for dynamic tables using the monitor privilege. refer to snowflake documentation to learn more.",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "to crawl existing dynamic tables from snowflake: grant permissions at a database level: grant monitor on all dynamic tables in database to role atlan user role ; grant permissions at a schema level: grant monitor on all dynamic tables in schema . to role atlan user role ; to crawl future dynamic tables from snowflake: grant permissions at a database level: grant monitor on future dynamic tables in database to role atlan user role ; grant permissions at a schema level: grant monitor on future dynamic tables in schema . to role atlan user role ; replace with the database and with the schema you want to crawl in atlan. (repeat the statements for every database and schema you want to integrate into atlan.) (optional) to crawl iceberg tables atlan currently supports fetching metadata for iceberg tables only for the information schema extraction method. to crawl iceberg tables from snowflake,",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "grant the following permissions: to crawl existing iceberg tables in snowflake: grant references on all iceberg tables in database to role atlan user role ; to crawl future iceberg tables in snowflake: grant references on future iceberg tables in database to role atlan user role ; to crawl iceberg catalog metadata for iceberg tables in snowflake: grant usage on integration to role atlan user role ; danger you must first grant permissions to crawl existing iceberg tables for this permission to work on catalogs. you must also grant permissions to all the catalogs you want to crawl in atlan individually. (optional) to crawl snowflake stages atlan supports crawling metadata for snowflake stages using the usage and read privileges. for more information, see the snowflake documentation for information schema.stages. to crawl stages from snowflake: grant usage and read privileges on all existing stages at the database level: grant usage on all",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "stages in database to role atlan user role ; grant read on all stages in database to role atlan user role ; replace with the name of your snowflake database replace with the role you ve granted atlan to use for crawling. grant usage and read privileges on all future stages at the database level: grant usage on future stages in database to role atlan user role ; grant read on future stages in database to role atlan user role ; replace with the name of your snowflake database replace with the role you ve granted atlan to use for crawling. allowlist the atlan ip if you are using the ip allowlist in your snowflake instance, you must add the atlan ip to the allowlist. please raise a support ticket from within atlan, or submit a request . (if you aren t using the ip allowlist in your snowflake instance,",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "you can skip this step.) create user and role in snowflake choose metadata fetching method grant permissions for account usage method grant permissions for information schema method allowlist the atlan ip",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake"
  },
  {
    "text": "set up databricks atlan documentation skip to main content on this page atlan supports three authentication methods for fetching metadata from databricks. you can set up any of the following authentication methods: personal access token authentication aws service principal authentication azure service principal authentication personal access token authentication who can do this? check that you have admin and databricks sql access for the databricks workspace. this is required for both cluster options described below. if you don t have this access, contact your databricks administrator. grant user access to workspace to grant workspace access to the user creating a personal access token: from the left menu of the account console, click workspaces and then select a workspace to which you want to add the user. from the tabs along the top of your workspace page, click the permissions tab. in the upper right of the permissions page, click add permissions",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": ". in the add permissions dialog, enter the following details: for user, group, or service principal , select the user to grant access. for permission , click the dropdown and select workspace user. generate a personal access token you can generate a personal access token in your databricks workspace to the authenticate the integration in atlan . to generate a personal access token: from the top right of your databricks workspace, click your databricks username, and then from the dropdown, click user settings . under the settings menu, click developer . on the developer page, next to access tokens , click manage . on the access tokens page, click the generate new token button. in the generate new token dialog: for comment , enter a description of the token s intended use for example, atlan crawler . for lifetime (days) , consider removing the number. this enables the token to",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "be used indefinitely it won t need to be refreshed. important! if you do enter a number, remember that you need to periodically regenerate it and update atlan s crawler configuration with the new token each time. at the bottom of the dialog, click generate . copy and save the generated token in a secure location, and then click done . select a cluster did you know? atlan recommends using serverless sql warehouses for instant compute availability. to enable serverless sql warehouses, refer to databricks documentation for aws databricks workspaces or microsoft documentation for azure databricks workspaces. you can set up personal access token authentication for your databricks instance using one of the following cluster options: interactive cluster sql warehouse (formerly sql endpoint) interactive cluster to confirm an all purpose interactive cluster is configured: from the left menu of any page of your databricks instance, click compute . under the",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "all purpose clusters tab, verify you have a cluster defined. click the link under the name column of the table to open your cluster. under the configuration tab, verify the autopilot options to terminate after ... minutes is enabled. at the bottom of the configuration tab, expand the advanced options expandable. under the advanced options expandable, open the jdbc odbc tab. confirm that all of the fields in this tab are populated, and copy them for use in crawling: server hostname , port , and http path . sql warehouse (formerly sql endpoint) to confirm a sql warehouse is configured: from the left menu of any page of your databricks instance, open the dropdown just below the databricks logo and change to sql . from the refreshed left menu, click sql warehouses . click the link under the name column of the table to open your sql warehouse. under the",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "connection details tab, confirm that all of the fields are populated and copy them for use in crawling: server hostname , port , and http path . aws service principal authentication who can do this? you need your aws databricks account admin to create a service principal and manage oauth credentials for the service principal and your aws databricks workspace admin to add the service principal to your aws databricks workspace you may not have access yourself. you need the following to authenticate the connection in atlan: client id client secret create a service principal you can create a service principal directly in your databricks account or from a databricks workspace. identity federation enabled on your workspaces: databricks recommends creating the service principal in the account and assigning it to workspaces. identity federation disabled on your workspaces: databricks recommends that you create your service principal from a workspace. identity federation",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "enabled to create a service principal from your databricks account, with identify federation enabled: log in to your databricks account console as an account admin. from the left menu of the account console, click user management . from the tabs along the top of the user management page, click the service principals tab. in the upper right of the service principals page, click add service principal . on the add service principal page, enter a name for the service principal and then click add . once the service principal has been created, you can assign it to your identity federated workspace. from the left menu of the account console, click workspaces and then select a workspace to which you want to add the service principal. from the tabs along the top of your workspace page, click the permissions tab. in the upper right of the permissions page, click add permissions",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": ". in the add permissions dialog, enter the following details: for user, group, or service principal , select the service principal you created. for permission , click the dropdown and select workspace user. identity federation disabled to create a service principal from a databricks workspace, with identity federation disabled: log in to your aws databricks workspace as a workspace admin. from the top right of your workspace, click your username, and then from the dropdown, click admin settings . in the left menu of the settings page, under the workspace admin subheading, click identity and access . on the identity and access page, under management and permissions , next to service principals , click manage . in the upper right of the service principals page, click add service principal . in the add service principal dialog, click the add new button. for new service principal display name , enter a",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "name for the service principal and then click add . create an oauth secret for the service principal you need to create an oauth secret to authenticate to databricks rest apis. to create an oauth secret for the service principal : log in to your databricks account console as an account admin. from the left menu of the account console, click user management . from the tabs along the top of the user management page, click the service principals tab. in the upper right of the service principals page, select the service principal you created . on the service principal page, under oauth secrets , click generate secret . from the generate secret dialog, copy the secret and client id and store it in a secure location. danger note that this secret is only revealed once during creation. the client id is the same as the application id of the",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "service principal. once you ve copied the client id and secret, click done . azure service principal authentication who can do this? you need your azure databricks account admin to create a service principal and your azure databricks workspace admin to add the service principal to your azure databricks workspace you may not have access yourself. you need the following to authenticate the connection in atlan: client id (application id) client secret tenant id (directory id) create a service principal to use service principals on azure databricks , an admin user must create a new microsoft entra id (formerly azure active directory) application and then add it to the azure databricks workspace to use as a service principal. to create a service principal: sign in to the azure portal . if you have access to multiple tenants, subscriptions, or directories, click the directories subscriptions (directory with filter) icon in the",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "top menu to switch to the directory in which you want to create the service principal. in search resources, services, and docs , search for and select microsoft entra id . click add and select app registration . for name , enter a name for the application. in the supported account types section, select accounts in this organizational directory only (single tenant) and then click register . on the application page s overview page, in the essentials section, copy and store the following values in a secure location: application (client) id directory (tenant) id to generate a client secret, within manage , click certificates secrets . on the client secrets tab, click new client secret . in the add a client secret dialog, enter the following details: for description , enter a description for the client secret. for expires , select an expiry time period for the client secret and",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "then click add . copy and store the client secret s value in a secure place. add a service principal to your account to add a service principal to your azure databricks account: log in to your azure databricks account console as an account admin. from the left menu of the account console, click user management . from the tabs along the top of the user management page, click the service principals tab. in the upper right of the service principals page, click add service principal . on the add service principal page, enter a name for the service principal. under uuid , paste the application (client) id for the service principal. click add . assign a service principal to a workspace to add users to a workspace using the account console, the workspace must be enabled for identity federation. workspace admins can also assign service principals to workspaces using",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "the workspace admin settings page. identity federation enabled to assign a service principal to your azure databricks account: log in to your databricks account console as an account admin. from the left menu of the account console, click workspaces and then select a workspace to which you want to add the service principal. from the tabs along the top of your workspace page, click the permissions tab. in the upper right of the permissions page, click add permissions . in the add permissions dialog, enter the following details: for user, group, or service principal , select the service principal you created. for permission , click the dropdown to select workspace user . identity federation disabled to assign a service principal to your azure databricks workspace: log in to your azure databricks workspace as a workspace admin. from the top right of your workspace, click your username, and then from the",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "dropdown, click admin settings . in the left menu of the settings page, under the workspace admin subheading, click identity and access . on the identity and access page, under management and permissions , next to service principals , click manage . in the upper right of the service principals page, click add service principal . in the add service principal dialog, click the add new button. for new service principal display name , paste the application (client) id for the service principal , enter a display name, and then click add . grant permissions to crawl metadata you must have a unity catalog enabled databricks workspace to crawl metadata in atlan. to extract metadata, you can grant the browse privilege , currently in public preview. you no longer require the data reader preset that granted the following privileges on objects in the catalog use catalog , use schema ,",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "execute , read volume , and select . to grant permissions to a user or service principal: log in to your databricks workspace as a workspace admin. from the left menu of your workspace, click catalog . in the left menu of the catalog explorer page, select the catalog you want to crawl in atlan. from the tabs along the top of your workspace page, click the permissions tab and then click the grant button. in the grant on (workspace name) dialog, configure the following: under principals , click the dropdown and then select the user or service principal. under privileges , check the browse privilege. at the bottom of the dialog, click grant . (optional) repeat steps 3 5 for each catalog you want to crawl in atlan. system tables extraction method to crawl metadata via system tables, you must have a unity catalog enabled workspace and a configured",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "sql warehouse. follow these steps to extract metadata using system tables: create one of the following authentication methods: personal access token aws service principal azure service principal grant the following privileges to the identity you created: can use on a sql warehouse use catalog on system catalog use schema on system.information schema select on the following tables: system.information schema.catalogs system.information schema.schemata system.information schema.tables system.information schema.columns system.information schema.key column usage system.information schema.table constraints cross workspace extraction to crawl metadata from all workspaces within a databricks metastore using a single connection, see set up cross workspace extraction for instructions. (optional) grant permissions to query and preview data danger atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. to grant permissions to query data and preview example data: log in to your databricks workspace as a workspace admin. from the left menu of your",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "workspace, click catalog . in the left menu of the catalog explorer page, select the catalog you want to query and preview data from in atlan. from the tabs along the top of your workspace page, click the permissions tab and then click the grant button. in the grant on (workspace name) dialog, configure the following: under principals , click the dropdown and then select the user or service principal. under privilege presets , click the dropdown and then click data reader to enable read only access to the catalog. doing so automatically selects the following privileges use catalog , use schema , execute , read volume , and select . at the bottom of the dialog, click grant . (optional) repeat steps 3 5 for each catalog you want to query and preview data from in atlan. (optional) grant permissions to import and update tags to import databricks tags",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": ", you must have a unity catalog enabled workspace and a sql warehouse configured. atlan supports importing databricks tags using system tables for all three authentication methods. once you have created a personal access token , an aws service principal , or an azure service principal , you will need to grant the following privileges: can use on a sql warehouse use catalog on system catalog use schema on system.information schema select on the following tables: system.information schema.catalog tags system.information schema.schema tags system.information schema.table tags system.information schema.column tags to push tags updated for assets in atlan to databricks, you need to grant the following privileges : apply tag on the object use catalog on the object s parent catalog use schema on the object s parent schema (optional) grant permissions to extract lineage and usage from system tables you must have a unity catalog enabled workspace to use system tables.",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "atlan supports extracting the following for your databricks assets using system tables : lineage usage and popularity metrics enable system.access schema you need your account admin to enable the system.access schema using the systemschemas api . this enables atlan to extract lineage using system tables. in atlan, one databricks connection corresponds to one metastore. repeat the following process for each metastore in your databricks environment for which you want to extract lineage. to verify that system schemas are enabled for each schema, follow the steps in databricks documentation : list system schemas using the systemschemas api to check the status. if enabled for any given schema, the state is enablecompleted . this confirms that the schema has been enabled for that specific metastore. atlan can only extract lineage using system tables when the state is marked as enablecompleted . (optional) enable system.information schema.table to generate lineage with the target type",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "set as path for a table, atlan uses metadata from system.information schema.table to resolve table paths and dependencies. to enable this, you must grant the following permissions on the relevant catalog, schema, and tables. grant permissions who can do this? you must be a metastore admin, have the manage privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. in atlan, one databricks connection corresponds to one metastore. repeat the following process for each metastore from which you want to extract lineage. open catalog explorer in your databricks workspace. navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). click the permissions tab. click grant . enter the user or group name (principal). assign the following permissions: usage on the catalog usage on the schema select on each relevant table click grant to apply",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "the changes. these privileges enable atlan to read table definitions and other metadata from the metastore. (optional) enable system.query schema this is only required if you also want to extract usage and popularity metrics from databricks. you need your account admin to enable the system.query schema using the systemschemas api . this enables atlan to mine query history using system tables for usage and popularity metrics. to verify that system schemas is enabled for each schema, follow the steps in databricks documentation . if enabled for any given schema, the state is enablecompleted . info did you know? can t grant select permissions on the system tables in system.access and system.query ? skip the previous steps and create cloned views in a separate catalog and schema. see create cloned views of system tables . grant permissions atlan supports extracting databricks lineage and usage and popularity metrics using system tables for",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "all three authentication methods . once you have created a personal access token , an aws service principal , or an azure service principal , you will need to grant the following permissions: can use on a sql warehouse use catalog on system catalog use schema on system.access schema use schema on system.query schema (tomine query history for usage and popularity metrics) select on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table lineage system.access.column lineage you need to create a databricks connection in atlan for each metastore. you can use the hostname of your unity catalog enabled workspace as the host for the connection. info did you know? can t grant select permissions on the system tables in system.access and system.query ? skip the previous steps and create cloned views in a separate catalog and schema. see create cloned views of system tables .",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "(optional) create cloned views of system tables when you don t want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. follow these steps to set up cloned views: create a catalog and schema to store cloned views. use meaningful and unique names for example, atlan cloned catalog and atlan cloned schema . create cloned views for the following system tables: lineage tables create or replace view . . column lineage as select from system . access . column lineage ; create or replace view . . table lineage as select from system . access . table lineage ; replace and with the catalog and schema names used in your environment. popularity metrics create or replace view . . query history as select from system . query . history ; replace and with the catalog and schema",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "names used in your environment. grant permissions grant the following permissions to enable access to the cloned views: can use on a sql warehouse use catalog on the catalog (for example, ) use schema and select on the schema (for example, . ) you must create a databricks connection in atlan for each metastore. you can use the hostname of your unity catalog enabled workspace as the host for the connection. locate warehouse id to extract lineage and usage and popularity metrics using system tables, you will also need the warehouse id of your sql warehouse . to locate the warehouse id: log in to your databricks workspace as a workspace admin. from the left menu of your workspace, click sql warehouses . on the compute page, select the warehouse you want to use. from the overview tab of your warehouse page, next to the name of your warehouse, copy",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "the value for your sql warehouse id . for example, example warehouse (id: 123ab4c5def67890) , copy the value 123ab4c5def67890 and store it in a secure location. (optional) grant view permissions to access databricks entities via apis atlan uses databricks rest apis to extract metadata for notebooks, queries, jobs, and pipelines. this information helps to understand which databricks enitity was used to create a lineage between assets. use the steps below for each object type to grant can view permission to the databricks user or service principal configured in your integration: notebook api ( api 2.0 workspace list ): grant can view permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. for more information, see manage access control lists with folders . queries api ( api 2.0 sql queries ): grant can view permission on individual queries, or on the workspace folder containing",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "the queries, or on the entire workspace. for more information, see view queries . job api ( api 2.2 jobs list ): grant can view permission on each job object directly. databricks jobs are distinct from notebooks or files and require permission set directly on the job object. for more information, see control access to a job . pipeline api ( api 2.0 pipelines ): grant can view permission on each delta live tables (dlt) pipeline object directly. for more information, see configure pipeline permissions . (optional) grant permissions for views and materialized views atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: log in to your databricks workspace as a workspace admin. from the left menu of your workspace, click catalog . in the catalog explorer , select the catalog you want to extract view definitions from and generate lineage",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "for in atlan. from the tabs at the top, click the permissions tab, and then click grant . in the grant on (workspace name) dialog, configure the following: select the user or service principal under principals . select the following privileges under privilege presets : use catalog use schema select click grant to apply the permissions. repeat steps 3 6 for each catalog you want to crawl in atlan. did you know? select permission is required to extract the definitions of views and materialized views. if you prefer not to grant select at the catalog level, you can grant it on individual views and materialized views instead. (optional) grant permissions to mine query history to mine query history using rest api, you will need to assign the can manage permission on your sql warehouses to the user or service principal. to grant permissions to mine query history: log in to",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "your databricks workspace as a workspace admin. from the left menu of your workspace, click sql warehouses . on the compute page, for each sql warehouse you want to mine query history, click the 3 dot icon and then click permissions . in the manage permissions dialog, configure the following: in the type to add multiple users or groups field, search for and select a user or service principal. expand the can use permissions dropdown and then select can manage . this permission enables the service principal to view all queries for the warehouse . click add to assign the can manage permission to the service principal. personal access token authentication aws service principal authentication azure service principal authentication grant permissions to crawl metadata (optional) grant permissions to query and preview data (optional) grant permissions to import and update tags (optional) grant permissions to extract lineage and usage from system",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "tables (optional) grant view permissions to access databricks entities via apis (optional) grant permissions for views and materialized views (optional) grant permissions to mine query history",
    "source": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks"
  },
  {
    "text": "set up microsoft power bi atlan documentation skip to main content on this page who can do this? depending on the authentication method you choose, you may need a combination of your cloud application administrator or application administrator for microsoft entra id, microsoft 365 administrator for microsoft 365, and fabric administrator ( formerly known as power bi administrator ) for microsoft power bi to complete these tasks you may not have access yourself. this guide outlines how to set up microsoft power bi so it can connect with atlan for metadata extraction and lineage tracking. before you begin register application in microsoft entra id who can do this? you need your cloud application administrator or application administrator to complete these steps you may not have access yourself. this is required if the creation of registered applications isn t enabled for the entire organization. to register a new application in microsoft",
    "source": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi"
  },
  {
    "text": "entra id: log in to the azure portal . search for microsoft entra id and select it. click app registrations from the left menu. click new registration . enter a name for your client application and click register . from the overview screen, copy and securely store: application (client) id directory (tenant) id click certificates secrets from the left menu. under client secrets , click new client secret . enter a description, select an expiry time, and click add . copy and securely store the client secret value . create security group in microsoft entra id who can do this? you need your cloud application administrator or application administrator to complete these steps you may not have access yourself. to create a security group for your application: log in to the azure portal . search for microsoft entra id and select it. click groups under the manage section. click new",
    "source": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi"
  },
  {
    "text": "group . set the group type to security . enter a group name and optional description. click no members selected . add the appropriate member: for delegated user authentication : search for the user and select it. for service principal authentication : search for the application registration created earlier and select it. click select and then create . by the end of these steps, you have registered an application with microsoft entra id and created a security group with the appropriate member. configure authentication options atlan supports two authentication methods for fetching metadata from microsoft power bi: service principal authentication (recommended) when using service principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. there are two supported options: admin api only this option grants permissions that let the service principal to access only admin level power bi apis. in this mode, atlan",
    "source": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi"
  },
  {
    "text": "extracts metadata exclusively using administrative endpoints. this option is recommended for stricter access control environments. who can do this? you need your fabric administrator ( formerly known as power bi administrator ) to complete these tasks you may not have access yourself. to configure admin api access: log in to the power bi admin portal . click tenant settings under admin portal. under admin api settings : expand enable service principals to use read only power bi admin apis and set to enabled add your security group under specific security groups click apply expand enhance admin apis responses with detailed metadata and set to enabled add your security group click apply expand enhance admin apis responses with dax and mashup expressions and set to enabled add your security group click apply admin and non admin apis this option grants permissions that let the service principal to access both admin and",
    "source": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi"
  },
  {
    "text": "non admin power bi apis. this enables atlan to extract richer metadata and build detailed lineage across power bi assets. assign security group to power bi workspaces in powerbi service portal who can do this? you need to be at least a member of the microsoft power bi workspace to which you want to add the security group to complete these steps you may not have access yourself. make sure that you add the security group from the homepage and not the admin portal. to assign a microsoft power bi workspace role to the security group: open the microsoft power bi homepage . open workspaces and select the workspace you want to access from atlan. click access . in the panel: enter the name of your security group where it says enter email addresses choose one of the following roles: viewer : for workspaces without parameters contributor : for workspaces",
    "source": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi"
  },
  {
    "text": "with semantic models containing parameters or to generate lineage for measures member : to generate lineage for dataflows click add . configure admin and non admin api access in powerbi service portal who can do this? you need your fabric administrator ( formerly known as power bi administrator ) to complete these tasks you may not have access yourself. to enable both admin and non admin api access: log in to the power bi admin portal . click tenant settings under admin portal. under developer settings : expand service principals can use fabric apis and set to enabled add your security group under specific security groups click apply under admin api settings : expand enable service principals to use read only power bi admin apis and set to enabled add your security group click apply expand enhance admin apis responses with detailed metadata and set to enabled add your security",
    "source": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi"
  },
  {
    "text": "group click apply expand enhance admin apis responses with dax and mashup expressions and set to enabled add your security group click apply after making these changes, you typically need to wait 15 30 minutes for the settings to take effect across microsoft s services. delegated user authentication info atlan doesn t recommend using delegated user authentication as it s also not recommended by microsoft. fabric administrator role assignment who can do this? you need your microsoft 365 administrator to complete these steps you may not have access yourself. to assign the delegated user to the fabric administrator role: open the microsoft 365 admin portal . click users and then active users from the left menu. select the delegated user. under roles , click manage roles . expand show all by category . under collaboration , select fabric administrator . click save changes . api permissions who can do this?",
    "source": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi"
  },
  {
    "text": "you need your cloud application administrator or application administrator to complete these steps, you may not have access yourself. danger the following permissions are only required for delegated user authentication. if using service principal authentication, you don t need to configure any delegated permissions for a service principal it s recommended that you avoid adding these permissions. these are never used and can cause errors that may be hard to troubleshoot. to add permissions for the registered application : in your app registration, click api permissions under the manage section. click add a permission . search for and select power bi service . click delegated permissions and select: capacity.read.all dataflow.read.all dataset.read.all report.read.all tenant.read.all workspace.read.all click grant admin consent (if you only see the add permissions button, you aren t an administrator). admin api settings configuration who can do this? you need your fabric administrator ( formerly known as power bi",
    "source": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi"
  },
  {
    "text": "administrator ) to complete these tasks, you may not have access yourself. to enable the microsoft power bi admin api: log in to the power bi admin portal . click tenant settings under admin portal. under admin api settings : expand enhance admin apis responses with detailed metadata and set to enabled add your security group click apply expand enhance admin apis responses with dax and mashup expressions and set to enabled add your security group click apply . before you begin configure authentication options",
    "source": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi"
  },
  {
    "text": "atlan architecture atlan documentation skip to main content on this page atlan is a cloud first solution. single tenant saas is the recommended deployment model. atlan currently supports hosting tenants on the following cloud platforms: amazon web services (aws) microsoft azure google cloud platform (gcp) the components of atlan are isolated, across both compute and data. for more details, see how are resources isolated? platform components kong is an api gateway. it handles rate limiting and token verification on all incoming api requests. apache keycloak is an identity and access management component. it manages everything to do with users, login, sso and so on. heracles is atlan s api service. it houses the business logic used by the frontend and apis to interact with other platform components. postgresql is a sql database. many services on the platform use it for storage. hashicorp vault is a secret manager. it stores sensitive",
    "source": "https://docs.atlan.com/platform/references/atlan-architecture"
  },
  {
    "text": "credentials provided by the user. apache ranger is the policy engine. it provides fine grained access control over data in the metastore. argo workflows is a workflow orchestrator for k8s. it runs and manages long running jobs in a container and k8s native fashion. admission controller is a k8s admission controller. it performs certain actions when argo workflows are updated such as workflow alerts. metastore stores metadata as data in a graph store. it is based on apache atlas and has fine grained access control on top. apache zookeeper manages consensus and coordination for the metastore services. elasticsearch indexes data and drives search functionality. apache cassandra is an object oriented database used to store the metastore s data. apache kafka is an event stream. it enables event driven use cases across the platform. heka is atlan s sql component. it parses, rewrites and optimizes sql queries and is powered by",
    "source": "https://docs.atlan.com/platform/references/atlan-architecture"
  },
  {
    "text": "apache calcite . redis is a cache layer used by heracles. platform management components velero performs cluster backups. kibana explores and filters log data stored in elasticsearch. fluent bit is a logging and metrics processor. it parses and pushes logs from pods to various destinations. elasticsearch stores and indexes logs. central components zenduty is used for incident response. alerts are sent when something goes wrong in one of the clusters. argo cd is used for continuous deployment. changes in git repositories lead to upgrades in the clusters. github actions update the docker container images as part of the development process. sendgrid is used to send emails. the frontend is a vue.js web application that s hosted on s3 and delivered via amazon cloudfront content delivery network (cdn) service. alertmanager sends alerts generated by metrics stored in prometheus. grafana provides observability dashboards. victoriametrics is a fast, cost effective, and scalable monitoring",
    "source": "https://docs.atlan.com/platform/references/atlan-architecture"
  },
  {
    "text": "solution and time series database. it processes high volumes of data and enables long term storing. atlan marketplace (not pictured) the marketplace offers packages (workflows) that perform long running tasks on the atlan platform. the ecosystem enables the creation of metadata and lineage connectors. see security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. platform components platform management components central components atlan marketplace (not pictured)",
    "source": "https://docs.atlan.com/platform/references/atlan-architecture"
  },
  {
    "text": "use the atlan browser extension atlan documentation skip to main content on this page the atlan browser extension provides metadata context directly in your supported data tools . you can use the extension in the following chromium based browsers: google chrome and microsoft edge. install the extension to install the atlan browser extension, first log into your atlan instance. atlan saves your atlan domain in a cookie when you log in. to install atlan s browser extension: you can either: find the extension in the chrome web store: https: chrome.google.com webstore detail atlan fipjfjlalpnbejlmmpfnmlkadjgaaheg from the upper right of any screen in atlan, navigate to your name and then click profile . click the four dots icon in the resulting dialog to get to integrations. under apps , for browser extension , click install . to install the atlan browser extension: for google chrome, in the upper right of your",
    "source": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/use-the-atlan-browser-extension"
  },
  {
    "text": "screen, click add to chrome . when prompted for confirmation, click the add extension button. for microsoft edge, follow the steps in add an extension to microsoft edge from the chrome web store . currently, you can t install the browser extension on mobile devices or tablets. did you know? you can also install atlan s browser extension at the workspace level . to set this up, you need to be an administrator or have access to the admin console of your organization s google account. if your organization uses managed browsers, you can configure the extension for managed browsers . configure the extension once installed, configure the atlan browser extension to get started. optionally, atlan admins can preconfigure custom domains for data sources , if any. configure the extension as a user to configure the browser extension, once installed: if you are logged into your atlan instance, skip to",
    "source": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/use-the-atlan-browser-extension"
  },
  {
    "text": "the next step. if you haven t logged into atlan, log in to your atlan instance when prompted. in the options page, to enter the url of your atlan instance: if your organization uses an atlan domain (for example, mycompany .atlan.com ), the atlan instance url appears preselected. click get started . (optional) switch to a different atlan domain, if required. if your organization uses a custom domain (for example, atlan .mycompany.com ), enter the url of your atlan instance and then click get started . after a successful login, the message updated successfully appears. (optional) if your data tools are hosted on custom domains, rather than the standard saas domain of each tool: click the configure custom domain link at the bottom. in the dropdown on the left, select your data tool. in the text box on the right, enter the custom domain you use for that tool. repeat",
    "source": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/use-the-atlan-browser-extension"
  },
  {
    "text": "these steps for each tool hosted on a custom domain. click the save button when finished. if your atlan admin has preconfigured custom domains for data sources , you won t be able to update or remove these selections. click add to configure custom domains for additional data sources as required. you can now close the options tab. the extension is now ready to use! (optional) configure custom domains as an admin who can do this? you need to be an admin user in atlan to configure custom domains for data sources from the admin center. to configure custom domains, from within atlan: from the left menu of any screen, click admin . under workspace , click integrations . under apps , expand the browser extension tile. in the browser extension tile, for set up your custom data source... , if your data tools are hosted on custom domains rather",
    "source": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/use-the-atlan-browser-extension"
  },
  {
    "text": "than the standard saas domain of each tool, click the configure link to configure them for users in your organization. for connector , select a supported tool for the browser extension. in the adjacent field, enter the url of the custom domain for your data source. (optional) click add to add more. click save to save your configuration. info did you know? for any supported tools that you have configured, your users won t be able to update or remove these selections. they can, however, add additional custom domains for data sources. (optional) for download atlan extension or share with your team , you can either install the atlan browser extension for your own use or share the link with your users. usage who can do this? anyone with access to atlan any admin, member, or guest user and a supported tool can use the browser extension. first, log into",
    "source": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/use-the-atlan-browser-extension"
  },
  {
    "text": "atlan. did you know? when using atlan s browser extension in a supported tool , the extension only reads the url of your browser tab no other data is accessed. if using atlan s browser extension on any website , it only reads the favicon, page title, and url of your browser tab. learn more about atlan browser extension security . access and enrich context in flow to access context for an asset, from within a supported tool: log into the supported tool. open any supported asset. in the lower right corner of the page, click the small atlan icon. danger the icon to activate atlan is not the extension icon that appears at the top of your chrome browser. this small atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. in",
    "source": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/use-the-atlan-browser-extension"
  },
  {
    "text": "the sidebar that appears: click the tabs and links to view all context about the asset. make changes to any of the metadata you d like. now you can understand and enrich assets without leaving your data tools themselves! the atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you re currently viewing. did you know? your permissions in atlan control what metadata you can see and change in the extension. search for metadata to search for context for any information on any website: select the text you d like to search on the web page you re viewing. right click, and then select search in atlan . the extension opens a new browser tab on atlan s discovery page, with the results for that text! add a resource you can link any web page as a resource to your",
    "source": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/use-the-atlan-browser-extension"
  },
  {
    "text": "assets in atlan using the browser extension. to add a web page as a resource to an asset: in the top right of the web page you re viewing, click the atlan chrome extension . in the resource clipper menu, under link this page to an asset , select the asset to which you d like to add the web page as a resource. click save to confirm your selection. (optional) once the resource has been linked successfully, click the open in atlan button to view the linked asset directly in atlan. you can now add resources to your assets in atlan from any website! did you know? the tableau extension offers native embeddings directly in your dashboards. see enable embedded metadata in tableau for more information. supported tools currently, the atlan browser extension supports assets in the following tools: amazon quicksight : analyses, dashboards, and datasets databricks : databases,",
    "source": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/use-the-atlan-browser-extension"
  },
  {
    "text": "schemas, views, and tables dbt cloud : models and sources in the model editor and dbt docs google bigquery : datasets, schemas, views, and tables ibm cognos analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules looker : dashboards, explores, and folders microsoft power bi : dashboards, reports, dataflows, and datasets mode : collections, reports, queries, and charts qlik sense cloud : apps, datasets, sheets, and spaces redash : queries, dashboards, and visualizations salesforce : objects sigma : datasets, pages, and data elements snowflake (via snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes tableau : dashboards, data sources, workbooks, and metrics. additionally, you can choose to switch the tableau extension to offer native embeddings directly in your dashboards. see enable embedded metadata in tableau for more information. thoughtspot : liveboards, answers, visualizations, and tables microstrategy : dossiers, reports, documents install the extension configure",
    "source": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/use-the-atlan-browser-extension"
  },
  {
    "text": "administrators atlan documentation skip to main content on this page user management user management is a critical part of data governance. atlan s user management capabilities should be a mainstay of how you organize and control access for people in your organization. add and manage users from the admin center it s super simple to invite and remove users from atlan from the admin center . you can also manage existing users by adding them to groups, changing their roles, or set up sso , scim , and smtp configurations. manage access control from the governance center the governance center is where you can build access control mechanisms to manage user access . personas allow you to group users into teams, such as financial analysts or cloud engineers , and set policies based on the access those personas should have. purposes are where you can build policies based on the",
    "source": "https://docs.atlan.com/get-started/how-tos/quick-start-for-admins"
  },
  {
    "text": "actions or access that a user might need. for example, you can use atlan s policy based access controls to manage access to pii and other sensitive data. this is a best practice for data governance. once you set these policies, atlan will enforce them throughout your users experience. this means that users who don t have access to a particular type of data will not be able to see it. governance workflows help you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out of the box workflow templates and automated execution. asset profile the asset profile in atlan gives you a quick and clear understanding of what a data asset contains. you can think of the asset profile as the tl;dr about your data. glossary the atlan glossary is a rich tool for defining and organizing your data terminology to improve",
    "source": "https://docs.atlan.com/get-started/how-tos/quick-start-for-admins"
  },
  {
    "text": "transparency and share knowledge. no need to ask around for what a column name means. the glossary functions as a source of truth for teams to understand their data assets. start keeping all your definitions in one searchable place. the glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as: owners of your data, so you know who to ask for clarification. certificate status, to easily understand if metadata enrichment is still in progress or the asset is ready to be used. linked assets that are relevant to the term, so you can explore other helpful material. did you know? the glossary helps power atlan s powerful search tool , so tagging and defining assets are critical to helping your team find what they need. discovery we rely on search bars to find things in almost every corner of the",
    "source": "https://docs.atlan.com/get-started/how-tos/quick-start-for-admins"
  },
  {
    "text": "internet. atlan uses a similar search tool to help you explore your data assets. the discovery tool is atlan s powerful in platform search, powered by the terms and descriptions you ve added to your data assets. here are a few of the things that make atlan s discovery awesome: every attribute of your data is searchable in atlan saved sql queries, schemas, links, and more. this lets you search far and wide to find exactly what you need. intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. search assets from just about any page in atlan using cmd ctrl k or by clicking search assets across atlan at the top of any page. control your search by using facets about your data (such as the verification status or owner) to find what s most important to you. sort by",
    "source": "https://docs.atlan.com/get-started/how-tos/quick-start-for-admins"
  },
  {
    "text": "popularity to quickly discover what assets your teammates are using every day. user management asset profile glossary discovery",
    "source": "https://docs.atlan.com/get-started/how-tos/quick-start-for-admins"
  },
  {
    "text": "secure agent atlan documentation skip to main content on this page the atlan secure agent is a lightweight, kubernetes based application that enables secure metadata extraction. it connects internal systems with atlan saas while keeping sensitive data protected and doesn t require inbound connectivity. running within an organization s controlled environment, the secure agent ensures compliance with security policies and automates metadata processing. figure 1: the secure agent runs in the customer environment and acts as a gateway. key capabilities the secure agent is designed for secure, scalable, and efficient metadata extraction. security first architecture runs entirely within the organization s infrastructure, preventing secrets from leaving its boundary. uses outbound, encrypted communication to interact with atlan saas. supports logging and monitoring and integrates with external monitoring systems for auditing and compliance. scalable metadata extraction a single deployment of the agent can connect to multiple source systems. supports multiple concurrent metadata",
    "source": "https://docs.atlan.com/secure-agent"
  },
  {
    "text": "extraction jobs. uses kubernetes based workloads for efficient resource management. flexible deployment deploys on cloud based kubernetes environments (such as amazon eks, azure aks, and google gke) or on premises clusters. scales dynamically based on workload demands. automated operations continuously monitors system health and sends heartbeats to atlan. captures and uploads execution logs for troubleshooting and auditing. provides performance insights through metrics and alerts. how it works the secure agent follows a job based execution model where metadata extraction tasks are scheduled and executed within the organization s environment. the workflow typically involves: atlan triggers a metadata extraction job. the secure agent retrieves job details and extracts metadata using source specific connectors. extracted metadata is shared with atlan either through cloud storage or direct ingestion. atlan workflows process the extracted metadata and publish the assets. logs and execution status are sent to atlan for monitoring and auditing. see also deployment",
    "source": "https://docs.atlan.com/secure-agent"
  },
  {
    "text": "architecture : learn more about how the secure agent integrates with your environment and supports secure metadata extraction. key capabilities how it works see also",
    "source": "https://docs.atlan.com/secure-agent"
  },
  {
    "text": "playbooks atlan documentation skip to main content on this page overview: atlan s playbooks provide reusable workflows and automation for common data tasks. create, share, and execute standardized processes to maintain consistency, reduce manual effort, and enable self service for data consumers while following governance standards. get started how to set up playbooks guides playbook management how to manage playbooks : monitor and maintain your playbook workflows. how to automate data profiling : set up automated data quality checks. troubleshooting troubleshooting playbooks : solutions for common playbook issues. get started guides troubleshooting",
    "source": "https://docs.atlan.com/product/capabilities/playbooks"
  },
  {
    "text": "discovery atlan documentation skip to main content on this page overview: atlan s discovery capabilities help users find, understand, and use data assets across your organization. with powerful search, filtering, and browsing features, users can quickly locate relevant data assets, explore their context, and access the information they need to make data driven decisions. get started how to search and discover assets for detailed search, filtering, and troubleshooting information, use the sidebar navigation. get started",
    "source": "https://docs.atlan.com/product/capabilities/discovery"
  },
  {
    "text": "contracts atlan documentation skip to main content on this page overview: manage data contracts and agreements in atlan to ensure data quality and compliance. define and track data quality expectations, service level agreements (slas), and data sharing agreements between teams and systems. get started follow these steps to implement contracts in atlan: create data contracts guides add contract impact analysis in github : detailed instructions on adding contracts for impact analysis in github. get started guides",
    "source": "https://docs.atlan.com/product/capabilities/governance/contracts"
  },
  {
    "text": "integrations atlan documentation skip to main content atlan integrates with a wide range of tools to help you automate workflows, connect with your favorite apps, and manage identity and access. these integrations connect your data catalog with the tools your teams already use, creating a seamless data experience across your tech stack. key concepts integration categories : atlan offers integrations across five categories: project management, communication, collaboration, automation, and identity management. connection methods : most integrations use secure authentication methods like oauth, api keys, or service accounts. bi directional sync : updates flow between atlan and integrated tools, ensuring data consistency across platforms. custom webhooks : extend atlan s capabilities by building custom integrations using the provided apis and webhooks. core offerings automation connect with platforms like aws lambda to automate data workflows and streamline routine tasks. collaboration integrate with tools like slack and microsoft teams to enhance team collaboration",
    "source": "https://docs.atlan.com/product/integrations"
  },
  {
    "text": "and knowledge sharing. communication connect with smtp for real time alerts. identity management integrate with identity providers like okta and azure ad for seamless authentication and user management. project management connect with tools like jira and service now to link data assets to projects and track data related tasks. get started 1 select an integration choose from atlan s available integrations based on your team s tools and workflows. 2 configure connection follow the integration specific setup guide to establish a secure connection with your tool. 3 test and activate verify the integration is working correctly with a test action, then activate for your organization. need a custom integration? atlan provides apis and webhooks that let you build custom integrations with any tool in your tech stack.",
    "source": "https://docs.atlan.com/product/integrations"
  },
  {
    "text": "customer support atlan documentation skip to main content on this page one of atlan s core values is to help you and your team do your life s best work. that s why atlan wants to make it as easy as possible for you to keep driving your work forward with data. atlan s customer support is a combination of several teams in atlan: product support personnel cloud support personnel devops engineering support personnel vast repository of self service resources service level commitment atlan s technical support team provides support globally with high response commitment levels. this includes 24 7 sre support for critical (p0) issues. customers get a service level commitment, including the following: 99.5 uptime for atlan dedicated support center, available from within the atlan product commitments for aggressive response times for business critical issues designated customer success manager to assist with escalations ways to contact support email",
    "source": "https://docs.atlan.com/support/references/customer-support"
  },
  {
    "text": "support at a dedicated customer support email account ( [email protected] ) in product support widget to log tickets and a help desk portal to log and track tickets. you can sign up to track support tickets on the help desk portal. you must use your organizational email address as the username and create a password. submit a support request via the online form. to track your support tickets: navigate to https: atlan.zendesk.com and log into the help desk portal with your credentials or via sso. from the top right, click your avatar, and then from the dropdown, click my activities . on the my activities page, you can do the following: my requests and requests i m cc d on view and edit the support tickets you either created or were copied on, respectively. organization requests to access all other support tickets for your organization, please reach out to",
    "source": "https://docs.atlan.com/support/references/customer-support"
  },
  {
    "text": "your customer success manager. atlan provides you with read access to all the support tickets for your organization. to be able to comment on or close them, you must be cc d on all tickets. hours of operation 24x7 availability for all requests and issues severity levels the atlan technical support team determines the severity of an issue. the customer s position is considered, and these guidelines are followed to determine priority. below are the response time slas: severity description basic support advanced support s0 production software is unavailable; all customers are blocked and productivity halted 2 hours 1 hour s1 production software is available; functionality or performance is severely impaired 4 hours 2 hours s2 production software is available and usable with partial, noncritical loss of functionality. or, production software has an occasional issue that customer requests identification and resolution. also includes requests for help with administrative tasks 16",
    "source": "https://docs.atlan.com/support/references/customer-support"
  },
  {
    "text": "hours 4 hours s3 cosmetic issues or request for general information about the software, documentation, processes, or procedures 24 hours 14 hours escalation procedure if the business impact of a support request changes or a ticket isn t being handled according to your expectations, you may escalate the ticket. please first speak with the technical support representative assigned to the ticket to confirm that the business impact and urgency are understood. you may further escalate by contacting: 1st level of escalation : technical support engineer 2nd level of escalation : director, support 3rd level of escalation : head of customer experience service level commitment ways to contact support hours of operation severity levels escalation procedure",
    "source": "https://docs.atlan.com/support/references/customer-support"
  },
  {
    "text": "atlan developers dear developers, are you keen to activate your metadata? to automate tedious manual work, or reduce the complexity of some of your largest data challenges? we re here to help. welcome to our for the developer community of atlan. here you ll find reference materials for working with atlan programmatically. we look forward to helping you, the s of data, develop the future s of data! get started jump in overview 2022 08 22 2022 08 22 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your",
    "source": "https://developer.atlan.com/"
  },
  {
    "text": "consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/"
  },
  {
    "text": "introductory walkthrough developer skip to content an introductory walkthrough atlan university you might also like our atlan platform essentials certification . not sure where to start? allow us to introduce atlan development through example. 1 setting up we strongly recommend using one of our sdks to simplify the development process. as a first step, set one up: java python kotlin go the sdk is available on maven central , ready to be included in your project: build.gradle.kts repositories mavencentral () dependencies implementation ( com.atlan:atlan java: ) (1) testruntimeonly ( ch.qos.logback:logback classic:1.2.11 ) (2) include the latest version of the java sdk in your project as a dependency. you can also give a specific version instead of the , if you d like. the java sdk uses slf4j for logging purposes. you can include logback as a simple binding mechanism to send any logging information out to your console (standard out).",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "provide two values to create an atlan client: atlanlivetest.java 1 2 3 4 5 6 7 8 9 10 11 12 import com.atlan.atlanclient ; public class atlanlivetest public static void main ( string [] args ) try ( atlanclient client new atlanclient ( https: tenant.atlan.com , (1) ... ) (2) ) (3) provide your atlan tenant url as the first parameter. you can also read the value from an environment variable, if you leave out both parameters. provide your api token as the second parameter. you can also read the value from another environment variable, by leaving out this parameter. you can then start writing some actual code to run within a static main method. (we ll show some examples of this further below.) once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. set up logging for sdk you can also",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "checkout to the advanced configuration section of the sdk to learn about how to set up logging. the sdk is available on pypi . you can use pip to install it as follows: install the sdk pip install pyatlan provide two values to create an atlan client: atlan live test.py 1 2 3 4 5 6 from pyatlan.client.atlan import atlanclient client atlanclient ( base url https: tenant.atlan.com , (1) api key ... (2) ) provide your atlan tenant url to the base url parameter. (you can also do this through environment variables .) provide your api token to the api key parameter. (you can also do this through environment variables .) set up logging for sdk you can also checkout to the advanced configuration section of the sdk to learn about how to set up logging. the sdk is available on maven central , ready to be included in your",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "project: build.gradle.kts repositories mavencentral () dependencies implementation ( com.atlan:atlan java: ) (1) implementation ( io.github.microutils:kotlin logging jvm:3.0.5 ) (2) implementation ( org.slf4j:slf4j simple:2.0.7 ) include the latest version of the java sdk in your project as a dependency. you can also give a specific version instead of the , if you d like. the java sdk uses slf4j for logging purposes. you can include slf4j simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin logging jvm microutil. provide two values to create an atlan client: atlanlivetest.kt 1 2 3 4 5 6 7 8 9 10 import com.atlan.atlanclient; fun main () atlanclient ( https: tenant.atlan.com , (1) ... (2) ). use client (3) provide your atlan tenant url as the first parameter. you can also read the value from an environment variable, if you leave out both parameters.",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "provide your api token as the second parameter. you can also read the value from another environment variable, by leaving out this parameter. you can then start writing some actual code to run within a static main method. (we ll show some examples of this further below.) once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. set up logging for sdk you can also checkout to the advanced configuration section of the sdk to learn about how to set up logging. the sdk is available on github , ready to be included in your project: main.go 1 2 3 4 5 package main import ( github.com atlanhq atlan go atlan assets ) provide two values to set up connectivity to atlan: main.go 6 7 8 9 10 11 func main () ctx , : assets . context ( https: tenant.atlan.com ,",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "(1) ... (2) ) provide your atlan tenant url to the assets.context() method. if you prefer using the value from an environment variable, you can use assets.newcontext() without any parameters. provide your api token as the second parameter to the assets.context() method. (or again, have it picked up automatically by the assets.newcontext() method.) set up logging for sdk you can also checkout to the advanced configuration section of the sdk to learn about how to set up logging. don t forget to give permissions if you want to be able to access existing metadata with an api token, don t forget that you need to assign one or more personas to the api token that grant it access to metadata. retrieving metadata now that you have an sdk installed and configured, you are ready to code! before we jump straight to code, though, let s first introduce some key concepts",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "in atlan: what is an asset? in atlan, we refer to all objects that provide context to your data as assets . classdiagram class table certificatestatus announcementtype columncount rowcount ... atlanschema() columns() class column certificatestatus announcementtype datatype isnullable ... table() table column each type of asset in atlan has a set of: properties , such as: certificates announcements relationships to other assets, such as: schema child tables table parent schema table child columns column parent table assets are instances of metadata. in an object oriented programming sense, think of an asset as an instance of a class. the structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that s for another day. so as you can see: there are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. assets inter relate with",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columncount that only exists on tables, not on schemas or databases). when you know the asset when you already know which asset you want to retrieve, you can read it from atlan using one of its identifiers . we ll discuss these in more detail as part of updates, but for now you can think of them as: guid is a primary key for an asset: completely unique, but meaningless by itself qualifiedname is a business key for an asset: unique for a given kind of asset, and interpretable java python kotlin go retrieve an asset (atlanlivetest.java) 5 6 7 8 try ( atlanclient",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "client new atlanclient ()) table table table . get ( client , b4113341 251b 4adc 81fb 2420501c30e6 ); (1) table table . get ( client , default snowflake 1234567890 my db my schema my table ); you can retrieve an asset using the static get() method on any asset type, providing the client and either the asset s guid or qualifiedname . (each asset type is its own unique class in the sdk.) retrieve an asset (atlan live test.py) 7 8 9 10 11 12 13 14 table client . asset . get by guid ( (1) asset type table , guid b4113341 251b 4adc 81fb 2420501c30e6 ) table client . asset . get by qualified name ( asset type table , qualified name default snowflake 1234567890 my db my schema my table ) you can retrieve an asset using the asset.get by guid() method on the atlan client, providing",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "both the type of asset you expect to retrieve and its guid. (each asset type is its own unique class in the sdk.) you can also retrieve an asset using the asset.get by qualified name() method on the atlan client, providing the type of asset you expect to retrieve and its qualified name . (each asset type is its own unique class in the sdk.) retrieve an asset (atlanlivetest.kt) 4 5 6 7 atlanclient (). use client var table table . get ( client , b4113341 251b 4adc 81fb 2420501c30e6 ) (1) table table . get ( client , default snowflake 1234567890 my db my schema my table ) you can retrieve an asset using the static get() method on any asset type, providing the client and either the asset s guid or qualifiedname . (each asset type is its own unique class in the sdk.) request an asset (main.go)",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "11 12 13 14 15 16 response , err : assets . getbyguid [ assets . table ]( (1) b4113341 251b 4adc 81fb 2420501c30e6 ) response , err : assets . getbyqualifiedname [ assets . table ]( (2) default snowflake 1234567890 my db my schema my table ) you can retrieve an asset using the assets.getbyguid() method on the atlan client, providing both the type of asset you expect to retrieve and its guid. (each asset type is its own unique class in the sdk.) you can also retrieve an asset using the assets.getbyqualifiedname() method on the atlan client, providing the type of asset you expect to retrieve and its qualifiedname . (each asset type is its own unique class in the sdk.) note that the response is strongly typed: if you are retrieving a table, you will get a table back (as long as it exists). you do not",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "need to figure out what properties or relationships exist on a table the table class defines them for for you already. in any modern ide, this means you have type ahead support for retrieving the properties and relationships from the table variable. you can also refer to the types reference in this portal for full details of every kind of asset. retrieval by identifier can be more costly than you might expect even though you are retrieving an asset by an identifier, this can be more costly than you might expect. retrieving an asset in this way will: retrieve all its properties and their values retrieve all its relationships imagine the asset you are retrieving has 100 s or 1000 s of these. if you only care about its certificate and any owners, you will be retrieving far more information than you need. when you need to find it first",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "what if you don t know the asset s identifier? or what if you want to retrieve many assets with some common set of characteristics? in that case, you can search for the asset(s) . for example, imagine you want to find all tables named my table : java python kotlin go search for an asset (atlanlivetest.java) 5 6 7 8 9 10 try ( atlanclient client new atlanclient ()) list tables table . select ( client ) (1) . where ( table . name . eq ( my table )) (2) . stream () (3) . tolist (); you can search all active assets of a given type using the select() static method. chain onto this method any conditions you want to apply to the search, in this example a where clause that will match any table whose name equals my table . you can then stream the results",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "from this search and process them as any standard java stream: filter them, limit them, apply an action to each one, and so on. the results of the search are automatically paged and each page is lazily fetched. search for an asset (atlan live test.py) 7 8 9 10 11 12 13 14 15 16 17 18 from pyatlan.model.fluent search import fluentsearch from pyatlan.model.assets import table request ( fluentsearch () (1) . where ( fluentsearch . asset type ( table )) . where ( fluentsearch . active assets ()) . where ( table . name . eq ( my table )) (2) ) . to request () (3) tables [] for result in client . asset . search ( request ): (4) tables . append ( result ) you can search all active assets of a given type by creating a fluentsearch() object and chaining two where clauses: fluentsearch.asset type",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "to limit to a particular kind of asset fluentsearch.active assets() to limit to only active assets of that kind chain onto this method any conditions you want to apply to the search, in this example a where clause that will match any table whose name equals my table . you can then convert this object into a search request using the to request() method. run the request using the asset.search() method on the atlan client, and you can directly iterate through the search results. the results of the search are automatically paged and each page is lazily fetched. search for an asset (atlanlivetest.kt) 4 5 6 7 8 9 atlanclient (). use client val tables table . select ( client ) (1) . where ( table . name . eq ( my table )) (2) . stream () (3) . tolist () you can search all active assets of a",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "given type using the select() static method. chain onto this method any conditions you want to apply to the search, in this example a where clause that will match any table whose name equals my table . you can then stream the results from this search and process them as any standard kotlin stream: filter them, limit them, apply an action to each one, and so on. the results of the search are automatically paged and each page is lazily fetched. search for an asset (main.go) 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 searchresponse , err : assets . newfluentsearch (). (1) activeassets (). pagesizes ( 300 ). where ( ctx . table . name . eq ( my table )). (2) execute () (3) entities , erriter : searchresponse . iter () (4) for asset : range entities fmt . println",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "( asset name: , asset . name ) if err : tables table . select ( client ) . where ( table . name . eq ( my table )) . includeonresults ( table . certificate status ) (1) . stream () . tolist (); only this line differs from the original query. you can chain as many includeonresults calls as you want to specify the properties and relationships you want to retrieve for matching assets. search for an asset (atlan live test.py) 7 8 9 10 11 12 13 14 15 16 17 18 19 from pyatlan.model.fluent search import fluentsearch from pyatlan.model.assets import table request ( fluentsearch () . where ( fluentsearch . asset type ( table )) . where ( fluentsearch . active assets ()) . where ( table . name . eq ( my table )) . include on results ( table . certificate status ) (1)",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": ") . to request () tables [] for result in client . asset . search ( request ): tables . append ( result ) only this line differs from the original query. you can chain as many include on results calls as you want to specify the properties and relationships you want to retrieve for matching assets. search for an asset (atlanlivetest.kt) 4 5 6 7 8 9 10 atlanclient (). use client val tables table . select ( client ) . where ( table . name . eq ( my table )) . includeonresults ( table . certificate status ) (1) . stream () . tolist () only this line differs from the original query. you can chain as many includeonresults calls as you want to specify the properties and relationships you want to retrieve for matching assets. search for an asset (main.go) 11 12 13 14 15 16",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "17 18 19 20 21 22 23 24 25 26 searchresponse , err : assets . newfluentsearch (). activeassets (). pagesizes ( 300 ). where ( ctx . table . name . eq ( my table )). includeonresults ( assets . certificate status ). (1) execute () entities , erriter : searchresponse . iter () for asset : range entities fmt . println ( asset name: , asset . name ) if err : val toupdate table . updater ( (1) default snowflake 1234567890 my db my schema my table , my table ) . certificatestatus ( certificatestatus . verified ) (2) . build () (3) val response toupdate . save ( client ) (4) you can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedname . using the updater() static method on any asset type, you pass in (typically) the qualifiedname",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "and name of the asset. this returns a builder onto which you can then chain any updates. you can then chain onto the returned builder as many updates as you want. in this example, we change the certificate status to verified . at the end of your chain of updates, you need to build the builder (into an object, in memory). and then, finally, you need to .save() that object to persist those changes in atlan (passing the client for the tenant you want to save it in). the response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. update an asset (main.go) 11 12 13 14 15 16 17 toupdate : assets . table (1) toupdate . updater ( default snowflake 1234567890 my db my schema my table , my table ) toupdate . certificatestatus atlan",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": ". certificatestatusverified (2) response , err : assets . save ( toupdate ) (3) you can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedname . using the updater() method on any asset type, you pass in (typically) the qualifiedname and name of the asset. this returns an object into which you can then place any updates. you can place into the returned object as many updates as you want. in this example, we change the certificate status to verified . and then, finally, you need to .save() that object to persist those changes in atlan. the response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. atlan will handle idempotency by sending only the changes you want to apply, atlan can make idempotent updates. atlan will only attempt",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "to update the asset with the changes you send. atlan leaves any existing metadata on the asset as is. if the asset already has the metadata values you are sending, atlan does nothing. it will not even update audit details like the last update timestamp, and is thus idempotent. bulk changes what if you want to make changes to many assets, as efficiently as possible? in that case, you are best making use of a combination of sdk functionality search, trim, and batch : java python kotlin go bulk changes (atlanlivetest.java) 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 try ( atlanclient client new atlanclient ()) parallelbatch batch parallelbatch ( client , 20 ); (1) table . select ( client ) (2) . where ( table . name . eq ( my table )) . includeonresults ( table . certificate status",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": ") (3) . pagesize ( 20 ) (4) . stream ( true ) (5) . foreach ( a batch . add ( (6) a . trimtorequired () (7) . certificatestatus ( certificatestatus . deprecated ) . build ()); ); batch . flush (); (8) list created batch . getcreated (); (9) list updated batch . getupdated (); start by initializing a batch. through this batch, we can automatically queue up and bulk upsert assets in this example, 20 at a time. then use the search pattern we discussed earlier to find all the assets you want to update. be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). it is a good idea to set the page size for search results to match the asset batch size, for maximal efficiency. when you stream",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "the results of the search, you can send an optional boolean parameter. if set to true , this will stream the pages of results in parallel (across multiple threads), improving throughput. when you then operate on each search result, you can add() any updates directly into the batch you created earlier. the batch itself will handle saving these to atlan when a sufficient number have been queued up (20, in this example). to make an update to a search result, first call trimtorequired() against the result. this will pare down the asset to its minimal required attributes and return a builder. you can then chain as many updates onto this builder as you want, keeping to the pattern we discussed above ensuring you are sending only changes. you must flush() the batch outside of any loop where you ve added assets into it. this ensures any final remaining elements in",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "the batch are still sent to atlan, even if the batch is not full . finally, from the batch you can retrieve the minimal details about any assets it created or updated. bulk changes (atlan live test.py) 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from pyatlan.model.fluent search import fluentsearch from pyatlan.model.assets import table from pyatlan.client.asset import batch from pyatlan.model.enums import certificatestatus batch batch ( client . asset , max size 20 ) (1) request ( (2) fluentsearch () . where ( fluentsearch . asset type ( table )) . where ( fluentsearch . active assets ()) . where ( table . name . eq ( my table )) . include on results ( table . certificate status ) (3) . page size ( 20 ) (4) ) . to request () tables [] for result",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "in client . asset . search ( request ): revised result . trim to required () (5) revised . certificate status certificatestatus . deprecated batch . add ( revised ) (6) batch . flush () (7) created batch . created (8) updated batch . updated start by initializing a batch. through this batch, we can automatically queue up and bulk upsert assets in this example, 20 at a time. then use the search pattern we discussed earlier to find all the assets you want to update. be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). it is a good idea to set the page size for search results to match the asset batch size, for maximal efficiency. when you then operate on each search result, first call trim to required() against the",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "result. this will pare down the asset to its minimal required attributes. you can then add as many updates onto this object as you want, keeping to the pattern we discussed above ensuring you are sending only changes. you can then add() any updated objects directly into the batch you created earlier. the batch itself will handle saving these to atlan when a sufficient number have been queued up (20, in this example). you must flush() the batch outside of any loop where you ve added assets into it. this ensures any final remaining elements in the batch are still sent to atlan, even if the batch is not full . finally, from the batch you can retrieve the minimal details about any assets it created or updated. bulk changes (atlanlivetest.kt) 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 atlanclient ().",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "use client val batch parallelbatch ( client , 20 ) (1) table . select ( client ) (2) . where ( table . name . eq ( my table )) . includeonresults ( table . certificate status ) (3) . pagesize ( 20 ) (4) . stream ( true ) (5) . foreach a batch . add ( (6) a . trimtorequired () (7) . certificatestatus ( certificatestatus . deprecated ) . build ()) batch . flush () (8) val created batch . created (9) val updated batch . updated start by initializing a batch. through this batch, we can automatically queue up and bulk upsert assets in this example, 20 at a time. then use the search pattern we discussed earlier to find all the assets you want to update. be sure to include any details you might need to make a decision about whether to update the asset",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "or not (and what to update it with). it is a good idea to set the page size for search results to match the asset batch size, for maximal efficiency. when you stream the results of the search, you can send an optional boolean parameter. if set to true , this will stream the pages of results in parallel (across multiple threads), improving throughput. when you then operate on each search result, you can add() any updates directly into the batch you created earlier. the batch itself will handle saving these to atlan when a sufficient number have been queued up (20, in this example). to make an update to a search result, first call trimtorequired() against the result. this will pare down the asset to its minimal required attributes and return a builder. you can then chain as many updates onto this builder as you want, keeping to the",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "pattern we discussed above ensuring you are sending only changes. you must flush() the batch outside of any loop where you ve added assets into it. this ensures any final remaining elements in the batch are still sent to atlan, even if the batch is not full . finally, from the batch you can retrieve the minimal details about any assets it created or updated. bulk changes (main.go) 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 batch : assets . newbatch ( ctx , 20 , true , atlan . ignore , true ) (1) searchresponse , : assets . newfluentsearch (). (2) assettype ( table ). activeassets (). where ( ctx . table . name . eq ( my table )). includeonresults",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "( assets . certificate status ). (3) pagesizes ( 20 ). (4) execute () entities , erriter : searchresponse . iter () for asset : range entities revised , err : assets . trimtorequired ( asset ) (5) if err ! nil logger . log . errorf ( error trimming asset: v , err ) revised . certificatestatus atlan . certificatestatusverified err batch . add ( revised ) (6) if err ! nil logger . log . errorf ( failed to add asset to batch: v , err ) if err : erriter ; err ! nil fmt . println ( error during iteration: , err ) batch . flush () (7) for , asset : range batch . created () (8) fmt . println ( asset ) for , asset : range batch . updated () fmt . println ( asset ) start by initializing a batch. through this",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "batch, we can automatically queue up and bulk upsert assets in this example, 20 at a time. then use the search pattern we discussed earlier to find all the assets you want to update. be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). it is a good idea to set the page size for search results to match the asset batch size, for maximal efficiency. when you then operate on each search result, first call trimtorequired() against the result. this will pare down the asset to its minimal required attributes. you can then add as many updates onto this object as you want, keeping to the pattern we discussed above ensuring you are sending only changes. you can then add() any updated objects directly into the batch you created earlier. the batch",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "itself will handle saving these to atlan when a sufficient number have been queued up (20, in this example). you must flush() the batch outside of any loop where you ve added assets into it. this ensures any final remaining elements in the batch are still sent to atlan, even if the batch is not full . finally, from the batch you can retrieve the minimal details about any assets it created or updated. where to go from here now that you know the basics, it s up to you to delve further into whichever areas you like. you can search (upper right) or use the top level menu: common tasks common operations on assets, that are available across all assets. discover actions asset specific operations that are specific to certain assets. focus on a specific kind of asset governance structures operations dealing with governance structures, rather than assets. manage",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "governance structures samples real code samples our customers use to solve particular use cases. review live samples searching delve deep into searching and aggregating metadata. learn more about searching events delve deep into the details of the events atlan triggers. learn more about events note that this is intentionally kept as simple as possible. the walkthrough is not intended to be exhaustive. where possible, we have cross referenced other detailed examples elsewhere in the site. there are orders of magnitude lower chances of guids conflicting with each other than there are grains of sand on the planet. (and generating them does not rely on a central id assigning registry.) 2023 04 19 2025 03 26 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/getting-started/"
  },
  {
    "text": "developing with atlan developer skip to content overall site map introduction if you are new to atlan, or to developing with atlan, start with one of the following two options. these will set you up to develop with atlan, step by step. atlan university self paced, video based walkthrough of the essentials of atlan as a platform. atlan platform essentials certification introductory walkthrough documentation based walkthrough, including step by step examples. start the walkthrough jumping in if you are confident setting yourself up, and instead want to jump straight into specific examples, you can search (upper right) or use the top level menu: common tasks common operations on assets, that are available across all assets. discover actions asset specific operations that are specific to certain assets. focus on a specific kind of asset governance structures operations dealing with governance structures, rather than assets. manage governance structures samples real code samples",
    "source": "https://developer.atlan.com/concepts/"
  },
  {
    "text": "our customers use to solve particular use cases. review live samples searching delve deep into searching and aggregating metadata. learn more about searching events delve deep into the details of the events atlan triggers. learn more about events 2022 08 22 2024 10 01 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/concepts/"
  },
  {
    "text": "toolkits developer skip to content toolkits writing code is great, but there additional frameworks you can use to tap more directly into atlan. in this toolkits section you will find details about these frameworks. internal use only for now, these toolkits are only available for atlanians . however, they should give a sense for our longer term ambitions for developers. packages your integration or automation, available in atlan s marketplace of packages. bundle for reuse typedefs your unique metadata modeled and available as first class objects in atlan. design custom assets testing your guide to write robust, reusable integration tests for connectors and utilities in atlan. test your connector and utilities how they work our toolkits are mostly built on pkl , a configuration language developed by apple. this allows you to develop your idea: safely, based on a guard railed set of options independently from any specific programming language",
    "source": "https://developer.atlan.com/toolkits/"
  },
  {
    "text": "offline from a running atlan tenant we can then automatically translate that into the necessary bindings for atlan s: ui widgets data model orchestration plane get started to get started, you ll need to install pkl and configure its plugin in your favorite ide. install pkl start by installing the pkl cli : macos linux (aarch64) linux (amd64) alpine (amd64) windows windows brew install pkl curl l o pkl https: github.com apple pkl releases download 0.28.1 pkl linux aarch64 chmod x pkl curl l o pkl https: github.com apple pkl releases download 0.28.1 pkl linux amd64 chmod x pkl curl l o pkl https: github.com apple pkl releases download 0.28.1 pkl alpine linux amd64 chmod x pkl curl l o jpkl https: repo1.maven.org maven2 org pkl lang pkl cli java 0.28.1 jpkl chmod x jpkl note the cli command is jpkl on windows (or any other platform using the java",
    "source": "https://developer.atlan.com/toolkits/"
  },
  {
    "text": "executable), the command is jpkl rather than pkl . configure in an ide then configure pkl in your favorite ide: jetbrains vs code neovim for now you ll have to rely on pkl s own instructions . for now you ll have to rely on pkl s own instructions . 2024 03 14 2025 04 28 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/"
  },
  {
    "text": "package toolkit developer skip to content package toolkit with the package toolkit you can bundle your integration or automation, and make it available in atlan s marketplace of packages. we use a pkl model to restrict the way you define the package so that it precisely fits the best practices and structures available in atlan s user interface for packages and the underlying argo orchestration. overview of the process config: mirroractors: false sequencediagram actor you as you loop until testing is successful create participant ide as ide you ide: define your package create participant pkl as pkl cli you pkl: render your package destroy pkl pkl you: yaml templates you ide: develop your logic you ide: test your logic destroy ide ide you: create participant atl as atlan tenant you atl: deploy your package atl you: you atl: test your package destroy atl atl you: end create participant git as",
    "source": "https://developer.atlan.com/toolkits/custom-package/"
  },
  {
    "text": "github you )git: raise prs (release) 2024 03 14 2025 03 12 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/custom-package/"
  },
  {
    "text": "running example developer skip to content running example to help explain how this all works, we ll use the example of a package we want to set up that will load api definitions from an openapi specification (json). we will want some inputs to configure the package, such as: the json containing the api definitions whether to load these into a new connection or reuse an existing connection the details for the connection (whether new or existing) and we will want to provide some general information about the package itself, such as: it s name an icon where it s documented 2025 03 12 2025 03 12 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow",
    "source": "https://developer.atlan.com/toolkits/custom-package/example/"
  },
  {
    "text": "you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/custom-package/example/"
  },
  {
    "text": "define package via template developer skip to content define package via template how to read this guide each section of this guide provides 2 tabs, which are linked throughout (once you swap in one section, all other sections will automatically reflect that same level of detail): simple when you are just starting out, follow these tabs to understand the basic structure of the toolkit and the fundamental elements that you must use. detailed as you start to wonder about additional complexity, consider changing to these tabs, which cover additional (optional) possibilities. start by creating a pkl file that amends our published package config toolkit model: mycustompackage.pkl 1 amends package: developer.atlan.com toolkits custom package config 5.0.3 framework.pkl if this is the first time you re creating a package, hover over that line and download the pkl package. define overall metadata then you can start defining your package. all packages should have",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "at least these main components: mycustompackage.pkl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 amends package: developer.atlan.com toolkits custom package config 5.0.3 framework.pkl import pkl:semver (1) import package: developer.atlan.com toolkits custom package config 5.0.3 connectors.pkl (2) packageid csa openapi spec loader (3) packagename openapi spec loader (4) version semver . version ( 1.0.0 ) (5) description loads api specs and paths from an openapi (v3) definition. (6) iconurl http: assets.atlan.com assets apispec.png (7) docsurl https: developer.atlan.com samples loaders openapi (8) implementationlanguage kotlin (9) containerimage ghcr.io atlanhq (name): (version) (10) containercommand (11) dumb init java openapispecloaderkt outputs (12) files [ debug logs ] tmp debug.log keywords (13) kotlin crawler openapi category custom (14) preview true (15) certified false",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "(16) connectortype connectors . api (17) uiconfig (18) ... credentialconfig (19) ... import the standard pkl library for semantic versioning, to define your package s version. (optional) if your package could create a connection, import the pre defined set of connections from the package toolkit. a packageid , which gives a unique namespaced identity for your package. it must be of the form namespace kebab case name . a packagename , which gives the human readable name of your package, as it should show in the ui. a version for your package, which must follow semantic versioning. a description for your package, as it should show in the ui. an iconurl giving an online location where the icon to use in the ui for this package can be accessed. a docsurl giving an online location where additional documentation about the package can be found. this will be linked in the",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "ui. the implementationlanguage defining the coding language you will use to implement the logic of your package. this is used by the toolkit to generate a strongly typed data class that captures the inputs a user provides during setup of the package. the containerimage that will encapsulate your package s code. use variables where possible note that pkl is itself a language, so you can use variables here as well (for example, if your container images follow a naming convention that can be generated from the packageid or to automatically set the version). the full containercommand used to run your custom logic within the container. this should be specified as a list, rather than a single spaced out string. (optional) any outputs your package s logic will produce, that you want a user to be able to download. in this example, our package will write debug logs out to tmp",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "debug.log and we will make these available for download. (optional) any keywords you want your package to have associated with it. (optional) the category controls the name of the pill in the marketplace in the ui where the package will be listed. it will default to custom if not specified. (optional) setting preview to true will show a small orange badge on the package indicating it is experimental (by default it will not be experimental). (optional) setting certified to false will remove the green tick badge from the package indicating it is certified (by default it will be certified). (optional) when your package could create a connection, use the connectortype to define what kind of connection it should create. a uiconfig section where you will define the inputs you want your package to receive from the user during setup. (optional) a credentialconfig section where you can define any sensitive inputs",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "you want the user to provide, typically to connect to an external system. any details provided by a user within these inputs will be encrypted and stored in a secure vault in atlan. define configurable inputs use the uiconfig section to define the configurable inputs you want the user to provide when setting up your package. these inputs fit into a two level hierarchy: tasks are the top level steps you see along the left side when setting up a new package. inputs are the individual widgets a user can enter information into, select from, etc within each task (step). in addition to these inputs, you can (optionally) define one or more rules to control which inputs appear on the screen, based on what a user has selected in some other input. simple detailed mycustompackage.pkl 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "50 51 52 53 uiconfig tasks (1) [ configuration ] (2) description openapi spec configuration (3) inputs (4) ... [ connection ] (5) description connection details inputs ... rules (6) ... you must define one (and only one) list of tasks . use the name as you want it to show in the ui for each top level step, surrounded in square brackets. you can then enclose the definition of that top level step in curly braces. include a description , which will show in the ui as part of the top level step (very space limited). also include the list of inputs that should be configured as part of that top level step. you can define as many of these top level steps as you like. they will be displayed in the order (top down) they are listed here in the model. (optional) you can then specify any rules",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "to control which inputs appear on the screen (or not), based on what a user has selected in some other input. mycustompackage.pkl 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 uiconfig tasks (1) [ configuration ] (2) description openapi spec configuration (3) inputs (4) [ spec url ] new textinput (5) title specification url (6) required true (7) helptext full url to the json form of the openapi specification. (8) placeholdertext https: petstore3.swagger.io api v3 openapi.json (9) [ connection ] (10) description connection details inputs [ connection usage ] new radio title connection required true possiblevalues [ create ] create [ reuse ]",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "reuse default reuse fallback default (11) helptext whether to create a new connection to hold these api assets, or reuse an existing connection. [ connection ] new connectioncreator title connection required true helptext enter details for a new connection to be created. [ connection qualified name ] new connectionselector title connection required true helptext select an existing connection to load assets into. rules (12) new uirule (13) wheninputs [ connection usage ] reuse (14) required connection qualified name (15) new uirule (16) wheninputs [ connection usage ] create required connection you must define one (and only one) list of tasks . use the name as you want it to show in the ui for each top level step, surrounded in square brackets. you can then enclose the definition of that top level step in curly braces. include a description , which will show in the ui as part of the",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "top level step (very space limited). also include the list of inputs that should be configured as part of that top level step. for each input, specify a unique variable name (in lower snake case ) surrounded by square brackets. you can choose from a variety of widgets to use for receiving that input from a user. all inputs will at least need a title , which defines the text to show in the ui for that widget. all inputs can also be configured as mandatory using required true (by default they ll be optional). all inputs can also be given a helptext , which when provided shows an information icon next to the title that can be hovered over for more information on how the input should be used. other configurable options will vary by the kind of input, but could include for example some light grey text to",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "show in the box by default to give the user some idea of the value you re expecting from them. you can define as many of these top level steps as you like, and within each as many inputs as you like. they will be displayed in the order (top down) they are listed here in the model. (optional) you can specify a fallback value to use for any input, in case the user does not make any selection or enter any value in the ui. (optional) you can then specify any rules to control which inputs appear on the screen (or not), based on what a user has selected in some other input. each new rule needs to be defined as new uirule , with its definition enclosed in curly braces. you must specify a wheninputs that defines an input variable name and value. when the input variable has",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "the value indicated above, all input variables defined in this required will be shown in the ui (otherwise they will be hidden). when you have multiple conditions or inputs to show or hide, you can use this more spaced out form. this example means that when a user has selected create for the connection usage input, the ui will show the connection input. define sensitive inputs use the credentialconfig section to define sensitive inputs you want the user to provide when setting up your package. encrypted and secured in a vault any details a user provides within this section will be encrypted and secured in a vault within atlan, for added protection. these inputs fit into two types: common inputs are inputs that are common irrespective of the type of credential the user is configuring. options define the different types of credentials a user can configure (for example, basic username",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "and password vs shared secrets). simple detailed mycustompackage.pkl 39 40 41 42 43 44 45 46 47 48 49 50 51 credentialconfig name csa connector custom (1) source connectors . api (2) icon https: assets.atlan.com assets apispec.svg (3) helpdesk https: ask.atlan.com hc en us articles 1234567890 (4) logo https: assets.atlan.com assets apispec.svg (5) commoninputs (6) ... options (7) ... you must give the credentials configuration a unique name. (this allows you to reuse credential configurations without needing to redefine them in different packages.) set the source to the type of connector through which these credentials can be used. define the icon to use when representing this particular source and credential in the ui. provide a link to any documentation about how to set up these credentials, for example any pre requites on the permissions that are required on the external system. define the logo to use when representing this particular",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "source and credential in the ui. (optional) you can define any common inputs that are shared across different connectivity options. you must define at least one option for connecting to the external system, and could define many. for example, you may want to support both a username and password as one option, and another option that uses some shared secrets or api tokens. mycustompackage.pkl 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 credentialconfig name csa connector custom (1) source connectors . api (2)",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "icon https: assets.atlan.com assets apispec.svg (3) helpdesk https: ask.atlan.com hc en us articles 1234567890 (4) logo https: assets.atlan.com assets apispec.svg (5) commoninputs (6) [ host ] new textinput (7) title hostname (8) placeholdertext petstore3.swagger.io (9) prepend https: width 6 [ port ] new numericinput title port default 443 width 2 options (10) [ basic ] (11) title basic (12) inputs (13) [ username ] new textinput (14) title username required true defaultvalue jane.smith width 4 [ password ] new passwordinput title password required true width 4 [ extra ] new nestedinput (15) title role (16) inputs (17) [ role ] new dropdown title role possiblevalues [ user ] user [ admin ] admin width 4 [ keypair ] title keypair inputs [ username ] new textinput title username placeholdertext jsmith width 4 [ password ] new textboxinput title encrypted private key placeholdertext begin encrypted private key miie6tabbgkqhkig9w0bbqmwdgqilypycppzowecaggabiieyligspeegse3xhp1whljfcyycupennlx2bd8 yx8xoxgsgfvb 99 pmslex0fmy9ov1j8h1h9y3lmwxbl...",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "end encrypted private key width 4 [ extra ] new nestedinput title private key password inputs [ private key password ] new passwordinput title private key password width 5 you must give the credentials configuration a unique name. (this allows you to reuse credential configurations without needing to redefine them in different packages.) set the source to the type of connector through which these credentials can be used. define the icon to use when representing this particular source and credential in the ui. provide a link to any documentation about how to set up these credentials, for example any pre requites on the permissions that are required on the external system. define the logo to use when representing this particular source and credential in the ui. (optional) you can define any common inputs that are shared across different connectivity options. for each input, specify a unique variable name (in lower",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "snake case ) surrounded by square brackets. you can choose from a variety of widgets to use for receiving that input from a user. all inputs will need at least a title , which defines the text to show in the ui for that widget. other configurable options will vary by the kind of input, but could include for example some light grey text to show in the box by default to give the user some idea of the value you re expecting from them. you must define at least one option for connecting to the external system, and could define many. for example, you may want to support both a username and password as one option, and another option that uses some shared secrets or api tokens. delegate publishing you can now use the toolkit to delegate publishing of assets through packages that use csv inputs. your own logic",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "simply needs to produce a csv output file that matches the required format of one of these publishing package s csv inputs. handles many common scenarios automatically these publishing packages are designed to handle common scenarios for you directly, such as: calculating differences (deltas) between a previous load and the current load, to automatically: identify which assets should be deleted (or archived), and do so optimize the publishing to only save any assets that have changed support only updating assets (avoiding any accidental asset creation), if desired allow case insensitive matching of qualifiedname s of assets resolve any ambiguity between tables or views for you, automatically to add a delegated publish step to your package, you need only add a publishconfig to your template: detailed mycustompackage.pkl 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 publishconfig",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "new assetimport (1) versiontag 5.0.1 (2) assetsfile transferfile ( (3) outputs , transformed file , current.csv ) assetsupsertsemantic transferconfiginput ( (4) uiconfig , assets upsert semantic ) assetspreviousfileprefix csa relational assets builder (5) assetsdeltasemantic transferconfiginput ( uiconfig , delta semantic ) assetsdeltaremovaltype transferconfiginput ( uiconfig , delta removal type ) assetsdeltareloadcalculation transferconfiginput ( uiconfig , delta reload calculation ) assetsfailonerrors transferconfiginput ( uiconfig , assets fail on errors ) assetsfieldseparator transferconfiginput ( uiconfig , assets field separator ) assetsbatchsize transferconfiginput ( uiconfig , assets batch size ) assetscmhandling transferconfiginput ( uiconfig , assets cm handling ) assetstaghandling transferconfiginput ( uiconfig , assets tag handling ) assetslinkidempotency transferconfiginput ( uiconfig , assets link idempotency ) trackbatches transferconfiginput ( uiconfig , track batches ) when configuring which publish package to delegate, you can choose from either assetimport , relationalassetsbuilder or lineagebuilder . each can be configured with different options, which are described",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "further in the sections below. you need to provide the specific version of the package you want to use for publishing. this version will match the version published at atlanhq atlan java . for any file based inputs, use the toolkit s transferfile() helper to indicate how to transfer the file from the outputs your own package produces: outputs is the name of the variable in your package defining its outputs (will always be this) transformed file is the key of the entry in that outputs variable that points to the file your package produces that you want to delegate for publishing current.csv is the name of the file you want to use when you pass it along to the publishing package for any configuration based inputs, use the toolkit s transferconfiginput() helper to indicate which ui based input variable s value to transfer to the publishing package. uiconfig is",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "the name of the variable in your package defining its ui based inputs (will always be this) assets upsert semantic is the name of the ui input (widget variable) from which to take the value to be transferred you can also use a literal primitive value for any of the options, if you just want to hard code it rather than transfer anything across from the ui based configuration or your own package s outputs. assetimport the assetimport publish config accepts the following configurable inputs. you can use any combination of these, and any you leave out will use sensible defaults. glossaries, categories, terms variable name data type description glossariesfile frameworkrenderer.namepathpair file containing glossaries, categories and terms to import; typically passed through using transferfile() . glossariesupsertsemantic importsemantic whether to allow the creation of new glossaries, categories and terms from the input csv, or ensure these are only updated if they",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "already exist in atlan. glossariesconfig configtype options to optimize how glossaries, categories and terms are imported. glossariesattrtooverwrite listing list of attributes you want to clear (remove) from glossaries, categories and terms if their value is blank in the provided file. glossariesfailonerrors boolean whether an invalid value in a field should cause the import to fail (true) or log a warning, skip that value, and proceed (false). glossariesfieldseparator string single character used to separate fields in the input file (for example, , or ; ). glossariescmhandling custommetadatasemantic how custom metadata in the input should be handled: ignore it, merge it with any existing asset custom metadata, or overwrite the existing asset custom metadata. glossariestaghandling atlantagsemantic how atlan tags on assets in the input should be handled: ignore them, append them to any existing asset tags, replace the existing asset tags, or remove them from the assets. glossarieslinkidempotency linkidempotencyinvariant how linked resources",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "on glossaries in the input should be updated: based on their unique url or their unique name. glossariesbatchsize int maximum number of rows to process at a time (per api request). data domains and products variable name data type description dataproductsfile frameworkrenderer.namepathpair file containing data domains and data products to import; typically passed through using [transferfile()]. dataproductsupsertsemantic importsemantic whether to allow the creation of new domains and data products from the input csv, or ensure these are only updated if they already exist in atlan. dataproductsconfig configtype options to optimize how domains and data products are imported. dataproductsattrtooverwrite listing select attributes you want to clear (remove) from domains and data products if their value is blank in the provided file. dataproductsfailonerrors boolean whether an invalid value in a field should cause the import to fail (true) or log a warning, skip that value, and proceed (false). dataproductsfieldseparator string single character",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "used to separate fields in the input file (for example, , or ; ). dataproductscmhandling custommetadatasemantic how custom metadata in the input should be handled: ignore it, merge it with any existing asset custom metadata, or overwrite the existing asset custom metadata. dataproductstaghandling atlantagsemantic how atlan tags on assets in the input should be handled: ignore them, append them to any existing asset tags, replace the existing asset tags, or remove them from the assets. dataproductslinkidempotency linkidempotencyinvariant how linked resources on assets in the input should be updated: based on their unique url or their unique name. dataproductsbatchsize int maximum number of rows to process at a time (per api request). tag (structural) definitions variable name data type description tagsfile frameworkrenderer.namepathpair file containing tag definitions to manage; typically passed through using transferfile() . tagsconfig configtype options to optimize how tag definitions are imported. tagsfailonerrors boolean whether an invalid value in",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "a field should cause the import to fail (true) or log a warning, skip that value, and proceed (false). tagsfieldseparator string single character used to separate fields in the input file (for example, , or ; ). tagsbatchsize int maximum number of rows to process at a time (per api request). all other assets variable name data type description assetsfile frameworkrenderer.namepathpair file containing assets to import, typically passed through using transferfile() . assetsupsertsemantic importsemantic whether to allow the creation of new assets from the input csv (full or partial assets), or ensure assets are only updated if they already exist in atlan. assetsdeltasemantic deltasemantic whether to treat the input file as an initial load, full replacement (deleting any existing assets not in the file) or only incremental (no deletion of existing assets). assetsdeltaremovaltype removaltype how to delete any assets not found in the latest file. assetsdeltareloadcalculation reloadcalculation which assets to",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "reload from the latest input csv file. changed assets only will calculate which assets have changed between the files and only attempt to reload those changes. assetspreviousfiledirect frameworkrenderer.namepathpair path to a direct file (locally) to use for delta processing. note: providing a value for this will ignore any other previously processed file in the object store, so please be sure this is the option you want to use (should be rare). assetspreviousfileprefix string object store prefix in which previous files exist for delta processing. assetsconfig configtype options to optimize how assets are imported. assetsattrtooverwrite listing list of attributes you want to clear (remove) from assets if their value is blank in the provided file. assetsfailonerrors boolean whether an invalid value in a field should cause the import to fail (true) or log a warning, skip that value, and proceed (false). assetscasesensitive boolean whether to use case sensitive matching when running",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "in update only mode (true) or try case insensitive matching (false). assetstableviewagnostic boolean whether to treat tables, views and materialized views as interchangeable (true) or strictly adhere to specified types in the input (false). assetsfieldseparator string single character used to separate fields in the input file (for example, , or ; ). assetscmhandling custommetadatasemantic how custom metadata in the input should be handled: ignore it, merge it with any existing asset custom metadata, or overwrite the existing asset custom metadata. assetstaghandling atlantagsemantic how atlan tags on assets in the input should be handled: ignore them, append them to any existing asset tags, replace the existing asset tags, or remove them from the assets. assetslinkidempotency linkidempotencyinvariant how linked resources on assets in the input should be updated: based on their unique url or their unique name. assetsbatchsize int maximum number of rows to process at a time (per api request). trackbatches",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "boolean whether to track details about every asset across batches (true) or only counts (false). relationalassetsbuilder the relationalassetsbuilder publish config accepts the following configurable inputs. you can use any combination of these, and any you leave out will use sensible defaults. relational assets variable name data type description assetsfile frameworkrenderer.namepathpair file containing assets to import, typically passed through using transferfile() . assetsupsertsemantic importsemantic whether to allow the creation of new (full or partial) assets from the input csv, or ensure assets are only updated if they already exist in atlan. deltasemantic deltasemantic whether to treat the input file as an initial load, full replacement (deleting any existing assets not in the file) or only incremental (no deletion of existing assets). deltaremovaltype removaltype how to delete any assets not found in the latest file. deltareloadcalculation reloadcalculation which assets to reload from the latest input csv file. changed assets only will calculate",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "which assets have changed between the files and only attempt to reload those changes. previousfiledirect frameworkrenderer.namepathpair path to a direct file (locally) to use for delta processing. note: providing a value for this will ignore any other previously processed file in the object store, so please be sure this is the option you want to use (should be rare). assetsattrtooverwrite listing list of attributes you want to clear (remove) from assets if their value is blank in the provided file. assetsfailonerrors boolean whether an invalid value in a field should cause the import to fail (true) or log a warning, skip that value, and proceed (false). assetsfieldseparator string single character used to separate fields in the input file (for example, , or ; ). assetsbatchsize int maximum number of rows to process at a time (per api request). assetscmhandling custommetadatasemantic how custom metadata in the input should be handled: ignore",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "it, merge it with any existing asset custom metadata, or overwrite the existing asset custom metadata. assetstaghandling atlantagsemantic how atlan tags on assets in the input should be handled: ignore them, append them to any existing asset tags, replace the existing asset tags, or remove them from the assets. trackbatches boolean whether to track details about every asset across batches (true) or only counts (false). lineagebuilder the lineagebuilder publish config accepts the following configurable inputs. you can use any combination of these, and any you leave out will use sensible defaults. lineage assets variable name data type description lineagefile frameworkrenderer.namepathpair file containing lineage to import, typically passed through using transferfile() . lineageupsertsemantic importsemantic whether to allow the creation of new (full or partial) assets from the input csv, or ensure assets are only updated if they already exist in atlan. lineagefailonerrors boolean whether an invalid value in a field should",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "cause the import to fail (true) or log a warning, skip that value, and proceed (false). fieldseparator string single character used to separate fields in the input file (for example, , or ; ). batchsize int maximum number of rows to process at a time (per api request). cmhandling custommetadatasemantic how custom metadata in the input should be handled: ignore it, merge it with any existing asset custom metadata, or overwrite the existing asset custom metadata. taghandling atlantagsemantic how atlan tags on assets in the input should be handled: ignore them, append them to any existing asset tags, replace the existing asset tags, or remove them from the assets. 2025 03 12 2025 03 12 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/custom-package/define/"
  },
  {
    "text": "render your package developer skip to content render your package full example (expand for details) following is the complete package file for the running example, without any comments, in case you want to try it yourself as a sort of hello world example: mycustompackage.pkl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 amends package: developer.atlan.com toolkits custom package config 5.0.3 framework.pkl import pkl:semver import package: developer.atlan.com toolkits custom package config 5.0.3 connectors.pkl packageid csa openapi",
    "source": "https://developer.atlan.com/toolkits/custom-package/render/"
  },
  {
    "text": "spec loader packagename openapi spec loader version semver . version ( 1.0.0 ) description loads api specs and paths from an openapi (v3) definition. iconurl http: assets.atlan.com assets apispec.png docsurl https: developer.atlan.com samples loaders openapi implementationlanguage kotlin containerimage ghcr.io atlanhq atlan kotlin samples: (version) containercommand dumb init java openapispecloaderkt outputs files [ debug logs ] tmp debug.log keywords kotlin crawler openapi preview true connectortype connectors . api uiconfig tasks [ configuration ] description openapi spec configuration inputs [ spec url ] new textinput title specification url required true helptext full url to the json form of the openapi specification. placeholdertext https: petstore3.swagger.io api v3 openapi.json [ connection ] description connection details inputs [ connection usage ] new radio title connection required true possiblevalues [ create ] create [ reuse ] reuse default reuse fallback default helptext whether to create a new connection to hold these api assets, or reuse an",
    "source": "https://developer.atlan.com/toolkits/custom-package/render/"
  },
  {
    "text": "existing connection. [ connection ] new connectioncreator title connection required true helptext enter details for a new connection to be created. [ connection qualified name ] new connectionselector title connection required true helptext select an existing connection to load assets into. rules new uirule wheninputs [ connection usage ] reuse required connection qualified name new uirule wheninputs [ connection usage ] create required connection render through pkl once your package is defined, you can then render it into the files atlan needs using the pkl cli: pkl eval mycustompackage.pkl m . this will generate multiple yaml files representing the package, in the folder structure required by atlan, ready to be submitted in a pr. output produced rendering the package will create various files and subdirectories under the output directory you specify (the location you specify for m ), depending on the implementationlanguage you defined in your package: python kotlin requirements.txt",
    "source": "https://developer.atlan.com/toolkits/custom-package/render/"
  },
  {
    "text": "(1) requirements dev.txt (2) version.txt (3) dockerfile (4) package name init .py (5) logging.conf (6) main.py.example (7) package name cfg.py (8) build (9) package package name package.json (10) index.js (11) configmaps default.yaml (12) templates default.yaml (13) minimal dependencies for a python based package (you can of course extend this with other dependencies if your package requires them). minimal dependencies for testing a python based package (you can of course extend this with other dependencies if your package requires them). version of the python package. default container image file for a python package. empty init file for python. default logging configuration for python to separate info and debug level logging. skeletal starting point for a main program using the runtime toolkit. a strongly typed class capturing all the configuration details a user could provide, which we can use with the package s runtime toolkit. the build subdirectory will contain the artifacts",
    "source": "https://developer.atlan.com/toolkits/custom-package/render/"
  },
  {
    "text": "needed by argo to deploy your package. the package.json contains descriptive metadata about your package, such as its name, description, icon, and documentation links. the index.js is a placeholder file, which should be left as is. your package s ui configuration is bundled into this configmaps default.yaml file. your package s orchestration is bundled into this templates default.yaml file. src main kotlin packagename cfg.kt (1) build (2) package package name package.json (3) index.js (4) configmaps default.yaml (5) templates default.yaml (6) the src subdirectory will contain a generated kotlin class for transferring the ui based inputs to your code (if the you have configured your package s implementationlanguage as kotlin ). the build subdirectory will contain the artifacts needed by argo to deploy your package. the package.json contains descriptive metadata about your package, such as its name, description, icon, and documentation links. the index.js is a placeholder file, which should be",
    "source": "https://developer.atlan.com/toolkits/custom-package/render/"
  },
  {
    "text": "left as is. your package s ui configuration is bundled into this configmaps default.yaml file. your package s orchestration is bundled into this templates default.yaml file. output produced for our running example (since the implementationlanguage is kotlin ), this would produce: src main kotlin openapispecloadercfg.kt build package csa openapi spec loader package.json index.js configmaps default.yaml templates default.yaml 2025 03 12 2025 03 12 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/custom-package/render/"
  },
  {
    "text": "develop your logic developer skip to content develop your package s logic manage dependencies to start implementing your custom logic, we highly recommend using one of our sdks: python kotlin in python, a requirements.txt was rendered for you with the bare minimal set of dependencies (the python sdk): requirements.txt 1 pyatlan (1)! you can of course add other lines to this file to include other third party dependencies and libraries, or to restrict to the use of a specific version of even pyatlan . in kotlin, we recommend using the gradle build tool: build.gradle.kts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 plugins kotlin ( jvm ) version 1.9.24 (1) id ( jvm test",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "suite ) id ( com.adarshr.test logger ) version 4.0.0 id ( org.pkl lang ) version 0.25.3 id ( com.diffplug.spotless ) version 6.21.0 id ( com.github.johnrengelman.shadow ) version 7.1.2 (2) dependencies implementation ( com.atlan:atlan java: ) (3) implementation ( com.atlan:package toolkit runtime: ) implementation ( com.atlan:package toolkit config: ) implementation ( io.github.microutils:kotlin logging jvm:3.0.5 ) testimplementation ( com.atlan:package toolkit testing: ) testimplementation ( org.jetbrains.kotlin:kotlin test:1.9.24 ) runtimeonly ( org.apache.logging.log4j:log4j core:2.23.0 ) (4) runtimeonly ( org.apache.logging.log4j:log4j slf4j2 impl:2.23.0 ) implementation ( io.swagger.parser.v3:swagger parser:2.1.20 ) (5) tasks shadowjar (6) iszip64 true dependencies (7) include ( dependency ( io.swagger.parser.v3:swagger parser:. )) include ( dependency ( io.swagger.core.v3:swagger models:. )) include ( dependency ( io.swagger.core.v3:swagger core:. )) include ( dependency ( io.swagger.parser.v3:swagger parser core:. )) include ( dependency ( io.swagger.parser.v3:swagger parser v3:. )) include ( dependency ( io.swagger.parser.v3:swagger parser safe url resolver:. )) include ( dependency ( io.swagger.core.v3:swagger annotations:. )) include ( dependency ( com.fasterxml.jackson.dataformat:jackson dataformat",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "yaml:. )) include ( dependency ( com.fasterxml.jackson.datatype:jackson datatype jsr310:. )) include ( dependency ( org.yaml:snakeyaml:. )) include ( dependency ( org.apache.commons:commons lang3:. )) mergeservicefiles () jar (8) actions listof () dolast shadowjar these plugins are the minimum necessary to develop a kotlin based package. the shadow plugin is necessary when you want to bundle additional dependencies for your code that are not part of the out of the box java sdk or runtime toolkit. these dependencies are the minimum necessary to develop a kotlin based package using the sdk and package toolkits. you must provide some binding for slf4j logging. this example shows how to bind log4j2 , but you could replace this with some other log binding if you prefer. you can of course add other lines to this file to include other third party dependencies and libraries, or to restrict to the use of a specific version of",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "even the java sdk. in this example, we are using a third party library for parsing the openapi specification, from swagger. when using external dependencies, use the shadowjar task to define all the dependencies that should be bundled together into your .jar file. list the dependencies themselves in the inner dependencies section. override the default jar task so that you get the shadowed jar (with all the dependencies) as the only jar output. implement custom logic naturally your custom logic will depend on your use case. however, there is a standard pattern to help you get started in particular, to use the runtime portion of the package toolkit. this will handle common things like: receiving input values from what the user has entered in the ui (strongly typed in your code) setting up standard logging etc delegate publishing where possible you can now delegate publishing of assets to another package",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "to simplify the logic of your own package. if you use this delegation, remember your package only needs to produce the csv output it does not need to create or save any assets directly in atlan. python kotlin main.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from open api spec loader.open api spec loader cfg import runtimeconfig (1) import logging logger logging . getlogger ( name ) (2) def main (): (3) runtime config runtimeconfig () (4) custom config runtime config . custom config (5) spec url custom config . spec url further parameter retrieval and or custom logic logger . info ( doing some further custom logic... ) (6) if name main : main () you will always use these imports for setting up the runtime portion of the package toolkit. replace the import according to your package of course,",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "keep in mind that the specific name of the module and class within it will vary based on the name of your package. you should initialize a logger for your package. you need an executable file in python. use the runtimeconfig() method to retrieve all the runtime information, including inputs provided in the ui by a user. from the runtime configuration, you can retrieve the custom config (the inputs provided in the ui by a user). strongly types inputs this returns an object of the type of the class generated for you when you render your package. this class strongly types all of the inputs a user provides into things like numbers, booleans, strings, lists, and even full connection objects. (without it you re left to parse all of that yourself.) when you log information, the following apply: info level and above ( warn , error , etc) are all",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "output to the console. only these will appear when a user clicks the overall logs button for a package s run. use info for user targeted messages for this reason, we recommend using info level logging for tracking overall progress of your package s logic. keep it simple and not overly verbose to avoid overwhelming users of the package. debug level is not printed out to the console, but captured in a file. to allow users to download this debug log, you must define an output file mapped to tmp debug.log (like in line 22 of define overall metadata ). use debug for troubleshooting details with this separation, you can capture details that would be useful for troubleshooting in debug level without overwhelming users with that information. openapispecloader.kt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import com.atlan.pkg.utils (1) import mu.kotlinlogging object",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "openapispecloader (2) private val logger utils . getlogger ( this . javaclass . name ) (3) jvmstatic fun main ( args : array ) (4) val config utils . initializecontext (). use ctx (5) val specurl ctx . config . specurl (6) further parameter retrieval and or custom logic logger . info doing some further custom logic... (7) you will always use these imports for setting up the runtime portion of the package toolkit. you need an executable object in kotlin. what you name it here will need to match your containercommand when you define overall metadata of your package. you should initialize a logger for your package. use this method to initialize your logger use this utils.getlogger() method to ensure your logger is initialized and set up for use with opentelemetry. this will ensure all of the logging for your package run is tracked and traceable for troubleshooting purposes.",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "you must implement a jvmstatic main method, with this precise signature. more details you don t actually need to parse or use the command line arguments, everything will be passed as an environment variable, but you still need to have this method signature.) use the utils.initializecontext () reified method to retrieve all of the inputs provided in the ui by a user. strongly types inputs this returns an object of the type within the , which is the class generated for you when you render your package. this class strongly types all of the inputs a user provides into things like numbers, booleans, strings, lists, and even full connection objects. (without it you re left to parse all of that yourself.) when you have defined fallback values in your config, you will have strongly typed, non null values for every input (minimally the value for fallback you specified in the",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "config, if a user has not selected anything in the ui). alternatively, you can also use the utils.getordefault(ctx.config. , ) method to give you a default value. empty inputs are null by default if the input in the ui is optional, and you have not specified any fallback in your pkl config, you will by default receive a null if the user did not enter any value into it, so utils.getordefault() allows you to force things into non null values. a common practice is to set the fallback configuration value to the same value you show in placeholdertext or have defined as the default , and then you do not need to use utils.getordefault() to ensure you have a non null value. when you log information, the following apply: info level and above ( warn , error , etc) are all output to the console. only these will appear when",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "a user clicks the overall logs button for a package s run. use info for user targeted messages for this reason, we recommend using info level logging for tracking overall progress of your package s logic. keep it simple and not overly verbose to avoid overwhelming users of the package. debug level is not printed out to the console, but captured in a file. to allow users to download this debug log, you must define an output file mapped to tmp debug.log (like in line 22 of define overall metadata ). use debug for troubleshooting details with this separation, you can capture details that would be useful for troubleshooting in debug level without overwhelming users with that information. bundle into a container packages run as workflows using argo . so before you can run your package in an atlan tenant, it must be built into a self contained container image",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "which argo can then orchestrate. to bundle your package into a container image: python kotlin ensure you first render your package . this will output a dockerfile you can at least use as a starting point. build your container image from the dockerfile (must be run in the same directory as the dockerfile ): podman build . t openapi spec loader:latest publish your container image to a registry from which it can then be pulled by a tenant: podman push ghcr.io atlanhq openapi spec loader:latest (1)! you will likely need to first authenticate with the remote registry, which is beyond the scope of this document to explain. automate the build and publish via ci cd we highly recommend automating the container image build and publication via ci cd. for example, a github action like the following should do this: publish.yml 1 2 3 4 5 6 7 8 9 10",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 name : publish on : push : branches : [ main ] jobs : custom package image : (1) runs on : ubuntu latest name : publish container steps : uses : actions checkout v4 uses : docker setup buildx action v2 (2) name : log in to container registry uses : docker login action v2 with : registry : ghcr.io username : github.actor password : secrets.github token name : set image tag from file (3) id : set image tag run : tag (cat . pkg version.txt) echo image tag tag github env name : build and publish container image uses : docker build push action v4 with : build args : version env.image tag push : true (4) tags : ghcr.io atlanhq open api spec",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "loader: env.image tag , ghcr.io atlanhq open api spec loader:latest context : . pkg (5) platforms : linux amd64 you can run a single job to both build and publish the container image. use docker s own github actions to set up the ability to build container images, login to the private github registry, etc. set the version number for your package from the version.txt file. to ensure your image is published, not only built, you must set push: true . the context in which you run the container build must include the dockerfile you constructed earlier (in this example, that dockerfile resides in the github repository at this location: . pkg , so the earlier actions checkout v4 action ensures it exists here). build your package .jar file (assuming you followed the gradle approach outlined in manage dependencies ): . gradlew assemble shadowjar create a dockerfile that builds on",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "the ghcr.io atlanhq atlan java base image: dockerfile 1 2 3 arg version from ghcr.io atlanhq atlan java: version copy assembly opt jars create a sub directory called assembly under the directory where you created the dockerfile , and copy over the .jar file you built to this assembly sub directory: mkdir assembly cp ... openapi spec loader .jar assembly . build your container image from the dockerfile (must be run in the same directory as the dockerfile ): podman build . t openapi spec loader:latest publish your container image to a registry from which it can then be pulled by a tenant: podman push ghcr.io atlanhq openapi spec loader:latest (1)! you will likely need to first authenticate with the remote registry, which is beyond the scope of this document to explain. automate the build and publish via ci cd we highly recommend automating the container image build and publication",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "via ci cd. for example, a github action like the following should do this: publish.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 name : publish on : push : branches : [ main ] jobs : merge build : (1) runs on : ubuntu latest name : build steps : uses : actions checkout v4 uses : actions setup java v4 with : java version : 17 distribution : temurin name : check formatting run : . gradlew check name : build artifacts run : . gradlew assemble shadowjar env : gh username : github.actor gh token : secrets.github",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "token uses : actions upload artifact v4 (2) with : name : openapi spec loader path : jars openapi spec loader .jar custom package image : (3) runs on : ubuntu latest name : publish container needs : merge build (4) steps : uses : actions checkout v4 uses : docker setup buildx action v2 (5) name : log in to container registry uses : docker login action v2 with : registry : ghcr.io username : github.actor password : secrets.github token name : create assembly area (6) run : mkdir p . containers custom package assembly uses : actions download artifact v4 (7) with : name : openapi spec loader path : . containers custom package assembly name : build and publish container image uses : docker build push action v4 (8) with : build args : version 1.13.0 (9) push : true (10) tags : ghcr.io atlanhq openapi spec",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "loader:1.13.0, ghcr.io atlanhq openapi spec loader:latest context : . containers custom package (11) platforms : linux amd64 we recommend separating the code compilation job (here) from the container image build and publish (next job). at the end of the code compilation job, you can upload the artifact ( .jar file) that it produces to github itself. then you can run the separate container image build and publish job. ensure the container image build and publish job depends on the code already being successfully compiled and .jar file being uploaded. use docker s own github actions to set up the ability to build container images, login to the private github registry, etc. we recommend creating a directory where you can assemble all the pieces of the container image. you can then download the .jar file produced by the first job into this assembly directory. you can then build the container image",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "from this assembly directory. you probably want this version to come from some variable or input. to ensure your image is published, not only built, you must set push: true . the context in which you run the container build must include the dockerfile you constructed earlier (in this example, that dockerfile resides in the github repository at this location: . containers custom package , so the earlier actions checkout v4 action ensures it exists here). 2025 03 12 2025 03 12 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/custom-package/develop/"
  },
  {
    "text": "test your package developer skip to content test your package s logic through testing frameworks 4.0.0 2.4.5 you should start by testing your package s logic using standard testing frameworks. these will allow you to run automated regression tests to ensure that your package s logic behaves as you intend as both the package and any of its dependencies evolve. in fact, you can even start with testing this way before you ve bundled up your package into a container image and merged it into atlanhq marketplace packages ! python kotlin in python, we use the pytest testing framework to write integration tests. make sure it s installed in your environment: requirements dev.txt 1 2 3 4 pytest pytest plugins (optional) pytest order pytest sugar pip install r requirements dev.txt tests test package.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 from typing import generator import pytest from open apispec loader.main import main from open apispec loader.open apispec loader cfg import customconfig , runtimeconfig from pyatlan.client.atlan import atlanclient from pyatlan.model.assets import connection from pyatlan.model.enums import atlanconnectortype from pyatlan.test utils import ( testid , create connection , delete asset , validate error free logs , validate files exist , ) test id testid . make unique ( oapi ) (1) files [ tmp debug.log , tmp pyatlan.json ] (2) pytest . fixture ( scope module ) def client () atlanclient : (3) return atlanclient () pytest",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": ". fixture ( scope module ) def connection ( client : atlanclient ) generator [ connection , none , none ]: (4) result create connection ( client client , name test id , connector type atlanconnectortype . api ) yield result delete asset ( client , guid result . guid , asset type connection ) pytest . fixture ( scope function ) def custom config ( monkeypatch , connection ): (5) custom config customconfig ( spec url https: petstore3.swagger.io api v3 openapi.json , connection usage create , connection connection , ) runtime config runtimeconfig ( custom config custom config ) for key , value in runtime config . envars as dict . items (): monkeypatch . setenv ( key , value ) class testpackage : (6) def test main ( self , caplog , custom config : customconfig , ): main () assert f starting execution of open apispec loader...",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "in caplog . text validate files exist ( files ) (7) validate error free logs ( files ) (8) class testconnection : (9) def test connection ( self , client : atlanclient , connection : connection ): results client . asset . find connections by name ( name test id , connector type atlanconnectortype . api ) assert results assert len ( results ) 1 assert results [ 0 ] . name test id class testprocessor : (10) def test process ( self ): pass use the built in testid.make unique() method to create a unique id for the test run. this appends some randomly generated characters onto the string you provide to ensure each run of the test is unique. use this generated id for all objects your test creates to ensure your test is appropriately isolated from other tests (and possible later runs of the same test), use",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "this generated id in the naming of all of the objects your test creates. this will ensure it does not clobber, conflict or overlap with any other tests or test runs that might happen in parallel. provide a list of file paths to the log files that need to be validated. instead of duplicating code across tests, create fixtures and attach these functions to the tests. pytest.fixture() run before each test, providing the necessary data or setup for the test. when creating fixtures for atlan assets (e.g: connection ), ensure that you call the delete asset() utility function after yield to clean up the test object upon test completion. create a customconfig fixture for your test package with test values. use monkeypatch.setenv(key, value) to patch runtimeconfig environment variables. this approach is useful for testing code that depends on environment variables without altering the actual system environment. a common pattern is",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "to create a test class, such as testpackage , with methods that directly invoke the main() function of your package ( main.py ). this simulates running your package in a test environment. it is also common to include a method that calls the utility function validate files exist() to ensure that certain files are created by the package. additionally, include a method that calls the utility function validate error free logs() to verify that there are no error level messages in the log files generated by the package. optionally, you can create multiple test classes and methods to cover various conditions for the package. for example: testconnection class can be used to test connection functionality. optionally, you can create multiple test classes and methods to cover various conditions for the package. for example: testprocessor class can include methods that call the package s process.process() method (if implemented) to validate different",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "processing logic within your package. in kotlin, to write an integration test you need to extend the package toolkit s packagetest class. use ppackagetests option to run the test by default, integration tests will be skipped, since they require first setting up appropriate connectivity to an atlan tenant to run. if you want to run them, you need to pass the ppackagetests argument to gradle. src test kotlin importpetstoretest.kt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import com.atlan.atlan import com.atlan.model.assets.connection import com.atlan.model.enums.atlanconnectortype import com.atlan.pkg.packagetest import org.testng.assert.assertfalse import org.testng.assert.asserttrue import org.testng.itestcontext import org.testng.annotations.afterclass import org.testng.annotations.beforeclass import kotlin.test.test import kotlin.test.assertequals import kotlin.test.assertnotnull class importpetstoretest : packagetest",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "( x ) (1) private val testid makeunique ( oapi ) (2) private val files listof ( debug.log , ) override fun setup () (3) runcustompackage ( (4) openapispecloadercfg ( (5) specurl https: petstore3.swagger.io api v3 openapi.json , connectionusage create , connection connection . creator ( client , testid , atlanconnectortype . api ). build (), ), openapispecloader :: main , (6) ) override fun teardown () (7) removeconnection ( testid , atlanconnectortype . api ) (8) test (9) fun connectioncreated () val results connection . findbyname ( testid , atlanconnectortype . api ) assertnotnull ( results ) assertequals ( 1 , results . size ) assertequals ( testid , results [ 0 ] . name ) test (10) fun filescreated () validatefilesexist ( files ) test fun errorfreelog () (11) validateerrorfreelog () extend the built in packagetest class to define a package test. provide it a unique string to",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "distinguish it from other integration tests. use the built in makeunique() method to create a unique id for the test run. this appends some randomly generated characters onto the string you provide to ensure each run of the test is unique. use this generated id for all objects your test creates to ensure your test is appropriately isolated from other tests (and possible later runs of the same test), use this generated id in the naming of all of the objects your test creates. this will ensure it does not clobber, conflict or overlap with any other tests or test runs that might happen in parallel. override the setup() method to set up any necessary prerequisites for your integration test (such as creating any objects it will rely on when it runs). call the runcustompackage() method to actually run your package, with a predefined set of inputs and configuration. pass",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "the runcustompackage() method a new configuration object specific to your package. this simulates the hand off from the ui for your package to your code. in this example, we create a new configuration for the openapispecloadercfg with the settings we want to test. you also need to pass the runcustompackage() method the entry point for your package (usually just its main method). any integration test that actually creates some objects in the tenant (whether as part of the prerequisites, or the actual running of the package), should override the teardown() method and implement any cleanup of created or side effected objects. do this just after setup while this overridden teardown() method can technically be defined anywhere, it is a good practice to define it just after the setup() . this helps keep clear what has been created or side effected in the setup() with what needs to then be cleaned",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "up in the teardown() . you can use built in operations like removeconnection() to remove all assets that were created within (and including) a connection. you can then use as many test annotated methods as you like to test various conditions of the result of running your package. these will only execute after the beforeclass method s work is all completed. a common pattern is to include a method that calls the built in validatefilesexist() method to confirm that certain files are created by the package. another common pattern is to include a method that calls the built in validateerrorfreelog() method to confirm there are no error level messages in the log file that is generated by the package. (optional) writing tests for non toolkit based scripts you can write integration tests for existing scripts in the marketplace csa scripts repository, even if they are not based on package toolkits.",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "these tests help verify script behavior end to end in a real atlan tenant. we ll begin by performing minimal refactoring of the existing script, as it s necessary to enable writing integration tests. step 1: rename directory to snake case if the script is in kebab case directory, convert it to snake case . do this just after renaming update references in mkdocs.yml , delete the old directory, and verify imports links still work. for example: before: scripts designation based group provisioning main.py index.md tests test main.py after: scripts designation based group provisioning main.py index.md tests test main.py step 2: refactor main.py do refactor the script without altering logic or flow. wrap all logic inside functions. create a single entry point: main(args: argparse.namespace) call helper functions from main() each should receive only required args or inputs . do not rename or restructure existing functions. change the sequence or logic",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "flow. modify argument parsing. add remove logging unless required for debugging. for example main.py : def load input file ( file : any ): pass def do something with file ( client : atlanclient , file : any ): pass def main ( args : argparse . namespace ): client get client ( impersonate user id args . user id ) client set package headers ( client ) file load input file ( args . input file ) do something with file ( client , file ) if name main : parser argparse . argumentparser () parser . add argument ( user id , required true ) parser . add argument ( input file , required true ) args parser . parse args () main ( args ) step 3: add integration tests before writing tests, make sure you ve installed the test dependencies in your local environment. you can",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "do that by running the following command: pip install e .[test] alternatively, you can explicitly install the required packages by creating a requirements test.txt file and installing them using: requirements dev.txt 1 2 3 4 5 6 pytest coverage pytest plugins (optional) pytest order pytest sugar pytest timer [ termcolor ] pip install r requirements test.txt test layout for test main.py create a tests folder if not already present: scripts my script main.py tests test main.py function purpose test main functions test small pure helper functions individually (useful for quick validation of logic) test main run the main() function with a config to simulate full script execution (end to end) test after main (optional) validate side effects after running the script, such as asset creation, retrieval, audit logs, etc. for example, you can refer to this real world integration test for designation based group provisioning main.py : recommended testing strategy",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "for scripts when writing integration tests for scripts in marketplace csa scripts , follow these practices to ensure reliable and production relevant test coverage: best practices avoid using mock , patch , or mocking pyatlan clients or any atlan interactions unless absolutely necessary. integration tests should interact with a real atlan tenant to validate actual behavior. use mocking or patching only (for example): external third party api calls database interactions not managed by atlan non deterministic behavior (e.g: random data, time based logic) use environment variables for all secrets and configuration values. load them via .env files, ci cd secrets, or shell configs never hardcode. things to avoid hardcoding sensitive values such as api keys, user specific secrets, or test asset names. instead, use environment variables and pyatlan.test utils like testid.make unique() to generate unique asset names and avoid naming collisions. ensure that test objects are generated in fixtures, which",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "can be reused across different tests, and cleaned up safely after tests are complete. using fake or placeholder data that doesn t reflect the actual structure or behavior of entities in atlan. always use data that closely mirrors production data for more meaningful tests. mocking pyatlan client methods integration tests must execute real operations against a live atlan tenant to ensure validity and detect regressions. mocking undermines the purpose of integration testing. full example (expand for details) test main.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 import pytest from types import simplenamespace from pyatlan.pkg.utils import get client , set package headers import pandas as pd from scripts.designation based group provisioning.main import ( review groups , get default groups , get ungrouped users , map users by designation , main , ) from pyatlan.model.group import atlangroup , creategroupresponse from pyatlan.client.atlan import atlanclient from pyatlan.test utils import testid from typing import generator import os from pathlib import path test path path ( file ) . parent test group name testid . make unique ( csa dbgp test ) pytest . fixture ( scope module",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": ") def config () simplenamespace : return simplenamespace ( user id os . environ . get ( atlan user id ), mapping file f test path test mapping.csv , missing groups handler skip , remove from default group , domain name mock tenant.atlan.com , ) pytest . fixture ( scope module ) def client ( config ): if config . user id : client get client ( impersonate user id config . user id ) else : client atlanclient () client set package headers ( client ) return client pytest . fixture ( scope module ) def group ( client : atlanclient ) generator [ creategroupresponse , none , none ]: to create atlangroup . create ( test group name ) g client . group . create ( group to create ) read the csv file df pd . read csv ( f test path mapping.csv ) replace values in",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "the group name column with the test group name df [ group name ] df [ group name ] . replace ( data engineers and scientists , test group name ) save the updated test csv df . to csv ( f test path test mapping.csv , index false ) assert os . path . exists ( f test path test mapping.csv ) yield g client . group . purge ( g . group ) os . remove ( f test path test mapping.csv ) def test main functions ( config : simplenamespace , client : atlanclient , group : atlangroup , caplog : pytest . logcapturefixture , ): test configuration validation assert config . mapping file . endswith ( .csv ) test group review functionality verified groups review groups ( config . mapping file , config . missing groups handler , client ) assert caplog . records [ 0",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "] . levelname info assert source information procured. in caplog . records [ 0 ] . message assert isinstance ( verified groups , set ) default groups get default groups ( client ) assert caplog . records [ 6 ] . levelname info assert default groups found: in caplog . records [ 6 ] . message assert isinstance ( default groups , list ) and len ( default groups ) 0 groupless users get ungrouped users ( default groups default groups , client client ) assert isinstance ( groupless users , list ) and len ( groupless users ) 0 unmappable users map users by designation ( user list groupless users , mapping file config . mapping file , verified groups verified groups , client client , ) assert isinstance ( unmappable users , list ) and len ( unmappable users ) 0 def test main ( config : simplenamespace",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": ", client : atlanclient , group : atlangroup , caplog : pytest . logcapturefixture , ): test end to end main function execution main ( config ) verify expected log messages assert caplog . records [ 0 ] . levelname info assert sdk client initialized for tenant in caplog . records [ 0 ] . message assert input file path in caplog . records [ 1 ] . message assert source information procured. in caplog . records [ 2 ] . message assert total distinct groups in the input: in caplog . records [ 3 ] . message pytest . mark . order ( after test main ) def test after main ( client : atlanclient , group : creategroupresponse ): result client . group . get by name ( test group name ) assert result and len ( result ) 1 test group result [ 0 ] assert test",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "group . path assert test group . name assert test group . id group . group assert test group . attributes assert not test group . attributes . description make sure users are successfully assigned to the test group after running the workflow assert test group . user count and test group . user count 1 (optional) writing tests for non toolkit based scripts using cursor ai code editor you can leverage ai code editors like cursor to help with refactoring existing scripts and generating integration tests for the marketplace csa scripts repository. however, it s important to be aware of the potential issues and risks that may arise. step 1: setup cursor rules to ensure the ai agent provides the desired results based on your prompts, you need to set up custom rules for your code editor. create a rules file: create the file .cursor rules csa scripts tests.mdc",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "in your project directory. you can start by copying the example rule and modifying them to match your needs. refine rules over time: as you use ai for refactoring and generating tests, you can refine the rules. by adding more context (e.g: multiple packages and varied test patterns), the ai will become more effective over time, improving its results. step 2: running the agent with the defined rules to run the ai agent with the defined rules, follow these steps: open the cursor chat: press cmd l to open a new chat in the cursor ide. click on add context , then select csa scripts tests.mdc to load the rules you defined. provide a clear prompt: after loading the rules, provide a clear prompt like the following to refactor your script and add integration tests: refactor scripts asset change notification main.py using the latest cursor rules and add integration tests",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "in scripts asset change notification tests test main.py to ensure functionality and coverage. review results: once the ai completes the task, review the generated results carefully. you may need to accept or reject parts of the refactoring based on your preferences and quality standards. common issues low accuracy across models: ai results can be highly inconsistent, even after experimenting with different combinations of rules and prompts. in many cases, only a small fraction of attempts yield satisfactory results. inconsistent output: regardless of using detailed or minimal rules, and trying various ai models ( claude 3.7, sonnet 3.5, gemini, openai ), the output often lacks consistency, leading to unsatisfactory refactorings. risks in refactoring code deletion: ai can unintentionally remove important parts of the original code during refactoring. unnecessary code addition: ai might add code that changes the behavior of the script, potentially introducing bugs. flaky or insufficient tests: generated tests are",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "often overly simplistic or unreliable. ai may also mock components that should not be mocked, leading to incomplete test coverage. live on a tenant you should then test the package live on a tenant. this will confirm: the ui renders as you intend, any inputs provided by the user through the ui are properly handed off to your logic, and your bundled package is orchestrated successfully through atlan s back end workflow engine (argo). deploy the package kubectl github if you have kubectl access to your cluster, you can selectively deploy your package directly: ensure you are on your cluster: loft use vcluster project default (1)! replace with the name of your tenant. (this assumes you are already logged in to loft naturally log in there first, if you are not already.) (one off) install node , if you do not already have npm available: brew install node install the",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "latest version of argopm : npm i g argopm deploy the package from its rendered output directory: argopm install . n default c force (1)! if you are not in the output directory where your package was rendered, replace the . with the directory path for the rendered output. package must first be generally available to follow these steps, you must first make your package generally available . (generally available in this sense just means it is available to be deployed it is not actually deployed to any tenant by default.) if you do not have kubectl access to your cluster, you will need to selectively deploy the package through the atlanhq marketplace packages repository. clone atlanhq marketplace packages to your local machine (if you have not already): git clone git github.com:atlanhq marketplace packages.git (1)! cd marketplace packages this assumes you have configured your git client with appropriate credentials. if",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "this step fails, you ll need to setup git first. start from an up to date master branch (in particular if you already have the repository cloned locally): git checkout master git merge origin master create a branch in the local repository: git branch jira task id (1)! git checkout jira task id replace jira task id with the unique id of the task in jira where you are tracking your work. create or edit the file deployment tenants .pkl for the tenant where you want to deploy the package, with at least the following content: deployment tenants .pkl 1 2 3 4 5 amends .. deployment.pkl include [ csa openapi spec loader ] (1)! of course, use your own package s id in place of csa openapi spec loader . stage your new (or modified) .pkl file: git add deployment tenants .pkl (1)! remember to replace with your actual",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "tenant name. (this tells git which files to include all together in your next commit.) commit your new (or modified) file to the branch: git commit m package deployment for ... (1)! provide a meaningful message for the new package you re deploying. (this tells git to take a (local) snapshot of all the changes you staged (above).) push your committed changes to the remote repository: git push set upstream origin jira task id (1)! remember that jira task id is just a placeholder replace with the name of your actual branch. (this tells git to push all the (local) commits you ve made against this branch to the remote github repository, so they re available to everyone there.) raise a pull request (pr) from your branch ( jira task id ) to master on atlanhq marketplace packages . will be auto approved as long as you have named the",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "file correctly and written valid contents, it will be auto approved by a bot. once auto approved, you can self merge to master . 1 once the pr is merged, wait for the atlan update script to run and complete on your tenant. by default it will run every 30 minutes, so could take up to 1 hour before it has completed on your tenant. 2 test the package now that the package is deployed on your tenant: hover over the new button in the upper right, and then click new workflow . select the pill that matches the name of the category you specified for your package. (if you did not specify one, it should be under custom , by default.) select the tile for your package, and then click the setup workflow button in the upper right. fill in appropriate inputs to the ui to configure your package,",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "click next through each step (if more than one), and finally run the package. running example for our running example, this would produce the following ui: confirm: the inputs shown in the ui are as you expect, in particular if you use any rules to limit what inputs are shown. the values you provided in the inputs are picked up by your custom logic and influence how the package behaves. your package runs to completion when you provide valid inputs. your package fails with an error when you provide inputs it cannot use to run successfully. if it fails, double check you have the correct filename, which must end in .pkl . it is also possible that synchronization has been disabled on your tenant, in which case atlan update may not run at all. if that is the case, you will need to speak with whoever manages your tenant to",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "see how you can test your package. 2025 03 12 2025 04 18 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/custom-package/test/"
  },
  {
    "text": "release (ga) the package developer skip to content release (ga) the package for your package to actually be usable by others, it needs to be committed to an internal atlan repository. add to atlanhq marketplace packages commit the package s definition to the atlanhq marketplace packages repository. clone atlanhq marketplace packages to your local machine (if you have not already): git clone git github.com:atlanhq marketplace packages.git (1)! cd marketplace packages this assumes you have configured your git client with appropriate credentials. if this step fails, you ll need to setup git first. start from an up to date master branch (in particular if you already have the repository cloned locally): git checkout master git merge origin master create a branch in the local repository: git branch jira task id (1)! git checkout jira task id replace jira task id with the unique id of the task in jira where you",
    "source": "https://developer.atlan.com/toolkits/custom-package/release/"
  },
  {
    "text": "are tracking your work. move your rendered package outputs to a new folder under packages csa : mv ... build package csa openapi spec loader packages csa openapi spec loader (1)! of course, use your own package s id in place of csa openapi spec loader . remove the prefix portion of the name from the folder also be careful to remove the csa prefix portion of the folder when you move it across. note in this example that the source folder of csa openapi spec loader becomes just openapi spec loader in the target directory within the repository. ensure your new package s directory and your github id are added to the end of the codeowners file. this will allow you to self manage any changes to the package going forward (without forcing you to have pr reviews): codeowners ... packages csa openapi spec loader cmgrote stage your new",
    "source": "https://developer.atlan.com/toolkits/custom-package/release/"
  },
  {
    "text": "(or modified) package files (and codeowners , if this is the first time you re committing the package): git add codeowners packages csa openapi spec loader (1)! remember to replace openapi spec loader with your actual package s folder. (this tells git which files to include all together in your next commit.) commit your new (or modified) files to the branch: git commit m feat: ... (1)! provide a meaningful message for the new package ( feat: ... ) or whatever changes you ve made to it ( fix: ... ). push your committed changes to the remote repository: git push set upstream origin jira task id (1)! remember that jira task id is just a placeholder replace with the name of your actual branch. (this tells git to push all the (local) commits you ve made against this branch to the remote github repository, so they re available to",
    "source": "https://developer.atlan.com/toolkits/custom-package/release/"
  },
  {
    "text": "everyone there.) raise a pull request (pr) from your branch ( jira task id ) to master on atlanhq marketplace packages . request someone review the pr by posting a simple message with a link to the pr on collab marketplace . only necessary the first time if you ve followed the instructions above on adding yourself to codeowners , this initial pr should be the only one that needs to be reviewed and approved. any future changes or updates you make to the package you should be able to merge yourself without any further pr reviews. deploy the package once merged to master of atlanhq marketplace packages , anyone who wants to use it can follow the steps for github under test your package, live on a tenant to deploy it to their tenant. 2025 03 12 2025 03 12 was this page helpful? thanks for your feedback! thanks",
    "source": "https://developer.atlan.com/toolkits/custom-package/release/"
  },
  {
    "text": "for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/custom-package/release/"
  },
  {
    "text": "package widgets developer skip to content widgets reference there are a number of input widgets you can use when defining your inputs. for more details about the configurable options of each of these individual widgets, use hover over context help in your ide when writing your package. apitokenselector apitokenselector 39 40 41 42 43 44 45 46 inputs [ var name a ] new apitokenselector title api token selector (title) required false helptext example for api token selector widget (helptext). showall true (1)! controls whether to show all api tokens (true) or only the api tokens created by the user configuring the package (false). booleaninput booleaninput 39 40 41 42 43 44 45 46 inputs [ var name b ] new booleaninput title boolean input (title) required false helptext example for boolean input widget (helptext). defaultselection true connectioncreator connectioncreator 39 40 41 42 43 44 45 46 inputs [ var",
    "source": "https://developer.atlan.com/toolkits/custom-package/widgets/"
  },
  {
    "text": "name c ] new connectioncreator title connection creator (title) required false helptext example for connection creator widget (helptext). width 8 connectionselector connectionselector 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 inputs [ var name d ] new connectionselector title connection selector single (title) required false helptext example for connection selector widget (helptext). multiselect false limittoconnectors snowflake (1)! [ var name e ] new connectionselector title connection selector multi (title) required false helptext example for connection selector widget (helptext). multiselect true limittoconnectors snowflake bigquery limits the connections listed to only those with the specified connector type, which can either be a single string or a list. (if this limittoconnectors is left out entirely, all connections will be included in the drop down.) connectortypeselector connectortypeselector 39 40 41 42 43 44 45 46 inputs [ var name f ] new connectortypeselector title connector type",
    "source": "https://developer.atlan.com/toolkits/custom-package/widgets/"
  },
  {
    "text": "selector (title) required false helptext example for connector type selector widget (helptext). width 8 credentialinput credentialinput 39 40 41 42 43 44 45 46 inputs [ var name g ] new credentialinput title credential (title) credtype atlan connectors snowflake (1) helptext example for connector type selector widget (helptext). allowtestauthentication true (2) the value used for credtype must match an existing credential defined in packages atlan connectors configmaps .yaml , whether pre existing or generated through the use of credentialconfig and placed there yourself. you can remove the green test authentication button by setting allowtestauthentication to false . (if left out entirely, it will default to true .) dateinput dateinput 39 40 41 42 43 44 45 46 47 48 49 inputs [ var name h ] new dateinput title date input (title) required false helptext example for date input widget (helptext). past 14 (1) future 14 (2) defaultday 1 (3)",
    "source": "https://developer.atlan.com/toolkits/custom-package/widgets/"
  },
  {
    "text": "width 8 an offset from today (0) that indicates how far back in the calendar can be selected ( 1 is yesterday, 1 is tomorrow, and so on). an offset from today (0) that indicates how far forward in the calendar can be selected ( 1 is yesterday, 1 is tomorrow, and so on). an offset from today that indicates the default date that should be selected in the calendar (0 is today, 1 is yesterday, 1 is tomorrow, and so on). dropdown dropdown 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 inputs [ var name i ] new dropdown title drop down single (title) required false helptext example for drop down widget (restricted to a single) (helptext). possiblevalues [ one ] one [ two ] two [ three ] three multiselect",
    "source": "https://developer.atlan.com/toolkits/custom-package/widgets/"
  },
  {
    "text": "false width 4 [ var name j ] new dropdown title drop down multi (title) required false helptext example for drop down widget (allowing multiple) (helptext). possiblevalues [ one ] one [ two ] two [ three ] three multiselect true width 4 filecopier filecopier 39 40 41 42 43 44 45 46 47 inputs [ var name k ] new filecopier title file copier (title) required false helptext example for file copier widget (helptext). placeholdertext path file.csv (placeholdertext) width 8 fileuploader fileuploader 39 40 41 42 43 44 45 46 47 48 49 50 inputs [ var name l ] new fileuploader title file uploader (title) required false helptext example for file uploader widget (helptext). placeholdertext sample csv file (placeholdertext) filetypes (1)! text csv width 8 a list of the mime types that the upload widget will accept. if someone tries to upload a file of a different type,",
    "source": "https://developer.atlan.com/toolkits/custom-package/widgets/"
  },
  {
    "text": "it will be rejected automatically by the ui itself. keygeninput keygeninput 39 40 41 42 43 44 45 46 inputs [ var name m ] new keygeninput title keygen input (title) required false helptext example for keygen input widget (helptext). width 8 multiplegroups singlegroup singlegroup multiplegroups 39 40 41 42 43 44 45 46 47 48 49 50 51 52 inputs [ var name n ] new singlegroup title single group (title) required false helptext example for single group widget (helptext). width 4 [ var name o ] new multiplegroups title multiple groups (title) required false helptext example for multiple groups widget (helptext). width 4 multipleusers singleuser singleuser multipleusers 39 40 41 42 43 44 45 46 47 48 49 50 51 52 inputs [ var name p ] new singleuser title single user (title) required false helptext example for single user widget (helptext). width 4 [ var name q",
    "source": "https://developer.atlan.com/toolkits/custom-package/widgets/"
  },
  {
    "text": "] new multipleusers title multiple users (title) required false helptext example for multiple users widget (helptext). width 4 numericinput numericinput 39 40 41 42 43 44 45 46 47 inputs [ var name r ] new numericinput title numeric input (title) required false helptext example for numeric input widget (helptext). placeholdervalue 50 width 8 passwordinput passwordinput 39 40 41 42 43 44 45 46 inputs [ var name s ] new passwordinput title password input (title) required false helptext example for password input widget (helptext). width 8 radio radio 39 40 41 42 43 44 45 46 47 48 49 50 51 inputs [ var name t ] new radio title radio (title) required false helptext example for radio widget (helptext). possiblevalues [ one ] one (value) [ two ] two (value) [ three ] three (value) default two textinput textinput 39 40 41 42 43 44 45 46 inputs",
    "source": "https://developer.atlan.com/toolkits/custom-package/widgets/"
  },
  {
    "text": "[ var name u ] new textinput title text input (title) required false helptext example for text input widget (helptext). width 8 2025 03 12 2025 03 12 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/custom-package/widgets/"
  },
  {
    "text": "typedef toolkit developer skip to content typedef toolkit with the typedef toolkit you can model your unique metadata and make it available as first class objects in atlan. we use a pkl model to restrict the way you define the custom model so that it precisely fits the best practices and structures available in atlan s underlying metadata model. overview of the process config: mirroractors: false sequencediagram actor you as you loop until testing is successful create participant ide as ide you ide: define your typedefs create participant pkl as pkl cli you pkl: render your typedefs destroy pkl pkl you: json and ux code create participant atl as atlan tenant you atl: deploy your typedefs you ide: implement sdk templates you ide: regenerate sdk bindings ide atl: atl ide: ide you: you ide: implement integration tests you ide: run integration tests ide atl: (breakpoint before cleanup) you atl: test",
    "source": "https://developer.atlan.com/toolkits/typedef/"
  },
  {
    "text": "ux (assets present, discoverable, etc) atl you: destroy atl atl ide: (continue from breakpoint, allowing cleanup) destroy ide ide you: end create participant git as github you )git: raise prs (release) 2024 03 14 2025 03 11 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/typedef/"
  },
  {
    "text": "running example developer skip to content running example running example to summarize the overall example we want to have customdataset , customtable and customfield assets. these assets are related to each other in a hierarchy of parent child relationships: customdataset to customtable , through customtables customtable to customfield , through customfields all of these assets have a number of attributes: some common (like customsourceid , customdatasetname and customdatasetqualifiedname ). others unique (like customtemperature and customratings ). some of these attributes themselves have a set of restricted values ( customtemperature ) or are a nested set of information ( customratings ). most of the attributes capture only a single value ( customsourceid and customtemperature ) but some allow multiple values ( customratings ). to help explain how this all works, we ll use this fictional example of a metadata model we want to set up: erdiagram connection ..o customdataset : customdataset",
    "source": "https://developer.atlan.com/toolkits/typedef/example/"
  },
  {
    "text": "string customsourceid string customdatasetname string customdatasetqualifiedname customdataset o customtable : customtables customtable string customsourceid string customdatasetname string customdatasetqualifiedname struct[] customratings customtable o customfield : customfields customfield string customsourceid string customdatasetname string customdatasetqualifiedname string tablename string tablequalifiedname enum customtemperature customtemperature(enum) o o customfield : customtemperature(enum) val hot highly available val cold offline storage customrating(struct) o o customtable : customrating(struct) string customratingfrom long customratingof 2025 03 11 2025 03 14 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google",
    "source": "https://developer.atlan.com/toolkits/typedef/example/"
  },
  {
    "text": "define typedefs via template developer skip to content define typedefs via template how to read this guide each section of this guide provides 3 tabs, which are linked throughout (once you swap in one section, all other sections will automatically reflect that same level of detail): simple when you are just starting out, follow these tabs to understand the basic structure of the toolkit and the fundamental elements that you must use. detailed as you start to wonder about additional complexity, consider changing to these tabs, which cover additional (optional) possibilities. with ux when you are ready to start experimenting with the user interface for your typedefs, use this tab to provide inputs for generating a baseline set of ux code. running example (expand for details) throughout the guide, anywhere we are creating portions of the running example you will find a similar expandable section to this one, which explains",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "in more detail what the specific section is adding. start by creating a pkl file that amends our published typedef toolkit model: mycustommodel.pkl 1 2 amends package: developer.atlan.com toolkits typedef model 7.0.0 typedefs.pkl amends .. toolkit src main pkl typedefs.pkl (1)! you must use only one of these options, but when developing your typedefs directly in the atlanhq models repository under the typedefs directory, you can use this form to always be using the latest version of the toolkit (without ever needing to manually update the version number or separately download the toolkit). if this is the first time you re creating a model, hover over that line and download the package. set the overall structure then you can start defining your model. all models must have at least two components: a namespace , which uniquely prefixes all types and attributes in your model (to avoid any collisions with others).",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "a collection of customassettypes that define the objects in your model. if you use only these two components, various other defaults will be generated for you automatically (such as the abstract supertype for your model). you can also override or extend aspects of these generated objects, if you look at the detailed tab. (and finally, the with ux tab shows further options for configuring the user interface that will be coupled to your model.) simple detailed with ux mycustommodel.pkl 1 2 3 4 5 6 7 amends .. toolkit src main pkl typedefs.pkl namespace custom (1) customassettypes (2) ... the namespace is used for every type in the model (pascalcase). it will also automatically be decapitalized for use as an attribute prefix. customassettypes describe the objects you want to instantiate in your model. mycustommodel.pkl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "17 18 19 20 21 22 23 24 amends .. toolkit src main pkl typedefs.pkl namespace custom (1) attrprefix t . decapitalize () (2) customenumtypes (3) ... customstructtypes (4) ... supertypedefinition (5) ... customassettypes (6) ... customrelationshiptypes (7) ... the namespace is used for every type in the model (pascalcase). it will also automatically be decapitalized for use as an attribute prefix. (optional) you can override the attribute prefix, if you do not simply want to decapitalize the namespace. this prefix will be used for every attribute in the model (camelcase). pkl provides methods like decapitalize() to lowercase only the first letter of a string, or tolowercase() to convert an entire string to lowercase. or you can of course use a literal string here. (optional) customenumtypes describe any lists of valid values (enumerations) you want to be able to use anywhere in your model. (optional) customstructtypes describe any complex (nested)",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "attributes you want to be able to use anywhere in your model. (optional) supertypedefinition configures the abstract supertype for all other asset types you want to be able to instantiate. for example, you would use this section to define attributes that should exist across all objects in your model, or if you want your abstract supertype to extend something other than the default supertype ( catalog ). customassettypes describe the objects you want to instantiate in your model. (optional) customrelationshiptypes describe the relationships between objects in your model. mycustommodel.pkl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 amends .. toolkit src main pkl typedefs.pkl import .. toolkit src main pkl frontend.pkl (1) namespace custom (2) attrprefix t . decapitalize () (3) customenumtypes (4) ... customstructtypes (5) ... supertypedefinition (6)",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "... customassettypes (7) ... customrelationshiptypes (8) ... ui (9) ... (optional) if you intend to include any front end elements in your definition (such as icons), you will need to import the frontend.pkl portion of the toolkit. can also use the full online path if you do not have access to the atlanhq models repository, you can also use the full online path for the toolkit: import package: developer.atlan.com toolkits typedef model 7.0.0 frontend.pkl the namespace is used for every type in the model (pascalcase). it will also automatically be decapitalized for use as an attribute prefix. (optional) you can override the attribute prefix, if you do not simply want to decapitalize the namespace. this prefix will be used for every attribute in the model (camelcase). pkl provides methods like decapitalize() to lowercase only the first letter of a string, or tolowercase() to convert an entire string to lowercase. or",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "you can of course use a literal string here. (optional) customenumtypes describe any lists of valid values (enumerations) you want to be able to use anywhere in your model. (optional) customstructtypes describe any complex (nested) attributes you want to be able to use anywhere in your model. (optional) supertypedefinition configures the abstract supertype for all other asset types you want to be able to instantiate. for example, you would use this section to define attributes that should exist across all objects in your model, or if you want your abstract supertype to extend something other than the default supertype ( catalog ). customassettypes describe the objects you want to instantiate in your model. (optional) customrelationshiptypes describe the relationships between objects in your model. (optional) ui describes overall user interface setup, such as the filters for the discovery page or the breadcrumb trails to use to show an asset s containment",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "hierarchy. define reusable structures (optional) use the customenumtypes and customstructtypes sections to define any reusable structures for your model. running example (expand for details) from the running example, the reusable structures define these two objects: erdiagram customtemperature(enum) val hot highly available val cold offline storage customrating(struct) string customratingfrom long customratingof simple detailed with ux if you do not need these kinds of structures in your model, you can leave these sections out entirely. define local variables for your types to be capable of being referenced as types for attributes elsewhere in your model, the type names for these structures must be prefixed with the namespace. the toolkit provides a helper method for you to enforce this gettypename() . mycustommodel.pkl 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 local temperaturetype gettypename",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "( temperaturetype ) (1) local ratings gettypename ( ratings ) customenumtypes (2) [ temperaturetype ] (3) description valid values for (table) temperatures. validvalues (4) [ cold ] description lowest availability, can be offline storage. [ hot ] description highest availability, must be on solid state or in memory storage. customstructtypes (5) [ ratings ] (6) description ratings for an asset from the source system. attributes (7) [ ratingfrom ] (8) description username of the user who left the rating. type string [ ratingof ] description numeric score for the rating left by the user. type long define local variables (using the local keyword) for the name of each of your reusable structures. you can ensure they are properly namespaced by using the gettypename() helper method the toolkit provides. (optional) you may define any number of lists of valid values that can be used to constrain values for some attribute elsewhere",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "in your model within customenumtypes . use the local variables defined above in the form [variablename] and provide at least a description and a map of validvalues for each enumeration. the valid values should each be specified in the form [ value ] description , where the value in square brackets is one acceptable value for this enumeration, and the description gives the meaning of that value. (optional) you may define any number of complex nested attribute structures that can be used as an attribute elsewhere in your model within customstructtypes . use the local variables defined above in the form [variablename] and provide at least a description and a map of attributes for each struct. each attribute should take the form of [ name ] and have at least a description and type . the name of the attribute will automatically be prefixed with the attribute prefix ( attrprefix",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": ") for you, so you can focus on just a simple name for the attribute. define local variables for your types to be capable of being referenced as types for attributes elsewhere in your model, the type names for these structures must be prefixed with the namespace. the toolkit provides a helper method for you to enforce this gettypename() . mycustommodel.pkl 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 local temperaturetype gettypename ( temperaturetype ) (1) local ratings gettypename ( ratings ) customenumtypes (2) [ temperaturetype ] (3) description valid values for (table) temperatures. validvalues (4) [ cold ] description lowest availability, can be offline storage. [ hot ] description highest availability, must be on solid state or in memory storage. customstructtypes (5) [ ratings ] (6) description",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "ratings for an asset from the source system. attributes (7) [ ratingfrom ] (8) label from (9) description username of the user who left the rating. type string [ ratingof ] label score description numeric score for the rating left by the user. type long define local variables (using the local keyword) for the name of each of your reusable structures. you can ensure they are properly namespaced by using the gettypename() helper method the toolkit provides. (optional) you may define any number of lists of valid values that can be used to constrain values for some attribute elsewhere in your model within customenumtypes . use the local variables defined above in the form [variablename] and provide at least a description and a map of validvalues for each enumeration. the valid values should each be specified in the form [ value ] description , where the value in square brackets",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "is one acceptable value for this enumeration, and the description gives the meaning of that value. (optional) you may define any number of complex nested attribute structures that can be used as an attribute elsewhere in your model within customstructtypes . use the local variables defined above in the form [variablename] and provide at least a description and a map of attributes for each struct. each attribute should take the form of [ name ] and have at least a description and type . the name of the attribute will automatically be prefixed with the attribute prefix ( attrprefix ) for you, so you can focus on just a simple name for the attribute. (optional) setting the label will control how the attribute is labelled in the user interface. define abstract supertype (optional) use the supertypedefinition section to define reusable attributes for your model. this supertype itself would never be",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "directly instantiated, but will define attributes that are common across all types that can be instantiated. running example (expand for details) from the running example, the common metadata was not originally illustrated. if you noticed the same 3 attributes were defined again at each level, here this abstract type defines those attributes just once (to be inherited by all the other levels). erdiagram custom (abstract) string customsourceid string customdatasetname string customdatasetqualifiedname will inherit all attributes from its own supertype remember that this supertype will itself inherit all attributes from its supertype (by default, catalog ). so things like name , description , qualifiedname , createdby , updatedby , and so on do not need to be redefined here. simple detailed with ux if you have no other attributes you need across the asset types in your model, you can leave this supertypedefinition out entirely and the toolkit will generate it",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "for you. mycustommodel.pkl 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 supertypedefinition (1) name namespace (2) supertypes new listing catalog (3) attributes (4) [ sourceid ] (5) description unique identifier for the (namespace) asset from the source system. type string [ datasetname ] description simple name of the dataset in which this asset exists, or empty if it is itself a dataset. type string indexas both (6) [ datasetqualifiedname ] description unique name of the dataset in which this asset exists, or empty if it is itself a dataset. type string indexas keyword (7) you may define one (and only one) supertypedefinition . (optional) usually this should be the same as, or at least start with the type prefix ( namespace ). if unspecified, it will default to the string used for namespace . (optional) you may specify",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "an alternative supertype for your abstract type itself to extend. if unspecified, it will default to catalog . could vary depending on the supertype you want the code shown here is for setting the supertype to catalog . because the toolkit would already default this value, it is necessary in this case to create an entirely new listing, to completely override the toolkit s defaults. if you want to instead use some other supertype entirely (like bi ), you can simply use: 37 supertypes bi (optional) you can define any number of attributes that should be inherited by all custom asset types in the model. each attribute should take the form of [ name ] and have at least a description and type . the name of the attribute will automatically be prefixed with the attribute prefix ( attrprefix ) for you, so you can focus on just a simple",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "name for the attribute. (optional) you can also control how the attribute will be indexed. for example, both will create both an exact match useful keyword index as well as a tokenized fuzzy useful text index for that attribute. (optional) by default, string attributes will be indexed as a tokenized fuzzy useful text index. if you want to force them to use an exact match useful keyword index instead, you can set the indexas to keyword . why would i define these datasetname and datasetqualifiedname attributes as shared? this is necessary to ensure these attributes exist on all asset types within this area, so that the hierarchy filters on the asset discovery ui find all children objects across all levels of the containment hierarchy. this is a common pattern for new asset types that have a hierarchy of containment. you ll see the same pattern in our out of the",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "box sql asset types, for example, which have databasequalifiedname , databasename , schemaqualifiedname , schemaname , tablequalifiedname , tablename , viewqualifiedname , and viewname all defined at the shared supertype level ( sql ). mycustommodel.pkl 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 supertypedefinition (1) name namespace (2) supertypes new listing catalog (3) attributes (4) [ sourceid ] (5) label source id (6) description unique identifier for the (namespace) asset from the source system. type string [ datasetname ] label dataset description simple name of the dataset in which this asset exists, or empty if it is itself a dataset. type string indexas both (7) [ datasetqualifiedname ] description unique name of the dataset in which this asset exists, or empty if it is itself a dataset. type string indexas keyword (8) you may define one",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "(and only one) supertypedefinition . (optional) usually this should be the same as, or at least start with the type prefix ( namespace ). if unspecified, it will default to the string used for namespace . (optional) you may specify an alternative supertype for your abstract type itself to extend. if unspecified, it will default to catalog . could vary depending on the supertype you want the code shown here is for setting the supertype to catalog . because the toolkit would already default this value, it is necessary in this case to create an entirely new listing, to completely override the toolkit s defaults. if you want to instead use some other supertype entirely (like bi ), you can simply use: 40 supertypes bi (optional) you can define any number of attributes that should be inherited by all custom asset types in the model. each attribute should take the",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "form of [ name ] and have at least a description and type . the name of the attribute will automatically be prefixed with the attribute prefix ( attrprefix ) for you, so you can focus on just a simple name for the attribute. (optional) setting the label will control how the attribute is labelled in the user interface. (optional) you can also control how the attribute will be indexed. for example, both will create both an exact match useful keyword index as well as a tokenized fuzzy useful text index for that attribute. (optional) by default, string attributes will be indexed as a tokenized fuzzy useful text index. if you want to force them to use an exact match useful keyword index instead, you can set the indexas to keyword . why would i define these datasetname and datasetqualifiedname attributes as shared? this is necessary to ensure these attributes",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "exist on all asset types within this area, so that the hierarchy filters on the asset discovery ui find all children objects across all levels of the containment hierarchy. this is a common pattern for new asset types that have a hierarchy of containment. you ll see the same pattern in our out of the box sql asset types, for example, which have databasequalifiedname , databasename , schemaqualifiedname , schemaname , tablequalifiedname , tablename , viewqualifiedname , and viewname all defined at the shared supertype level ( sql ). define instantiate able types then, define the types in your custom model that you want to be able to instantiate. define local variables for your types and key attributes to be capable of being referenced in relationships and in the generated ui code, both your type names and certain attributes must be prefixed with the namespace (or attribute prefix). the toolkit",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "provides helper methods for you to get these gettypename() and getattributename() . new types of assets describe the new types of assets you want to be able to create and manage (and their attributes) under the customassettypes section. running example (expand for details) from the running example, the new asset types define these three objects: erdiagram customdataset string customsourceid from custom string customdatasetname from custom string customdatasetqualifiedname from custom customtable string customsourceid from custom string customdatasetname from custom string customdatasetqualifiedname from custom struct[] customratings customfield string customsourceid from custom string customdatasetname from custom string customdatasetqualifiedname from custom string tablename from column string tablequalifiedname from column enum customtemperature customtemperature(enum) o o customfield : customrating(struct) o o customtable : simple detailed with ux mycustommodel.pkl 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 local dataset gettypename ( dataset ) (1) local table gettypename ( table )",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "local field gettypename ( field ) customassettypes (2) [ dataset ] (3) description instances of (dataset) in atlan. [ table ] description instances of (table) in atlan. [ field ] description instances of (field) in atlan. define local variables (using the local keyword) for each of your types. you can ensure they are properly namespaced by using the gettypename() helper method the toolkit provides. you can define any number of custom types that can be instantiated within customassettypes . use the local variables defined above in the form [variablename] and provide at least a description for that custom asset type. mycustommodel.pkl 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 local dataset gettypename ( dataset ) (1) local table gettypename ( table ) local field gettypename (",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "field ) customassettypes (2) [ dataset ] (3) description instances of (dataset) in atlan. [ table ] description instances of (table) in atlan. attributes (4) [ ratings ] (5) description ratings for the (table) asset from the source system. type struct (6) structname ratings (7) multivalued true (8) supertypes table (9) [ field ] description instances of (field) in atlan. attributes [ temperature ] description temperature of the (field) asset. type enum (10) enumname temperaturetype (11) supertypes column define local variables (using the local keyword) for each of your types. you can ensure they are properly namespaced by using the gettypename() helper method the toolkit provides. you can define any number of custom types that can be instantiated within customassettypes . use the local variables defined above in the form [variablename] and provide at least a description for that custom asset type. you can specify any attributes specific to this",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "custom asset here. (remember any common attributes will be inherited automatically from the supertype.) the name of the attribute will automatically be prefixed with the attribute prefix ( attrprefix ) for you, so you can focus on just a simple name for the attribute. the type can either be primitive or point to a complex definition like struct or enum . when the type is struct , you must also provide the name of the struct in structname . use that local variable you created to define the struct! (optional) if you want to allow multiple instances of this attribute to be stored on each asset, set multivalued to true . (optional) you can specify any additional supertypes your custom type should have. the top level supertype defined under supertypedefinition (or generated from the namespace ) will be set automatically, so you only need to include this if you want",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "your new custom type to have multiple supertypes. the type can be enum to restrict its values to a set of predefined values. when the type is enum , you must also provide the name of the enumeration that defines the valid values in enumname . use that local variable you created to define the enum! mycustommodel.pkl 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 local dataset gettypename ( dataset )",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "(1) local table gettypename ( table ) local field gettypename ( field ) local datasetqn getattributename ( datasetqualifiedname ) (2) local datasetn getattributename ( datasetname ) local tableqn getattributename ( tablequalifiedname ) local tablen getattributename ( tablename ) local dataseticon new icon (3) name databasegray (4) nameactive database (5) svg database gray.svg (6) svgactive database.svg (7) local tableicon new icon name tablegray nameactive table svg table gray.svg svgactive table.svg local fieldicon new icon name columngray nameactive column svg column gray.svg svgactive column.svg customassettypes (8) [ dataset ] (9) label dataset (10) icon dataseticon (11) description instances of (dataset) in atlan. [ table ] label table icon tableicon description instances of (table) in atlan. parentqualifiedname datasetqn (12) attributes (13) [ ratings ] (14) label rating (15) description ratings for the (table) asset from the source system. type struct (16) structname ratings (17) multivalued true (18) supertypes table (19) [ field ]",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "label field icon fieldicon description instances of (field) in atlan. parentqualifiedname tableqn attributes [ temperature ] label temperature description temperature of the (table) asset. type enum (20) enumname temperaturetype (21) supertypes column ui (22) svgname (namespace).svg (23) filters (24) [ dataset ] (25) attribute datasetqn breadcrumb (26) [ dataset ] q datasetqn n datasetn [ table ] q tableqn n tablen define local variables (using the local keyword) for each of your types. you can ensure they are properly namespaced by using the gettypename() helper method the toolkit provides. define local variables (using the local keyword) for each attribute you will reference for the ui. you can ensure they are properly namespaced by using the getattributename() helper method the toolkit provides. you may define further local variables to represent the various icons you want to use in the ui. each icon must have a name that will be used to",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "refer to it in the generated code. when you want to reuse an existing icon, this must match the name of the existing icon in the existing front end code. (optional) each icon can also have an alternate variation that is used when the icon is selected, which also must be named. (again, if you want to reuse an existing icon, this must match the name of the existing icon in the existing front end code.) each icon must have an svg file that provides the actual image for the icon. you ll need to copy this image file into the appropriate location later, but the filename must be accurate here. (optional) when you want an alternate variation of the icon to use when the icon is selected, specify the svg filename for that alternative image. (again, you ll need to copy this image file into the appropriate location later,",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "but the filename must be accurate here.) you can define any number of custom types that can be instantiated within customassettypes . use the local variables defined above in the form [variablename] and provide at least a description for that custom asset type. setting the label will control how the type is labelled in the user interface. setting the icon will control the icon to display for this type in the user interface. these are themselves an object, which could either be defined inline here or (as in this example) as a separate local variable. for types that are contained within another type, specify the name of the de normalized attribute that contains the qualifiedname of the parent asset. this will be used to efficiently render parent child relationships in the ui. use that local variable you created for the attribute! you can specify any attributes specific to this custom",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "asset here. (remember any common attributes will be inherited automatically from the supertype.) the name of the attribute will automatically be prefixed with the attribute prefix ( attrprefix ) for you, so you can focus on just a simple name for the attribute. you can also set the label to control how each attribute is labelled in the user interface. the type can either be primitive or point to a complex definition like struct or enum . when the type is struct , you must also provide the name of the struct in structname . use that local variable you created to define the struct! (optional) if you want to allow multiple instances of this attribute to be stored on each asset, set multivalued to true . (optional) you can specify any additional supertypes your custom type should have. the top level supertype defined under supertypedefinition (or generated from the",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "namespace ) will be set automatically, so you only need to include this if you want your new custom type to have multiple supertypes. the type can be enum to restrict its values to a set of predefined values. when the type is enum , you must also provide the name of the enumeration that defines the valid values in enumname . use that local variable you created to define the enum! (optional) you can also define ui aspects that should apply across all assets of these types under the ui section. provide the filename for an svg image you want to use as the icon to visually present all of these assets. this could be a branded logo of the source system that these assets represent, for example. (optional) you can define the hierarchy of filters that users can apply on the asset discovery page. these are applicable in",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "top down order after a connection has been selected, and can include at most the top 2 levels of the asset containment hierarchy. each entry should be keyed by the type name, and have as a value the name of the denormalized attribute that every asset must have populated to be contained within that level of the hierarchy. use those local variables again! (optional) you can define the breadcrumb trail that should be shown for assets to indicate their containment hierarchy. these define the breadcrumb in top down order, and should generally not include more than 3 levels (or they will overrun the ui). each entry should be keyed by the type name, and have two values: q giving the name of the denormalized attribute that has the unique name of the asset this one is contained within a giving the name of the denormalized attribute that has the simple",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "name of the asset this one is contained within new asset relationships describe the new relationships you want to be able to create and manage between assets under the customrelationshiptypes section. running example (expand for details) from the running example, the new relationships define these linkages between the assets: erdiagram customdataset o customtable : customtables customdataset customtable o customfield : customfields customtable customfield o o customfield : customtofields customfromfields simple detailed with ux if you have no relationships you need between the new asset types in your model and any other asset types, you can leave this customrelationshiptypes section out entirely. mycustommodel.pkl 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 customrelationshiptypes (1) [ datasetanditstables ] new containmentrelationship",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "(2) parent (3) type dataset (4) attribute dataset (5) description dataset containing the table. (6) children (7) type table attribute tables description tables contained within the dataset. [ tableanditsfields ] new containmentrelationship parent type table attribute table description table containing the field. children type field attribute fields description fields contained within the table. [ interrelatedfields ] new peertopeerrelationship (8) description many to many peer to peer relationship between (field)s. peers (9) new type field (10) attribute fromfields (11) description (field)s from which this (field) is related. (12) new type field attribute tofields description (field)s to which this (field) is related. you can also specify any number of custom relationships. these should be listed under customrelationshiptypes and each take the form of [ name ] new relationship . there are two kinds of relationship that can be created: containmentrelationship defines a parent child (hierarchical) relationship. each parent can have many children,",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "but each child can refer to only a single parent. peertopeerrelationship defines an association relationship, which is many to many by default. make the name as informational as you want the name is not actually used other than to keep the relationships unique, so feel free to use as informational a name as you want. this example starts by creating a new parent child containmentrelationship between a dataset (the parent) and its tables (the children). a containmentrelationship must have one and only one parent , which describes the parent end of the relationship. each end of the relationship must have a type , defining the asset type for that end of the relationship. each end of the relationship must also define an attribute , which is how this end of the relationship will be referred to by the other end of the relationship. the name of the attribute will automatically",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "be prefixed with the attribute prefix ( attrprefix ) for you, so you can focus on just a simple name for the attribute. each end of the relationship must also provide a description for the attribute . a containerrelationship must also have one and only one children definition, which describes the end of the relationship containing the children. this example shows how you can define a new many to many relationship between assets. a peertopeerrelationship must have a listing of exactly two peers , each of which describes one end of the relationship. each end of the relationship must have a type , defining the asset type for that end of the relationship. each end of the relationship must also define an attribute , which is how this end of the relationship will be referred to by the other end of the relationship. the name of the attribute will automatically",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "be prefixed with the attribute prefix ( attrprefix ) for you, so you can focus on just a simple name for the attribute. each end of the relationship must also provide a description for the attribute . mycustommodel.pkl 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 customrelationshiptypes (1) [ datasetanditstables ] new containmentrelationship (2) parent (3) type dataset (4) attribute dataset (5) description dataset containing the table. (6) children (7) type table attribute tables description tables contained within the dataset. [ tableanditsfields ] new containmentrelationship parent type table attribute table description table containing the field. children type field attribute fields description fields contained within the table. [ interrelatedfields ] new peertopeerrelationship (8) description many to many peer",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "to peer relationship between (field)s. peers (9) new type field (10) attribute fromfields (11) description (field)s from which this (field) is related. (12) new type field attribute tofields description (field)s to which this (field) is related. you can also specify any number of custom relationships. these should be listed under customrelationshiptypes and each take the form of [ name ] new relationship . there are two kinds of relationship that can be created: containmentrelationship defines a parent child (hierarchical) relationship. each parent can have many children, but each child can refer to only a single parent. peertopeerrelationship defines an association relationship, which is many to many by default. make the name as informational as you want the name is not actually used other than to keep the relationships unique, so feel free to use as informational a name as you want. this example starts by creating a new parent child",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "containmentrelationship between a dataset (the parent) and its tables (the children). a containmentrelationship must have one and only one parent , which describes the parent end of the relationship. each end of the relationship must have a type , defining the asset type for that end of the relationship. each end of the relationship must also define an attribute , which is how this end of the relationship will be referred to by the other end of the relationship. the name of the attribute will automatically be prefixed with the attribute prefix ( attrprefix ) for you, so you can focus on just a simple name for the attribute. each end of the relationship must also provide a description for the attribute . a containerrelationship must also have one and only one children definition, which describes the end of the relationship containing the children. this example shows how you can",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "define a new many to many relationship between assets. a peertopeerrelationship must have a listing of exactly two peers , each of which describes one end of the relationship. each end of the relationship must have a type , defining the asset type for that end of the relationship. each end of the relationship must also define an attribute , which is how this end of the relationship will be referred to by the other end of the relationship. the name of the attribute will automatically be prefixed with the attribute prefix ( attrprefix ) for you, so you can focus on just a simple name for the attribute. each end of the relationship must also provide a description for the attribute . advanced attribute options there are further advanced options you can use when defining each attribute. these will all be set to sensible defaults based on the options",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "outlined in the example above, but you can also directly set or override them if needed. property usage default defaultvalue default value for the attribute. isdefaultvaluenull indicates whether the attribute has a default value of being empty (true) or not (false). isoptional indicates whether the attribute is mandatory (false) or optional (true). true valuesmincount minimum number of values the attribute should have. valuesmaxcount maximum number of values the attribute should have. isunique whether the attribute is unique (true) or not (false). false isindexable whether the attribute is indexed in memory via cassandra (true) or not (false). false includeinnotification whether the attribute should generate a notification when its value changes (true) or not (false). true skipscrubbing tbc true searchweight tbc indexas what kind of index(es) to create in elastic for this attribute: keyword , text , both . default 2025 03 11 2025 06 24 was this page helpful? thanks for",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/typedef/define/"
  },
  {
    "text": "render your model developer skip to content render your model full example (expand for details) in addition to defining the various entities and relationships outlined at the beginning of this page, we have created this inheritance structure: classdiagram direction lr catalog catalog sql custom class customtable struct[] customratings class customfield enum customtemperature following is the complete model file for the running example, without any comments, in case you want to try it yourself as a sort of hello world example: mycustommodel.pkl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69",
    "source": "https://developer.atlan.com/toolkits/typedef/render/"
  },
  {
    "text": "70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 amends .. toolkit src main pkl typedefs.pkl import .. toolkits src main pkl frontend.pkl namespace custom attrprefix namespace . decapitalize () local temperaturetype gettypename ( temperaturetype ) local ratings gettypename",
    "source": "https://developer.atlan.com/toolkits/typedef/render/"
  },
  {
    "text": "( ratings ) customenumtypes [ temperaturetype ] description valid values for (table) temperatures. validvalues [ cold ] description lowest availability, can be offline storage. [ hot ] description highest availability, must be on solid state or in memory storage. customstructtypes [ ratings ] description ratings for an asset from the source system. attributes [ ratingfrom ] label from description username of the user who left the rating. type string [ ratingof ] label score description numeric score for the rating left by the user. type long supertypedefinition name namespace supertypes new listing catalog attributes [ sourceid ] label source id description unique identifier for the (namespace) asset from the source system. type string [ datasetname ] label dataset description simple name of the dataset in which this asset exists, or empty if it is itself a dataset. type string indexas both [ datasetqualifiedname ] description unique name of the dataset",
    "source": "https://developer.atlan.com/toolkits/typedef/render/"
  },
  {
    "text": "in which this asset exists, or empty if it is itself a dataset. type string indexas keyword local dataset gettypename ( dataset ) local table gettypename ( table ) local field gettypename ( field ) local datasetqn getattributename ( datasetqualifiedname ) local datasetn getattributename ( datasetname ) local tableqn getattributename ( tablequalifiedname ) local tablen getattributename ( tablename ) local dataseticon new frontend . icon name databasegray nameactive database svg database gray.svg svgactive database.svg local tableicon new frontend . icon name tablegray nameactive table svg table gray.svg svgactive table.svg local fieldicon new frontend . icon name columngray nameactive column svg column gray.svg svgactive column.svg customassettypes [ dataset ] label dataset icon dataseticon description instances of (dataset) in atlan. [ table ] label table icon tableicon description instances of (table) in atlan. parentqualifiedname datasetqn attributes [ ratings ] label rating description ratings for the (table) asset from the source system. type",
    "source": "https://developer.atlan.com/toolkits/typedef/render/"
  },
  {
    "text": "struct structname ratings multivalued true supertypes table [ field ] label field icon fieldicon description instances of (field) in atlan. parentqualifiedname tableqn attributes [ temperature ] label temperature description temperature of the (field) asset. type enum enumname temperaturetype supertypes column ui svgname (namespace).svg filters [ dataset ] attribute datasetqn breadcrumb [ dataset ] q datasetqn n datasetn [ table ] q tableqn n tablen customrelationshiptypes [ datasetanditstables ] new containmentrelationship parent type dataset attribute dataset description dataset containing the table. children type table attribute tables description tables contained within the dataset. [ tableanditsfields ] new containmentrelationship parent type table attribute table description table containing the field. children type field attribute fields description fields contained within the table. [ interrelatedfields ] new peertopeerrelationship description many to many peer to peer relationship between (field)s. peers new type field attribute fromfields description (field)s from which this (field) is related. new type field attribute",
    "source": "https://developer.atlan.com/toolkits/typedef/render/"
  },
  {
    "text": "tofields description (field)s to which this (field) is related. render through pkl once your model is defined, you can then render it into the files atlan needs using the pkl cli: pkl eval mycustommodel.pkl m tmp this will generate multiple json files representing the custom model, in the folder structure required by atlan, ready to be submitted in a pr. output produced rendering the model will create two subdirectories under the output directory you specify (the location you specify for m ): models (1) namespace .json (2) namespace assettype .json (3) ... namespace relationships.json (4) frontend (5) namespace src api schemas (6) namespace .json namespace assettype .json ... namespace relationships.json assets images icons (7) components common icon (8) widgets (9) composables discovery (10) constant (11) locales (12) the models subdirectory will contain json files needed by atlas for the underlying metamodel. a single file containing details of the abstract supertype,",
    "source": "https://developer.atlan.com/toolkits/typedef/render/"
  },
  {
    "text": "all enumerations and all structs. multiple json files, one per custom asset type. a single json file containing all relationships. the frontend subdirectory will contain various json, typescript, vue, and placeholder image files. the same json files as output to the models subdirectory. empty placeholder files indicating the image files for icons you must copy over separately. code snippets you need to insert into an existing typescript file. code snippets you need to insert into an existing vue file. code snippets you need to insert into an existing typescript file for parent child navigation through de normalized qualifiedname attributes to work. mixture of code snippets and full typescript code files you need to copy over. internationalization snippets you need to insert into an existing json file. running example (expand for details) for our running example, this would produce: models custom.json (1)! custom customdataset.json customtable.json customfield.json custom relationships.json frontend custom src",
    "source": "https://developer.atlan.com/toolkits/typedef/render/"
  },
  {
    "text": "api schemas custom.json custom customdataset.json customtable.json customfield.json custom relationships.json assets images icons database gray.svg placeholder database.svg placeholder table gray.svg placeholder table.svg placeholder column gray.svg placeholder column.svg placeholder components common icon column.svg placeholder widgets summary types parentassetinline.vue snippet composables discovery usebody.ts snippet constant projection.ts snippet source index.ts snippet custom index.ts methods.ts common index.ts assettypes.ts getassettypes.ts hierarchyfilters.ts attributes customdataset.ts customtable.ts customfield.ts locales en.json snippet this file will contain all the attributes defined in the supertype, as well as the enumeration and struct. 2025 03 11 2025 06 24 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any",
    "source": "https://developer.atlan.com/toolkits/typedef/render/"
  },
  {
    "text": "information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/typedef/render/"
  },
  {
    "text": "test your model developer skip to content test your model add to atlanhq models once your model is rendered, you then need to add it to the atlanhq models repository: clone atlanhq models to your local machine (if you have not already): git clone git github.com:atlanhq models.git (1)! cd models this assumes you have configured your git client with appropriate credentials. if this step fails, you ll need to setup git first. start from an up to date master branch (in particular if you already have the repository cloned locally): git checkout master git merge origin master create a branch in the local repository: git branch jira task id (1)! git checkout jira task id replace jira task id with the unique id of the task in jira where you are tracking your work. move the generated model files to the cloned repository: mv ... tmp models atlas entitydefs referenceable",
    "source": "https://developer.atlan.com/toolkits/typedef/test-model/"
  },
  {
    "text": "asset catalog . (1)! the first directory should be the path to wherever you generated the files from the pkl command when rendering your model. the second directory is the appropriate location within the models repository for the rendered json. in this example, since our top level supertype was catalog , we place the files under referenceable asset catalog . . if you had instead directly extended asset , you would move the files under referenceable asset . . stage your new model files: git add atlas entitydefs referenceable asset catalog (1)! as above, if the files you added are in a different path, stage them from that different path. (this tells git which files to include all together in your next commit.) commit your new model files to the branch: git commit m new model for ... (1)! provide a meaningful message for the new model you re adding.",
    "source": "https://developer.atlan.com/toolkits/typedef/test-model/"
  },
  {
    "text": "(this tells git to take a (local) snapshot of all the changes you staged (above).) push your committed changes to the remote repository: git push set upstream origin jira task id (1)! remember that jira task id is just a placeholder replace with the name of your actual branch. (this tells git to push all the (local) commits you ve made against this branch to the remote github repository, so they re available to everyone there.) canary your model development purposes only these steps should only be applied to internal development tenants, where you can safely reset the tenant (erasing all metadata) to make changes. kubectl github if you have kubectl access to your cluster, you can canary your model directly: ensure you are on your cluster: loft use vcluster project default (1)! replace with the name of your tenant. (this assumes you are already logged in to loft naturally",
    "source": "https://developer.atlan.com/toolkits/typedef/test-model/"
  },
  {
    "text": "log in there first, if you are not already.) modify the modelsbranch value in the atlan runtime packages config configmap: editor vim (1)! kubectl edit cm atlan runtime packages config set whatever editor you prefer, but be sure it blocks until you have completed editing the configmap. atlan runtime packages config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 please edit the object below. lines beginning with a will be ignored, and an empty file will abort the edit. if an error occurs while saving this file will be reopened with the relevant failures. apiversion : v1 data : channel : master marketplacepackagesbranch : master marketplacescriptsbranch : master modelsbranch : jira task id (1)! packagesconfig : version : latest kind : configmap metadata : ... set the value of modelsbranch to the name of the branch you pushed to atlanhq models .",
    "source": "https://developer.atlan.com/toolkits/typedef/test-model/"
  },
  {
    "text": "quickly run the steps below to seed the development tenant. (a cron job may overwrite the value you just changed above back to master or whatever value is defined in the alternative approach described in the github tab.) if you do not have kubectl access to your cluster, you will need to canary the model through the atlanhq marketplace packages repository. clone atlanhq marketplace packages to your local machine (if you have not already): git clone git github.com:atlanhq marketplace packages.git (1)! cd marketplace packages this assumes you have configured your git client with appropriate credentials. if this step fails, you ll need to setup git first. start from an up to date master branch (in particular if you already have the repository cloned locally): git checkout master git merge origin master create a branch in the local repository: git branch jira task id (1)! git checkout jira task id replace",
    "source": "https://developer.atlan.com/toolkits/typedef/test-model/"
  },
  {
    "text": "jira task id with the unique id of the task in jira where you are tracking your work. create or edit the file deployment tenants .pkl for the tenant where you want to canary the typedefs, with at least the following content: deployment tenants .pkl 1 2 3 amends .. deployment.pkl modelsbranch jira task id stage your new (or modified) .pkl file: git add deployment tenants .pkl (1)! remember to replace with your actual tenant name. (this tells git which files to include all together in your next commit.) commit your new (or modified) file to the branch: git commit m model canary for ... (1)! provide a meaningful message for the new model you re canarying. (this tells git to take a (local) snapshot of all the changes you staged (above).) push your committed changes to the remote repository: git push set upstream origin jira task id (1)! remember",
    "source": "https://developer.atlan.com/toolkits/typedef/test-model/"
  },
  {
    "text": "that jira task id is just a placeholder replace with the name of your actual branch. (this tells git to push all the (local) commits you ve made against this branch to the remote github repository, so they re available to everyone there.) raise a pull request (pr) from your branch ( jira task id ) to master on atlanhq marketplace packages . as long as you did not make any mistakes in the filename or its contents, your pr will be automatically approved by a bot within a few minutes. you can then self merge it. once the pr is merged, wait for the atlan update script to run and complete on your tenant. by default it will run every 30 minutes, so could take up to 1 hour before it has completed on your tenant. 1 seed development tenant once the canary is set: log in to argo",
    "source": "https://developer.atlan.com/toolkits/typedef/test-model/"
  },
  {
    "text": "on your tenant by going to https: tenant name.atlan.com api orchestration and clicking the left most login button. then open https: tenant name.atlan.com api orchestration cluster workflow templates atlan typedef seeder and click the submit button in the upper right. in the resulting dialog, click the submit button. once this workflow completes (successfully), the typedefs will be available in that tenant. it is also possible that synchronization has been disabled on your tenant, in which case atlan update may not run at all. if that is the case, you will need to speak with whoever manages your tenant to see how you can test your typedefs. 2025 03 11 2025 03 11 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously",
    "source": "https://developer.atlan.com/toolkits/typedef/test-model/"
  },
  {
    "text": "measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/typedef/test-model/"
  },
  {
    "text": "generate sdk bindings developer skip to content bind the sdks of course, to actually test the ux you should create some assets of the new type(s) and test. the simplest way to do this is to make them programmatically accessible via an sdk. clone sdk repository to actually implement the sdk bindings, you will first need to clone the sdk s code repository: java python set up the development environment for the java sdk . open the terminal and clone the latest atlan java repository: git clone https: github.com atlanhq atlan java.git ensure you have a working java 21 jdk as your java home : java version echo java home export java home library java javavirtualmachines openjdk 21.jdk contents home ensure gradle is set up and able to compile (this will also ensure all dependencies are downloaded): . gradlew assemble shadowjar set up the development environment for pyatlan . open",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "the terminal and clone the latest pyatlan repository: git clone https: github.com atlanhq atlan python.git we recommend creating a new virtual environment : python3 m venv venv activate the virtual environment: source venv bin activate in the project root directory, install the required dependencies: pip install e . pip install r requirements dev.txt implement creator methods each sdk contains a generator package that generates sdk code based on the latest typedefs available in your atlan instance. however, each new asset type will also have a specific set of minimal attributes required when creating any instances of that type. this minimal set of attributes is defined for developers to understand and consume through a creator method. qualifiedname all asset types require a qualifiedname , and this must be unique for all instances of that asset type. consider carefully how the qualifiedname should be constructed since it must be unique across all",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "instances of that asset type, carefully consider how the qualifiedname should be constructed and automate or enforce this as much as possible in your creator implementation. while names are often used in the qualifiedname , if the system you are representing does not have unique names at a given level (or these are subject to change without the object itself changing) you may need to use some other string in the qualifiedname to ensure it remains unique. typically this will be: default connectortype epoch uniquename (1) default connectortype epoch ... uniquename (2) for a top level asset, the connection s qualifiedname concatenated with the (unique) name of the top level asset: for a child asset, the parent s qualifiedname concatenated with the (unique) name of the child asset: running example (expand for details) in our running example, this would mean qualifiedname s that look like this: default genericdb 1234567890 some",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "dataset (1) default genericdb 1234567890 some dataset some table (2) default genericdb 1234567890 some dataset some table some field (3) an example qualifiedname for a dataset named some dataset , in a generic database connection (that happened to be created on february 13, 2009 at 23:31:30 gmt). an example qualifiedname for a table named some table created within that dataset. an example qualifiedname for a field named some field created within that table. from the examples above, you can see all assets require at least the following information at creation (even top level assets): typename to define the type of asset being created qualifiedname which must be unique across all instances of the asset type, since it is used to determine whether to update or create a new instance connectionqualifiedname for ux filtering and access control purposes, which has embedded within it a connectortype (which in turn determines which icon",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "to use to represent each instance of an asset) running example (expand for details) sample creation payload (customdataset) entities : [ typename : customdataset , attributes : name : some dataset , qualifiedname : default genericdb 1234567890 some dataset , connectionqualifiedname : default genericdb 1234567890 , connectorname : genericdb ] the sections below walkthrough writing the code generator templates for the running example. customdataset template now that we know the minimal required attributes for creation, we can define these in a template for each sdk. these templates define only the unique portion of the generated code in each sdk any standard code will still continue to be generated automatically: java python sdk src main resources templates customdataset.ftl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 builds the minimal object necessary to create a customdataset. param name of the customdataset param connectionqualifiedname unique name of the connection through which the spec is accessible return the minimal object necessary to create the customdataset, as a builder public static customdatasetbuilder creator ( string name , string connectionqualifiedname ) (1) return customdataset . internal () . guid ( threadlocalrandom . current (). nextlong ( 0 , long . max value 1 )) (2) . qualifiedname ( connectionqualifiedname name ) . name ( name ) . connectionqualifiedname ( connectionqualifiedname ) . connectortype ( connection . getconnectortypefromqualifiedname ( connectionqualifiedname )); builds the minimal object necessary to update a customdataset. param qualifiedname of the customdataset param name of the customdataset return the minimal request necessary to update the customdataset, as a builder",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "public static customdatasetbuilder updater ( string qualifiedname , string name ) (3) return customdataset . internal () . guid ( threadlocalrandom . current (). nextlong ( 0 , long . max value 1 )) . qualifiedname ( qualifiedname ) . name ( name ); builds the minimal object necessary to apply an update to a customdataset, from a potentially more complete customdataset object. return the minimal object necessary to update the customdataset, as a builder throws invalidrequestexception if any of the minimal set of required properties for customdataset are not found in the initial object override public customdatasetbuilder trimtorequired () throws invalidrequestexception (4) validaterequired ( type name , map . of ( qualifiedname , this . getqualifiedname (), name , this . getname () )); return updater ( this . getqualifiedname (), this . getname ()); even though we require 4 attributes to create an customdataset , we can derive",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "all of them from just 2 inputs. so to keep the interface as simple as possible, we will only request the 2 inputs we need to derive (automatically) the rest. always set the guid to a random, negative integer. this allows the sdk (and atlan s back end) to handle referential integrity when multiple inter related assets are submitted in a single request even if some of them need to be created. also implement an updater() method that takes the minimal set of attributes required to update an asset of this type. in almost all cases this will be qualifiedname and name , but in some very rare cases could require other attributes. finally, implement the trimtorequired method to validate that an object of this type has the minimal set of attributes required to be used to update such an asset in atlan. like the updater method this will in",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "almost all cases just validate qualifiedname and name are present, but in some very rare cases could validate other attributes. in python, we need to create two templates: pyatlan generator templates methods asset custom dataset.jinja2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 classmethod init guid def creator ( cls , , name : str , connection qualified name : str ) customdataset : (1) validate required fields ( [ name , connection qualified name ], [ name , connection qualified name ] ) attributes customdataset . attributes . create ( name name , connection qualified name connection qualified name ) return cls ( attributes attributes ) classmethod init guid def create ( cls , , name : str , connection qualified name : str ) customdataset : warn ( ( this method",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "is deprecated, please use creator instead, which offers identical functionality. ), deprecationwarning , stacklevel 2 , ) return cls . creator ( name name , connection qualified name connection qualified name ) even though we require 4 attributes to create an customdataset , we can derive all of them from just 2 inputs. so to keep the interface as simple as possible, we will only request the 2 inputs we need to derive (automatically) the rest. pyatlan generator templates methods attribute custom dataset.jinja2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 classmethod init guid def create ( cls , , name : str , connection qualified name : str ) entity def . name . attributes : validate required fields ( [ name , connection qualified name ], [ name , connection qualified name ] ) return entity def . name . attributes",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "( (1) name name , qualified name f connection qualified name name , connection qualified name connection qualified name , connector name atlanconnectortype . get connector name ( connection qualified name ), ) it is within the attributes template that we derive all the required attributes from the minimal inputs requested in the asset level template. customtable template as stated earlier, the qualifiedname should be a concatenation onto the parent s qualifiedname (in our running example, the parent of a customtable is a customdataset ). java python sdk src main resources templates customtable.ftl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 builds the minimal object necessary to create a customtable. param name of the customtable param customdataset in which the customtable should be created, which must have at least a qualifiedname return the minimal request necessary to create the customtable, as a builder throws invalidrequestexception if the customdataset provided is without a qualifiedname public static customtablebuilder creator ( string name , customdataset customdataset ) throws invalidrequestexception (1) validaterelationship ( customdataset . type name , map . of ( qualifiedname , customdataset . getqualifiedname () )); return creator ( name , customdataset . getqualifiedname () ). customdataset ( customdataset . trimtoreference ()); (2) builds the minimal object necessary to create an customtable. param name unique name of the customtable param customdatasetqualifiedname unique name of the customdataset through which the table is accessible",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "return the minimal object necessary to create the customtable, as a builder public static customtablebuilder creator ( string name , string customdatasetqualifiedname ) (3) string connectionqualifiedname stringutils . getparentqualifiednamefromqualifiedname ( customdatasetqualifiedname ); return customtable . internal () . guid ( threadlocalrandom . current (). nextlong ( 0 , long . max value 1 )) . qualifiedname ( customdatasetqualifiedname name ) . name ( name ) . customdataset ( customdataset . refbyqualifiedname ( customdatasetqualifiedname )) . connectionqualifiedname ( connectionqualifiedname ) . connectortype ( connection . getconnectortypefromqualifiedname ( connectionqualifiedname )); builds the minimal object necessary to update a customtable. param qualifiedname of the customtable param name of the customtable return the minimal request necessary to update the customtable, as a builder public static customtablebuilder updater ( string qualifiedname , string name ) return customtable . internal () . guid ( threadlocalrandom . current (). nextlong ( 0 , long . max value",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "1 )) . qualifiedname ( qualifiedname ) . name ( name ); builds the minimal object necessary to apply an update to a customtable, from a potentially more complete customtable object. return the minimal object necessary to update the customtable, as a builder throws invalidrequestexception if any of the minimal set of required properties for customtable are not found in the initial object override public customtablebuilder trimtorequired () throws invalidrequestexception validaterequired ( type name , map . of ( qualifiedname , this . getqualifiedname (), name , this . getname () )); return updater ( this . getqualifiedname (), this . getname ()); for asset types that have a parent asset, you should provide multiple overloaded creator methods. for example, one that takes the parent object itself (and validates the provided object has the minimal set of attributes we require on it) and one that takes only a qualifiedname of",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "the parent asset. when implementing the method that takes a parent object, always set the relationship to the parent object explicitly and by using the trimtoreference() method on the parent object. this ensures that any guid on the parent object is used to create the reference to the parent object which ensures that any negative integer present for referential integrity is preferred over a qualifiedname for the parent object. (which further ensures that you can create both parent and child objects in the same request.) typically the fully parameterized creator() method (with various string parameters) will be the one you call through to from any other overloaded creator() methods, so they all share the same foundational implementation. pyatlan generator templates methods asset custom table.jinja2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 overload classmethod def creator ( cls , , name : str , custom dataset qualified name : str , ) customtable : ... overload classmethod def creator ( cls , , name : str , custom dataset qualified name : str , connection qualified name : str , ) customtable : ... classmethod init guid def creator ( cls , , name : str , custom dataset qualified name : str , connection qualified name : optional [ str ] none , ) customtable : validate required fields ( [ name , custom dataset qualified name ], [ name , custom dataset qualified name ] ) attributes customtable . attributes . create ( name name , custom dataset qualified name custom dataset qualified name",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": ", connection qualified name connection qualified name , ) return cls ( attributes attributes ) classmethod init guid def create ( cls , , name : str , custom dataset qualified name : str ) customtable : warn ( ( this method is deprecated, please use creator instead, which offers identical functionality. ), deprecationwarning , stacklevel 2 , ) return cls . creator ( name name , custom dataset qualified name custom dataset qualified name ) pyatlan generator templates methods attribute custom table.jinja2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 classmethod init guid def create ( cls , , name : str , custom dataset qualified name : str , connection qualified name : optional [ str ] none , ) customtable . attributes : validate required",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "fields ( [ name , custom dataset qualified name ], [ name , custom dataset qualified name ], ) if connection qualified name : connector name atlanconnectortype . get connector name ( connection qualified name ) else : connection qn , connector name atlanconnectortype . get connector name ( custom dataset qualified name , custom dataset qualified name , 4 ) return customtable . attributes ( name name , custom dataset qualified name custom dataset qualified name , connector name connector name , connection qualified name connection qualified name or connection qn , qualified name f custom dataset qualified name name , custom dataset customdataset . ref by qualified name ( custom dataset qualified name ), ) customfield template finally, the customfield template will be very similar to the customtable template. illustrates the case where a qualifiedname may contain forward slashes since qualifiedname s are typically constructed using a as",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "a delimiter, if the system you are representing could actually contain a in the unique information you are placing into the qualifiedname you need to be sure you pass all information to create the qualifiedname you will not be able to parse the parent s qualifiedname in these cases. java python sdk src main resources templates customfield.ftl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 builds the minimal object necessary to create a customfield. param name of the customfield param customtable in which the customfield should be created, which must have at least a qualifiedname return the minimal request necessary to create the customfield, as a builder throws invalidrequestexception if the customtable provided is without a qualifiedname public static customfieldbuilder creator ( string name , customtable customtable ) throws invalidrequestexception (1) map map new hashmap (); (2) map . put ( connectionqualifiedname , customtable . getconnectionqualifiedname ()); map . put ( customdatasetname , customtable . getcustomdatasetname ()); map . put ( customdatasetqualifiedname , customtable . getcustomdatasetqualifiedname ()); map . put ( name",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": ", customtable . getname ()); map . put ( qualifiedname , customtable . getqualifiedname ()); validaterelationship ( customtable . type name , map ); return creator ( name , customtable . getconnectionqualifiedname (), customtable . getcustomdatasetname (), customtable . getcustomdatasetqualifiedname (), customtable . getname (), customtable . getqualifiedname () ). customtable ( customtable . trimtoreference ()); (3) builds the minimal object necessary to create a customfield. param name unique name of the customfield param customtablequalifiedname unique name of the customtable through which the table is accessible return the minimal object necessary to create the customfield, as a builder public static customfieldbuilder creator ( string name , string customtablequalifiedname ) (4) string customtablename stringutils . getnamefromqualifiedname ( customtablequalifiedname ); string customdatasetqualifiedname stringutils . getparentqualifiednamefromqualifiedname ( customtablequalifiedname ); string customdatasetname stringutils . getnamefromqualifiedname ( customdatasetqualifiedname ); string connectionqualifiedname stringutils . getparentqualifiednamefromqualifiedname ( customdatasetqualifiedname ); return creator ( name , connectionqualifiedname , customdatasetname",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": ", customdatasetqualifiedname , customtablename , customtablequalifiedname ); builds the minimal object necessary to create a customfield. param name of the customfield param connectionqualifiedname unique name of the connection in which to create the customfield param customdatasetqualifiedname simple name of the customdataset in which to create the customfield param customdatasetname unique name of the customdataset in which to create the customfield param customtablename simple name of the customtable in which to create the customfield param customtablequalifiedname unique name of the customtable in which to create the customfield return the minimal request necessary to create the customfield, as a builder public static customfieldbuilder creator ( string name , string connectionqualifiedname , string customdatasetqualifiedname , string customdatasetname , string customtablename , string customtablequalifiedname ) atlanconnectortype connectortype connection . getconnectortypefromqualifiedname ( connectionqualifiedname ); return customfield . internal () . guid ( threadlocalrandom . current (). nextlong ( 0 , long . max value 1",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": ")) . name ( name ) . qualifiedname ( generatequalifiedname ( name , customtablequalifiedname )) . connectortype ( connectortype ) . customtablename ( customtablename ) . customtablequalifiedname ( customtablequalifiedname ) . customtable ( customtable . refbyqualifiedname ( customtablequalifiedname )) . customdatasetname ( customdatasetname ) . customdatasetqualifiedname ( customdatasetqualifiedname ) . connectionqualifiedname ( connectionqualifiedname ); generate a unique customfield name. param name of the customfield param customtablequalifiedname unique name of the customtable in which this customfield exists return a unique name for the customfield public static string generatequalifiedname ( string name , string customtablequalifiedname ) (6) return customtablequalifiedname name ; builds the minimal object necessary to update a customfield. param qualifiedname of the customfield param name of the customfield return the minimal request necessary to update the customfield, as a builder public static customfieldbuilder updater ( string qualifiedname , string name ) return customfield . internal () . guid ( threadlocalrandom",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": ". current (). nextlong ( 0 , long . max value 1 )) . qualifiedname ( qualifiedname ) . name ( name ); builds the minimal object necessary to apply an update to a customfield, from a potentially more complete customfield object. return the minimal object necessary to update the customfield, as a builder throws invalidrequestexception if any of the minimal set of required properties for customfield are not found in the initial object override public customfieldbuilder trimtorequired () throws invalidrequestexception validaterequired ( type name , map . of ( qualifiedname , this . getqualifiedname (), name , this . getname () )); return updater ( this . getqualifiedname (), this . getname ()); for asset types that have a parent asset, you should provide multiple overloaded creator methods. for example, one that takes the parent object itself (and validates the provided object has the minimal set of attributes we",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "require on it) and one that takes all the qualifiedname s and de normalized name s of the ancestral assets. when receiving only the parent asset, you will need to validate you have all the other information required on that parent asset (in particular, the full set of de normalized attributes needed to create this asset). when implementing the method that takes a parent object, always set the relationship to the parent object explicitly and by using the trimtoreference() method on the parent object. this ensures that any guid on the parent object is used to create the reference to the parent object which ensures that any negative integer present for referential integrity is preferred over a qualifiedname for the parent object. (which further ensures that you can create both parent and child objects in the same request.) you may still want to implement a creator() that parses details from",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "the immediate parent s qualifiedname , if you know you will have control over when you must use the other (because some element of the qualifiedname itself has a in it). typically the fully parameterized creator() method (with various string parameters) will be the one you call through to from any other overloaded creator() methods, so they all share the same foundational implementation. you may also want to define a distinct method to generate the qualifiedname for the asset based on a set of defined inputs. pyatlan generator templates methods asset custom field.jinja2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "58 59 60 61 overload classmethod def creator ( cls , , name : str , custom table qualified name : str , ) customfield : ... overload classmethod def creator ( cls , , name : str , custom table qualified name : str , custom table name : str , custom dataset name : str , custom dataset qualified name : str , connection qualified name : str , ) customfield : ... classmethod init guid def creator ( cls , , name : str , custom table qualified name : str , custom table name : optional [ str ] none , custom dataset name : optional [ str ] none , custom dataset qualified name : optional [ str ] none , connection qualified name : optional [ str ] none , ) customfield : validate required fields ( [ name , custom table qualified name",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "], [ name , custom table qualified name ] ) attributes customfield . attributes . create ( name name , custom table qualified name custom table qualified name , custom table name custom table name , custom dataset name custom dataset name , custom dataset qualified name custom dataset qualified name , connection qualified name connection qualified name , ) return cls ( attributes attributes ) classmethod init guid def create ( cls , , name : str , custom table qualified name : str ) customfield : warn ( ( this method is deprecated, please use creator instead, which offers identical functionality. ), deprecationwarning , stacklevel 2 , ) return cls . creator ( name name , custom table qualified name custom table qualified name ) pyatlan generator templates methods attribute custom field.jinja2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 classmethod init guid def create ( cls , , name : str , custom table qualified name : str , custom table name : optional [ str ] none , custom dataset name : optional [ str ] none , custom dataset qualified name : optional [ str ] none , connection qualified name : optional [ str ] none , ) customfield . attributes : validate required fields ( [ name , custom table qualified name ], [ name , custom table qualified name ], ) if connection qualified name : connector name atlanconnectortype . get connector name ( connection qualified name ) else : connection qn , connector name atlanconnectortype . get connector name ( custom table qualified",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "name , custom table qualified name , 5 ) fields custom table qualified name . split ( ) qualified name f custom table qualified name name connection qualified name connection qualified name or connection qn custom dataset name custom dataset name or fields [ 3 ] custom table name custom table name or fields [ 4 ] custom dataset qualified name ( custom dataset qualified name or f connection qualified name custom dataset name ) return customfield . attributes ( name name , qualified name qualified name , custom table qualified name custom table qualified name , custom table name custom table name , custom dataset name custom dataset name , custom dataset qualified name custom dataset qualified name , connector name connector name , connection qualified name connection qualified name , custom table customtable . ref by qualified name ( custom table qualified name ), ) generate model code",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "now that the creator method has been defined for each new asset type, you can regenerate the sdk s code to include these new asset types: java python before running any generator scripts, make sure you have configured your environment variables (setting the atlan base url and atlan api key environment variables is sufficient). generate the asset model, enums, and structs in the sdk based on the typedefs present in your atlan instance: . gradlew genmodel if you see failures the generator also generates unit tests for new asset types, which will use java reflection to investigate objects like enums. if you see an error during the generation that a given type (struct or enum) is unknown, you may need to simply re run the generator. the generated files will be unformatted, so we recommend running spotless to format the code nicely: . gradlew spotlessapply before running any generator scripts,",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "make sure you have configured your environment variables ( atlan base url and atlan api key ). retrieve the typedefs from an atlan instance and write them to a json file by running the following script: python3 pyatlan generator create typedefs file.py finally, to generate the asset model, enums, and structs modules in the sdk based on the typedefs present in your atlan instance, run: python3 pyatlan generator class generator.py the generated files will be unformatted, so we recommend running pyatlan formatter to format the code nicely: . pyatlan formatter now, you are ready to import the generated asset models into your test scripts and easily manage their objects! 2025 03 11 2025 06 24 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/typedef/bind-sdks/"
  },
  {
    "text": "integration test developer skip to content write an integration test coming soon 2025 03 11 2025 03 11 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/typedef/integration-test/"
  },
  {
    "text": "test baseline ux developer skip to content test the baseline ux add to atlanhq atlan frontend once your model is rendered, you then need to add it to the atlanhq atlan frontend repository: clone atlanhq atlan frontend to your local machine (if you have not already): git clone git github.com:atlanhq atlan frontend.git (1)! cd atlan frontend this assumes you have configured your git client with appropriate credentials. if this step fails, you ll need to setup git first. start from an up to date main branch (in particular if you already have the repository cloned locally): git checkout main git merge origin main create a branch in the local repository: git branch jira task id (1)! git checkout jira task id replace jira task id with the unique id of the task in jira where you are tracking your work. move the generated front end files to the cloned repository.",
    "source": "https://developer.atlan.com/toolkits/typedef/test-ux/"
  },
  {
    "text": "move the generated typedef jsons: mv ... tmp frontend src api schemas metastore atlas entitydefs src api schemas metastore atlas entitydefs referenceable asset catalog . (1)! the target directory is the appropriate location within the front end embedded model for the rendered json. in this example, since our top level supertype was catalog , we place the files under ... referenceable asset catalog . . if you had instead directly extended asset , you would move the files under ... referenceable asset . . copy move the connection icon file: cp ... somewhere ... custom.svg (1)! src assets images source svg custom.svg the icon file itself is not part of the pkl model. you will need to copy the icon image from wherever you are managing it locally to this appropriate location in the atlan frontend repository. (optional) copy move any icon files (not necessary if you are only reusing",
    "source": "https://developer.atlan.com/toolkits/typedef/test-ux/"
  },
  {
    "text": "existing icons): ls ... tmp frontend src assets images icons (1)! cp ... somewhere ... .svg src assets images icons . the icon files themselves are not part of the pkl model. what you will see listed under the generated directory are filenames ending with placeholder indicating the names of icons that you referenced somewhere in your model. if these are new icons you want to add, you need to move them into the src assets images icons directory of the atlan frontend repository. (optional) merge icon snippets (not necessary if you are only reusing existing icons): ... tmp frontend src components common icon iconmap.ts snippet src components common icon iconmap.ts import defineasynccomponent from vue source list import snowflake from assets images source svg snowflake.svg ... copy paste start (1) import databasegray from assets images icons database gray.svg , import database from assets images icons database.svg , import tablegray from",
    "source": "https://developer.atlan.com/toolkits/typedef/test-ux/"
  },
  {
    "text": "assets images icons table gray.svg , import table from assets images icons table.svg , import columngray from assets images icons column gray.svg , import column from assets images icons column.svg , end copy paste don t remove below comment used by plop insert new icon import here import rule from assets images icons rule.svg ... export default don t remove below comment used by plop insert return here ... copy paste start databasegray , database , tablegray , table , columngray , column , end copy paste ... only copy across the highlighted lines between the comments copy paste start and end copy paste beware of duplicates note that when you are reusing existing icons, you need to be careful not to introduce any duplicates into the target iconmap.ts . move the generated type specific attributes, methods and layouts: mv ... tmp frontend src constant source (1)! src constant source",
    "source": "https://developer.atlan.com/toolkits/typedef/test-ux/"
  },
  {
    "text": ". replace with the generated directory name that matches your specific typedef model. merge index snippets: ... tmp frontend src constant source index.ts snippet src constant source index.ts copy paste start (1) import as custom from . custom end copy paste import assettypelist as atlannativeassettypes from . atlannative assettypes import assettypeinterface from types sourceconfigs assettype.interface import tags asset typenames from constant governance classification utils import autoincrementgrouporder from utils sourceconfig grouporder import getassettypes from . bi preset getassettypes an array of all sources, including sql, bi, saas, objectstore, api, elt, and eventstore. export const sourcelist [ ... object . values ( queryablesql ). map (( component ) component . default ), ... object . values ( nonqueryablesql ). map (( component ) component . default ), ... copy paste start ... object . values ( custom ). map (( component ) component . default ), end copy paste api . default",
    "source": "https://developer.atlan.com/toolkits/typedef/test-ux/"
  },
  {
    "text": ", ] ... export const supertypenameenum sql : sql , bi : bi , saas : saas , ... copy paste start custom : custom , end copy paste only copy across the highlighted lines between the comments copy paste start and end copy paste merge projection snippets: ... tmp frontend src constant projection.ts snippet src constant projection.ts import policyattributes from constant projection import calculationviewminimalattributes , calculationviewadditionalattributes , from constant source sql common attributes calculationview ... copy paste start (1) import customdatasetattributes from constant source custom attributes customdataset import customtableattributes from constant source custom attributes customtable import customfieldattributes from constant source custom attributes customfield end copy paste ... export const assetattributes [ copy paste start ... customdatasetattributes , ... customtableattributes , ... customfieldattributes , end copy paste ] only copy across the highlighted lines between the comments copy paste start and end copy paste merge usebody snippets: ... tmp frontend",
    "source": "https://developer.atlan.com/toolkits/typedef/test-ux/"
  },
  {
    "text": "src composables discovery usebody.ts snippet src composables discovery usebody.ts ... export function applyfilters ( facets , base , connectorname , state , : facets : record base : bodybuilder connectorname? : string state : ref ) const authstore useauthstore () filters object . keys ( facets ?? ) ? . foreach (( mkey ) const filterobject facets [ mkey ] switch ( mkey ) ... copy paste start (1) case customdatasetqualifiedname : case customtablequalifiedname : end copy paste case cubequalifiedname : case cubedimensionqualifiedname : case cubeparentfieldqualifiedname : case cubehierarchyqualifiedname : if ( filterobject ) base . filter ( term , mkey , filterobject ) break ... ) only copy across the highlighted lines between the comments copy paste start and end copy paste merge locale snippets: ... tmp frontend src locales en.json snippet src locales en.json dataset : dataset (1)! datasets : datasets table : table tables : tables rating",
    "source": "https://developer.atlan.com/toolkits/typedef/test-ux/"
  },
  {
    "text": ": rating ratings : ratings field : field fields : fields temperature : temperature temperatures : temperatures copy across the name value pairs. beware of duplicates note that any of your labels could already exist in the file, so you should check for duplicates. stage your new and modified ux files: git add src (1)! if you have made other changes locally that you do not want to stage, specify individual files instead of using this all encompassing stage. commit your revised ux files to the branch: git commit m feat: new ux for ... (1)! provide a meaningful message for the new ux you re adding. (this tells git to take a (local) snapshot of all the changes you staged (above).) push your committed changes to the remote repository: git push set upstream origin jira task id (1)! remember that jira task id is just a placeholder replace with",
    "source": "https://developer.atlan.com/toolkits/typedef/test-ux/"
  },
  {
    "text": "the name of your actual branch. (this tells git to push all the (local) commits you ve made against this branch to the remote github repository, so they re available to everyone there.) test ux locally prerequisites you must first install pnpm . install the latest required front end modules: pnpm install generate the latest types based on the typedef files you copied into the repository: pnpm generate:api update your local development environment tenant: .env.development 1 2 3 4 5 6 7 8 must configure tenant to allow localhost front end (1) vite client id atlan frontend vite default realm default vite default request timeout 30000 vite dev api base url https: tenant name.atlan.com vite enable events tracking false vite segment analytics key ... vite launch darkly key ... todo: extra steps for configuring the tenant to allow a localhost front end run the ui on your localhost: pnpm dev",
    "source": "https://developer.atlan.com/toolkits/typedef/test-ux/"
  },
  {
    "text": "once the command above completes, it will open your browser to http: localhost:3333 running the atlan ui with any changes you have locally against all the metadata available in the tenant you ve configured it against. create assets of the new type and test create some new instances of assets of your new type(s) and test the ux behaves as you like. 2025 03 11 2025 06 09 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google",
    "source": "https://developer.atlan.com/toolkits/typedef/test-ux/"
  },
  {
    "text": "release (ga) the typedefs developer skip to content release (ga) your typedefs ga the model only do this after first testing the model and its ux be sure to first test your model and test the baseline ux before requesting your typedef become generally available, to validate that you have everything as you want it. making changes to typedefs after the fact can be problematic or impossible in many cases. title: atlanhq models config: gitgraph: mainbranchname: master themevariables: git0: 00bad6 git1: 3c71df git2: 3c71df git3: f34d77 gitgraph lr: commit id: once upon a time branch beta branch staging checkout master commit id: now branch jira task id checkout jira task id commit id: commit typedefs checkout beta commit id: raise beta pr merge jira task id tag: test beta checkout staging commit id: raise staging pr merge jira task id tag: test staging checkout master commit id: raise master pr",
    "source": "https://developer.atlan.com/toolkits/typedef/release/"
  },
  {
    "text": "merge jira task id tag: ga once you are happy with your typedef model and want to make it generally available for all: raise a pull request (pr) from your branch ( jira task id ) to beta on atlanhq models . request someone review the pr by posting a simple message with a link to the pr on collab models . once approved and merged to beta , you ll need to do the same to both staging and master : raise a pull request (pr) from your branch ( jira task id ) to staging on atlanhq models . request someone review the pr by posting a simple message with a link to the pr on collab models . raise a pull request (pr) from your branch ( jira task id ) to master on atlanhq models . request someone review the pr by posting a simple message",
    "source": "https://developer.atlan.com/toolkits/typedef/release/"
  },
  {
    "text": "with a link to the pr on collab models . ga the ux title: atlanhq atlan frontend config: themevariables: git0: 00bad6 git1: 3c71df git2: f34d77 gitgraph lr: commit id: once upon a time branch develop checkout main commit id: now branch jira task id checkout jira task id commit id: commit code checkout develop commit id: raise develop pr merge jira task id tag: staged for release checkout main merge develop tag: ga once your typedef is generally available and you are happy with the baseline ux: raise a pull request (pr) from your branch ( jira task id ) to develop on atlanhq atlan frontend . request someone review the pr by posting a simple message with a link to the pr on team frontend . once approved and merged to develop , the front end will automatically propagate through to main over the course of 3 business days.",
    "source": "https://developer.atlan.com/toolkits/typedef/release/"
  },
  {
    "text": "(once in main it will be generally available on all tenants.) 2025 03 11 2025 03 11 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/typedef/release/"
  },
  {
    "text": "testing toolkit developer skip to content testing toolkit with the testing toolkit we can guide you to write robust, reusable integration tests for connectors and utilities in atlan. writing tests for non toolkit based scripts you can write integration tests for existing scripts in the marketplace csa scripts repository, even if they are not based on package toolkits. these tests help verify script behavior end to end in a real atlan tenant. we ll begin by performing minimal refactoring of the existing script, as it s necessary to enable writing integration tests. step 1: rename directory to snake case if the script is in kebab case directory, convert it to snake case . do this just after renaming update references in mkdocs.yml , delete the old directory, and verify imports links still work. for example: before: scripts designation based group provisioning main.py index.md tests test main.py after: scripts designation based",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "group provisioning main.py index.md tests test main.py step 2: refactor main.py do refactor the script without altering logic or flow. wrap all logic inside functions. create a single entry point: main(args: argparse.namespace) call helper functions from main() each should receive only required args or inputs . do not rename or restructure existing functions. change the sequence or logic flow. modify argument parsing. add remove logging unless required for debugging. example refactored main.py : main.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import argparse from typing import any from pyatlan.client.atlan import atlanclient from pyatlan.pkg.utils import get client , set package headers def load input file ( file path : str ) any : load and validate the input file. your file loading logic",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "here pass def process data with atlan ( client : atlanclient , data : any ) none : process the loaded data using atlan client. your data processing logic here pass def main ( args : argparse . namespace ) none : main entry point for the script. initialize atlan client client get client ( impersonate user id args . user id ) client set package headers ( client ) load and process data data load input file ( args . input file ) process data with atlan ( client , data ) if name main : parser argparse . argumentparser ( description script description ) parser . add argument ( user id , required true , help user id for impersonation ) parser . add argument ( input file , required true , help path to input file ) args parser . parse args () main ( args )",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "step 3: add integration tests prerequisites: install test dependencies before writing tests, you need to install the required testing dependencies. choose one of the following methods: option 1: install from package (recommended if available) pip install e .[test] option 2: install explicitly with requirements file create a requirements test.txt file: requirements test.txt 1 2 3 4 5 6 pytest 7.4.0 coverage 7.6.1 pytest plugins (optional but recommended) pytest order 1.3.0 pytest sugar 1.0.0 pytest timer [ termcolor ] 1.0.0 install the dependencies: pip install r requirements test.txt ready to proceed once dependencies are installed, you can proceed to write your integration tests. test layout for test main.py create a tests folder if not already present: scripts my script main.py tests test main.py function purpose test main functions test small pure helper functions individually (useful for quick validation of logic) test main run the main() function with a config to simulate",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "full script execution (end to end) test after main (optional) validate side effects after running the script, such as asset creation, retrieval, audit logs, etc. example reference: for a complete real world example, see the integration test for designation based group provisioning main.py . recommended testing strategy for scripts when writing integration tests for scripts in marketplace csa scripts , follow these practices to ensure reliable and production relevant test coverage: best practices do: test against real atlan tenants integration tests should interact with actual atlan instances to validate real behavior use environment variables for all secrets and configuration values load configuration safely via .env files, ci cd secrets, or shell configs never hardcode sensitive data mock only when necessary: use mocking or patching sparingly, and only for: external third party api calls (non atlan services) database interactions not managed by atlan non deterministic behavior (e.g., random data, time based",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "logic) avoid: mocking pyatlan clients or any atlan interactions unless absolutely necessary common pitfalls to avoid don t hardcode sensitive values never hardcode api keys, user specific secrets, or test asset names instead: use environment variables and pyatlan.test utils.testid.make unique() for unique naming best practice: generate test objects in fixtures for reusability and proper cleanup don t use fake data avoid placeholder data that doesn t reflect real atlan entity structures instead: use data that closely mirrors production for meaningful tests don t mock atlan client methods integration tests must execute real operations against live atlan tenants why: mocking undermines the purpose of integration testing and may miss regressions remember: you re testing the integration, not the individual components full example (expand for details) test main.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 import pytest from types import simplenamespace from pyatlan.pkg.utils import get client , set package headers import pandas as pd from scripts.designation based group provisioning.main import ( review groups , get default groups , get ungrouped users , map users by designation , main , ) from pyatlan.model.group import atlangroup",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": ", creategroupresponse from pyatlan.client.atlan import atlanclient from pyatlan.test utils import testid from typing import generator import os from pathlib import path test path path ( file ) . parent test group name testid . make unique ( csa dbgp test ) pytest . fixture ( scope module ) def config () simplenamespace : return simplenamespace ( user id os . environ . get ( atlan user id ), mapping file f test path test mapping.csv , missing groups handler skip , remove from default group , domain name mock tenant.atlan.com , ) pytest . fixture ( scope module ) def client ( config ): if config . user id : client get client ( impersonate user id config . user id ) else : client atlanclient () client set package headers ( client ) return client pytest . fixture ( scope module ) def group ( client : atlanclient )",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "generator [ creategroupresponse , none , none ]: to create atlangroup . create ( test group name ) g client . group . create ( group to create ) read the csv file df pd . read csv ( f test path mapping.csv ) replace values in the group name column with the test group name df [ group name ] df [ group name ] . replace ( data engineers and scientists , test group name ) save the updated test csv df . to csv ( f test path test mapping.csv , index false ) assert os . path . exists ( f test path test mapping.csv ) yield g client . group . purge ( g . group ) os . remove ( f test path test mapping.csv ) def test main functions ( config : simplenamespace , client : atlanclient , group : atlangroup , caplog",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": ": pytest . logcapturefixture , ): test configuration validation assert config . mapping file . endswith ( .csv ) test group review functionality verified groups review groups ( config . mapping file , config . missing groups handler , client ) assert caplog . records [ 0 ] . levelname info assert source information procured. in caplog . records [ 0 ] . message assert isinstance ( verified groups , set ) default groups get default groups ( client ) assert caplog . records [ 6 ] . levelname info assert default groups found: in caplog . records [ 6 ] . message assert isinstance ( default groups , list ) and len ( default groups ) 0 groupless users get ungrouped users ( default groups default groups , client client ) assert isinstance ( groupless users , list ) and len ( groupless users ) 0 unmappable users map",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "users by designation ( user list groupless users , mapping file config . mapping file , verified groups verified groups , client client , ) assert isinstance ( unmappable users , list ) and len ( unmappable users ) 0 def test main ( config : simplenamespace , client : atlanclient , group : atlangroup , caplog : pytest . logcapturefixture , ): test end to end main function execution main ( config ) verify expected log messages assert caplog . records [ 0 ] . levelname info assert sdk client initialized for tenant in caplog . records [ 0 ] . message assert input file path in caplog . records [ 1 ] . message assert source information procured. in caplog . records [ 2 ] . message assert total distinct groups in the input: in caplog . records [ 3 ] . message pytest . mark . order",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "( after test main ) def test after main ( client : atlanclient , group : creategroupresponse ): result client . group . get by name ( test group name ) assert result and len ( result ) 1 test group result [ 0 ] assert test group . path assert test group . name assert test group . id group . group assert test group . attributes assert not test group . attributes . description make sure users are successfully assigned to the test group after running the workflow assert test group . user count and test group . user count 1 writing tests for non toolkit based scripts using cursor ai code editor you can leverage ai code editors like cursor to help with refactoring existing scripts and generating integration tests for the marketplace csa scripts repository. however, it s important to be aware of the potential issues",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "and risks that may arise. step 1: setup cursor rules to ensure the ai agent provides the desired results based on your prompts, you need to set up custom rules for your code editor. create a rules file: create the file .cursor rules csa scripts tests.mdc in your project directory. you can start by copying the example rule and modifying them to match your needs. refine rules over time: as you use ai for refactoring and generating tests, you can refine the rules. by adding more context (e.g: multiple packages and varied test patterns), the ai will become more effective over time, improving its results. step 2: running the agent with the defined rules to run the ai agent with the defined rules, follow these steps: open the cursor chat: press cmd l to open a new chat in the cursor ide. click on add context , then select csa",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "scripts tests.mdc to load the rules you defined. provide a clear prompt: after loading the rules, provide a clear prompt like the following to refactor your script and add integration tests: refactor scripts asset change notification main.py using the latest cursor rules and add integration tests in scripts asset change notification tests test main.py to ensure functionality and coverage. review results: once the ai completes the task, review the generated results carefully. you may need to accept or reject parts of the refactoring based on your preferences and quality standards. common issues low accuracy across models: ai results can be highly inconsistent, even after experimenting with different combinations of rules and prompts. in many cases, only a small fraction of attempts yield satisfactory results. inconsistent output: regardless of using detailed or minimal rules, and trying various ai models ( claude 3.7, sonnet 3.5, gemini, openai ), the output often lacks",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "consistency, leading to unsatisfactory refactorings. risks in refactoring code deletion: ai can unintentionally remove important parts of the original code during refactoring. unnecessary code addition: ai might add code that changes the behavior of the script, potentially introducing bugs. flaky or insufficient tests: generated tests are often overly simplistic or unreliable. ai may also mock components that should not be mocked, leading to incomplete test coverage. mocking patching third party http interactions when do you need this? this approach is essential when building connectors or utility packages that interact with external systems, such as: fetching data from third party apis integrating with external databases calling web services that require authentication the problem with real api calls in tests challenges with direct api testing: requires credentials and environment configurations difficult to integrate into automated test suites slow execution times, especially in ci cd pipelines hard to maintain as more integrations are",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "added external service availability can break tests the solution: vcr (video cassette recorder) benefits of using vcr: record real api interactions once during development replay saved responses in tests without network calls fast, reliable, and reproducible tests works offline and in ci environments the vcrpy library captures and saves http interactions in files called cassettes during development. how vcr works the workflow: record run tests once with real api calls to record interactions save store responses in local cassette files ( yaml or json ) replay future test runs use saved responses instead of real http requests customize optionally modify saved responses to simulate different scenarios the benefits: faster tests no network latency reliable no dependency on external service availability reproducible same responses every time configurable easy to simulate edge cases and error conditions hybrid approach vcr sits between integration and unit tests it uses real api behavior but avoids",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "needing a live environment every time. this makes tests easier to maintain, faster to run, and more configurable as your project grows. write vcr based integration tests 6.0.6 for this example, we are using httpbin.org , which provides a simple and fast way to test vcrpy by recording http request and response interactions. have you installed test dependencies? before writing tests, make sure you ve installed the test dependencies in your local environment. you can do that by running the following command: pip install e .[test] alternatively, you can explicitly install the required packages by creating a requirements test.txt file and installing them using: requirements test.txt 1 2 3 4 5 6 7 8 9 10 pytest 7.4.0 coverage 7.6.1 pytest plugins (optional but recommended) pytest order 1.3.0 pytest sugar 1.0.0 pytest timer[termcolor] 1.0.0 pytest vcr 1.0.2 pinned vcrpy to v6.x since vcrpy 7.0 requires urllib3 2.0 which breaks compatibility",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "with python 3.8 vcrpy 6.0.2 python tests integration test http bin.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 import pytest import requests import os from pyatlan.test utils.base vcr import basevcr (1) class testhttpbin ( basevcr ): integration tests to demonstrate vcr.py capabilities by recording and replaying http interactions using httpbin (https: httpbin.org) for get, post, put, and delete requests. base url https: httpbin.org pytest . fixture ( scope module ) (2) def vcr config ( self",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "): override the vcr configuration to use json serialization across the module. config self . base config . copy () config . update ( serializer : pretty json ) return config pytest . fixture ( scope module ) def vcr cassette dir ( self , request ): (3) override the directory path for storing vcr cassettes. if a custom cassette directory is set in the class, it is used; otherwise, the default directory structure is created under tests cassettes . return self . cassettes dir or os . path . join ( tests vcr cassettes , request . module . name ) pytest . mark . vcr () def test httpbin get ( self ): (4) test a simple get request to httpbin. url f self . base url get response requests . get ( url , params test : value ) assert response . status code 200 assert response",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": ". json ()[ args ][ test ] value pytest . mark . vcr () def test httpbin post ( self ): test a simple post request to httpbin. url f self . base url post payload name : atlan , type : integration test response requests . post ( url , json payload ) assert response . status code 200 assert response . json ()[ json ] payload pytest . mark . vcr () def test httpbin put ( self ): test a simple put request to httpbin. url f self . base url put payload update : value response requests . put ( url , json payload ) assert response . status code 200 assert response . json ()[ json ] payload pytest . mark . vcr () def test httpbin delete ( self ): test a simple delete request to httpbin. url f self . base url delete",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "response requests . delete ( url ) assert response . status code 200 httpbin returns an empty json object for delete assert response . json ()[ args ] start by importing the basevcr class from pyatlan.test utils.base vcr , which already includes base default configurations for vcr based tests, such as vcr config , vcr cassette dir , and custom serializers like pretty yaml (default for cassettes) and pretty json (another cassette format). (optional) to override any default vcr config() , you can redefine the pytest.fixture vcr config() inside your test class. for example, you can update the serializer to use the custom pretty json serializer. (optional) to override the default cassette directory path , you can redefine the pytest.fixture vcr cassette dir() inside your test class. when writing tests (e.g test my scenario ), make sure to add the pytest.mark.vcr() decorator to mark them as vcr test cases. for",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "each test case, a separate cassette (http recording) will created inside the tests vcr cassettes directory. once you run all the tests using: pytest tests integration test http bin.py since this is the first time running them, vcrpy will record all the http interactions automatically and save them into the tests vcr cassettes directory for example, here s a saved cassette for the testhttpbin.test httpbin post test: tests vcr cassettes tests.integration.test http bin testhttpbin.test httpbin post.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 interactions : request : body : name : atlan , type : integration test headers : method : post uri : https: httpbin.org post response : body : string : args : , data :",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "name : atlan , type : integration test , files : , form : , headers : accept : , accept encoding : gzip, deflate , content length : 45 , content type : application json , host : httpbin.org , user agent : python requests 2.32.3 , x amzn trace id : root 1 680f7290 276efa7f015f83d24d9fdfc4 , json : name : atlan , type : integration test , origin : x.x.x.x , url : https: httpbin.org post headers : status : code : 200 message : ok version : 1 vcrpy not sufficient for your use case? there might be cases where vcr.py s recorded responses are not sufficient for your testing needs, even after applying custom configurations. in such scenarios, you can switch to using python s built in mock patch object library for greater flexibility and control over external dependencies. containerizing marketplace scripts overview when your script",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "is ready for production deployment, you ll need to create package specific docker images for reliable and consistent execution across different environments. why containerize? consistent execution across all environments proper versioning and rollback capability isolated dependencies prevent conflicts automated deployment via ci cd pipelines prerequisites complete these steps first before containerizing your script, ensure you have: completed script refactoring from the writing tests for non toolkit based scripts section working integration tests that validate your script functionality script directory renamed to snake case format (if applicable) required files for containerization file checklist for each package script (e.g scripts designation based group provisioning ), you need to create 5 essential files : version.txt semantic versioning dockerfile container image definition requirements.txt package dependencies requirements test.txt testing dependencies vulnerability scan (using snyk cli) let s create each file step by step: 1. version.txt semantic versioning create a version file to track your package",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "releases: version.txt 1 1.0.0dev semantic versioning guidelines you should use .dev suffix for development follow semantic versioning principles: major version: incompatible api changes minor version: backwards compatible functionality additions patch version: backwards compatible bug fixes 2. dockerfile package specific image create a production ready docker image for your script: dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 use the latest pyatlan wolfi base image from ghcr.io atlanhq pyatlan wolfi base:8.0.1 3.13 build arguments arg pkg dir arg app dir app designation based group provisioning container metadata label org.opencontainers.image.vendor atlan pte. ltd. org.opencontainers.image.source https: github.com atlanhq marketplace csa scripts org.opencontainers.image.description atlan image for designation based group provisioning custom package. org.opencontainers.image.licenses apache 2.0 switch to root for package installation user root copy and install package requirements",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "copy pkg dir requirements.txt requirements.txt install additional requirements system wide with caching run mount type cache,target root .cache uv uv pip install system r requirements.txt rm requirements.txt copy application code and utilities copy pkg dir app dir copy utils app scripts utils switch back to nonroot user for security user nonroot set working directory workdir app about pyatlan wolfi base use pyatlan wolfi base images for package scripts. the image is built on top of chainguard wolfi image with pyatlan . we use it because it is a vulnerability free open source image and this image will auto publish to ghcr on every pyatlan release (see image tag contains suffix e.g: 8.0.1 3.13 pyatlan version python version ). if you want to use a custom pyatlan wolfi base for development (with different pyatlan version , pyatlan branch or python version ) you can also do this by manually triggering the",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "gh workflow . following are the inputs for that workflow: navigate to build pyatlan wolfi base image workflow click run workflow and provide the following inputs: input description example required branch use workflow from main build type build type ( dev uses amd64 only, release uses amd64 arm64 ) dev python version python version (leave empty for 3.13 ) 3.11 pyatlan version published pyatlan version (pull from pypi, (leave empty to use version.txt ie: latest ) 7.2.0 pyatlan git branch pyatlan git branch (overrides version installs from git: github.com atlanhq atlan python.git branch ) app 1234 3. requirements.txt package dependencies generate your package dependencies using pipreqs and include required otel logging dependencies: requirements.txt 1 2 3 4 5 6 7 8 9 10 11 package specific dependencies generated via: pipreqs path to pkg force pyatlan 8.0.0 pandas 2.0.0 add your specific dependencies here... required for opentelemetry logging opentelemetry api 1.29.0",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "opentelemetry sdk 1.29.0 opentelemetry instrumentation logging 0.50b0 opentelemetry exporter otlp 1.29.0 generating requirements automatically use pipreqs to automatically detect and generate your package dependencies: bash install pipreqs if not already installed pip install pipreqs generate requirements for your package pipreqs path to your package force example for a specific script pipreqs scripts designation based group provisioning force 4. requirements test.txt testing dependencies create testing specific dependencies for ci cd and local development: requirements test.txt 1 2 3 4 5 6 7 8 9 10 minimal required for testing coverage 7.6.1 pytest 7.4.0 pytest order 1.3.0 pytest timer[termcolor] 1.0.0 pytest sugar 1.0.0 add vcr support if using http mocking pytest vcr 1.0.2 vcrpy 6.0.2 5. run snyk vulnerability scan: we also recommend running a snyk vulnerability scan on your requirements so that any issues can be fixed before doing a ga release. step by step security scanning: authenticate with snyk cli:",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "snyk auth follow the prompts to login via sso and grant app access scan project dependencies: ensure your virtual environment is active and dependencies are installed snyk test scan docker image (optional): after building your docker image locally snyk container test ghcr.io atlanhq designation based group provisioning:1.0.0dev 0d35a91 file dockerfile create exceptions policy (if needed): if there are vulnerabilities that don t impact your project, create a .snyk policy file : designation based group provisioning .snyk snyk (https: snyk.io) policy file, patches or ignores known issues. version : v1.0.0 ignores vulnerabilities until expiry date; change duration by modifying expiry date ignore : snyk:lic:pip:certifi:mpl 2.0 : : reason : mpl 2.0 license is acceptable for this project certifi is a widely used certificate bundle development workflow testing your containerized package use the build package test image workflow for rapid development and testing: steps: navigate to the workflow: go to build package",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "test image trigger the build: click run workflow and provide the required inputs: input description example required branch select your development branch from dropdown app 001 containerize dbgp package directory name of the package directory designation based group provisioning package name image name (defaults to kebab case of directory) designation based group provisioning version tag custom version tag (defaults to version.txt githash) 1.0.0 dev the workflow will build a dev image with tag format: ghcr.io atlanhq designation based group provisioning:1.0.0 dev 8799072 benefits of development testing rapid iteration test containerized changes without affecting production environment consistency same container environment as production integration validation verify your script works in containerized context production release workflow step 1: prepare for ga release before creating your pull request: update version.txt : ensure the version reflects your changes (final ga version) version.txt 1.0.0 update history.md : document all changes in this release history.md 1 2",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "3 4 5 6 7 8 9 10 11 1.0.0 (july 1, 2025) features ... bug fixes ... breaking changes ... qol improvements migrated package to build specific docker image. verify integration tests : ensure all tests pass locally pytest tests s or run tests with coverage: coverage run m pytest tests coverage report step 2: create pull request create pr with your containerization changes: include all required files ( dockerfile , version.txt , requirements.txt , etc.) add or update integration tests following the testing guidelines update documentation if needed pr validation: the automated ci pipeline will: run unit and integration tests validate docker build process check code quality and coverage verify all required files are present integration tests required if your package doesn t have integration tests, this is the perfect time to add them following the testing toolkit guidelines . the ci pipeline expects comprehensive test coverage for",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "production releases. step 3: merge and deploy review and approval : get your pr reviewed and approved merge to main : once merged, this automatically triggers: ga image build : creates production image with semantic version tag registry publication : publishes to github container registry deployment preparation : image becomes available for argo template updates final ga image : your production image will be tagged as: ghcr.io atlanhq designation based group provisioning:1.0.0 step 4: update argo templates after the ga image is built, you need to update your package s argo workflow template to use the new containerized image. this involves two main changes: remove the git repository artifact (scripts are now embedded in the docker image) update the container configuration to use the new image and module path example pr : marketplace packages pull 18043 key changes required: remove git artifact update container config remove scripts repository pull inputs:",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "artifacts: name: scripts path: tmp marketplace csa scripts git: repo: git github.com:atlanhq marketplace csa scripts insecureignorehostkey: true singlebranch: true branch: main revision: main sshprivatekeysecret: name: git ssh key: private key name: config path: tmp config ... other artifacts remain unchanged update container image and module path container: image: ghcr.io atlanhq designation based group provisioning:1.0.0 imagepullpolicy: ifnotpresent env: name: oauthlib insecure transport value: 1 ... other env vars remain unchanged workingdir: tmp marketplace csa scripts command: [ python ] args: m scripts.designation based group provisioning.main designation based group provisioning.main why these changes are needed: no more git clone : scripts are now embedded in the docker image, eliminating the need to clone the repository at runtime simplified module path : direct import from the package directory instead of the nested scripts. path cleaner execution : container starts directly in the appropriate working directory ( app ) better security : no ssh",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "keys needed for git access during workflow execution once merged, this will automatically deploy your containerized script across all atlan tenants via the atlan update workflow production deployment complete your script is now fully containerized and ready for production deployment across all atlan tenants with: consistent execution environment proper versioning and rollback capability comprehensive testing coverage automated ci cd pipeline integration best practices for containerized scripts development practices practice description version management always update version.txt before creating prs dependency pinning use specific version ranges in requirements.txt for stability comprehensive testing ensure integration tests cover containerized execution paths documentation keep history.md updated with meaningful change descriptions security considerations security area best practice base image updates regularly update your base python image for security patches dependency scanning monitor for security vulnerabilities in your dependencies secret management never hardcode secrets in docker images use environment variables image scanning enable container scanning in your",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "ci cd pipeline 2025 04 28 2025 08 26 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/toolkits/testing/"
  },
  {
    "text": "important concepts developer skip to content other important concepts type definitions type definitions (or typedefs for short) describe the properties and relationships that each different type of asset can have in atlan. type definitions are the structure for metadata in an object oriented programming sense, think of a type definition as the class itself. they describe the underlying data model of atlan. for example: the model for database tables in atlan is defined by the table typedef. the table typedef describes characteristics unique to database tables, such as column counts and row counts. the table typedef inherits from an asset typedef. (as do most other objects in atlan.) the asset typedef describes characteristics that apply to all of these objects, such as certificates and announcements. classdiagram class asset name qualifiedname certificatestatus certificatestatusmessage announcementtype announcementtitle announcementmessage ... assignedterms() class table columncount rowcount atlanschema() columns() class column datatype isnullable table() asset upstream",
    "source": "https://developer.atlan.com/concepts/review/"
  },
  {
    "text": "p downstream t1 t2 tags tags give you a way to classify and group assets in different ways, for example: by industry standard information security or sensitivity schemes (for example: pii) by department or business domain (for example: hr, finance, marketing, and so on) by key characteristic for alerting (for example: data quality issue, load failure, or similar) really any other way you want to group together your assets propagation what s special about tags? atlan can propagate tags for you automatically, to related assets: from upstream assets to downstream assets (via lineage) from parent assets to child assets (for example, from a table to all of its columns) from a term to all of its linked assets this becomes particularly powerful when using tags to represent key information you want to let your users know about. for example: tagging problematic assets if you find a problem on an asset,",
    "source": "https://developer.atlan.com/concepts/review/"
  },
  {
    "text": "you can tag that asset as having a known issue. with propagation, atlan will automatically tag all downstream (impacted) assets as having a known issue as well. even better, you can see from that propagated tag which upstream asset(s) are the source of that known issue. tagging sensitive assets you can create a glossary of terms core to your business like customer details, accounts, etc. you can assign the terms in that glossary to the data assets that hold that information. you can then tag the terms with sensitivity ratings (like pii, confidential, public, etc). with propagation, atlan will automatically tag all related data assets with those same sensitivity ratings. even better, any assets or fields derived from those assets (even if named differently) will be propagated that sensitivity rating as well. branding you can also brand tags to provide quick visual distinction: choose from a predefined list of icons",
    "source": "https://developer.atlan.com/concepts/review/"
  },
  {
    "text": "apply a color to the tag or even upload your own image to use as an icon to represent the tag access control tags can also be used to control access to assets, through purposes . when combined with propagation, you gain a very powerful, automated means to protect your most sensitive data. custom metadata custom metadata gives you a way to extend the built in types with your own attributes. structurally custom metadata is composed of: an overall name (sometimes referred to as a set ) individual attributes contained within that set (optional) restrictions on which assets can possess values for the custom metadata runtime resolution note that unlike built in types and attributes, custom metadata can only be resolved at runtime. therefore custom metadata attributes are not strongly typed in the sdks the way built in types and attributes are they must be handled a little differently. badges",
    "source": "https://developer.atlan.com/concepts/review/"
  },
  {
    "text": "what s special about custom metadata? firstly, custom metadata gives you a way to define your own attributes for assets. in addition, on top of custom metadata attributes you can create badges to callout important information on assets. branding you can also brand custom metadata to provide quick visual distinction: choose from a predefined list of emojis or even upload your own image to use as an icon to represent the custom metadata this branding will also be used for any badges you create over the custom metadata attributes. 2023 12 28 2024 02 22 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do",
    "source": "https://developer.atlan.com/concepts/review/"
  },
  {
    "text": "not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/concepts/review/"
  },
  {
    "text": "documentation conventions developer skip to content documentation conventions we use icons to quickly convey contextual details about features illustrated in code. following is the complete list of such icons and their meaning: x.y.z atlan cli version the terminal symbol in conjunction with a version number denotes when a specific feature or behavior was added to the atlan cli. make sure you re at least on this version if you want to use it the way it is illustrated in any code blocks. x.y.z java sdk version the java symbol in conjunction with a version number denotes when a specific feature or behavior was added to the java sdk. make sure you re at least on this version if you want to use it the way it is illustrated in any code blocks. also applies to kotlin since the java sdk is also used for kotlin, the same minimum version requirement",
    "source": "https://developer.atlan.com/conventions/"
  },
  {
    "text": "applies to any kotlin functionality and code blocks as well. x.y.z python sdk version the python symbol in conjunction with a version number denotes when a specific feature or behavior was added to the python sdk. make sure you re at least on this version if you want to use it the way it is illustrated in any code blocks. x.y.z go sdk version the go symbol in conjunction with a version number denotes when a specific feature or behavior was added to the go sdk. make sure you re at least on this version if you want to use it the way it is illustrated in any code blocks. experimental features some newer features are still considered experimental. these will be clearly marked as such here in developer.atlan.com , as well as in the release notes for the sdk version where they are introduced. subject to change please note",
    "source": "https://developer.atlan.com/conventions/"
  },
  {
    "text": "that experimental features are subject to change prior to their final form. such changes will not cause a change to the major version of the sdk. conversely, for any feature not marked experimental, we aim ensure no breaking changes are made to it without incrementing the major version of the sdk. 2023 10 18 2024 06 11 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/conventions/"
  },
  {
    "text": "integration options developer skip to content integration options throughout the portal you can focus on your preferred integration approach (and switch between them as you like): cli use the atlan cli to manage data contracts for assets in atlan. get started with cli dbt use dbt s meta field to enrich metadata resources straight from dbt into atlan. get started with dbt java pull our java sdk from maven central, just like any other dependency. get started with java python pull our python sdk from pypi, just like any other dependency. get started with python kotlin pull our java sdk from maven central, just like any other dependency. get started with kotlin scala pull our java sdk from maven central, just like any other dependency. get started with scala clojure pull our java sdk from maven central, just like any other dependency. get started with clojure go pull our go",
    "source": "https://developer.atlan.com/sdks/"
  },
  {
    "text": "sdk from github, just like any other dependency. get started with go events tap into events atlan produces to take immediate action, as metadata changes. get started with events raw rest api you can call directly into our rest api, though we would recommend the sdks. get started with raw rest apis 2023 04 19 2025 01 17 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/sdks/"
  },
  {
    "text": "atlan cli developer skip to content atlan cli limited functionality (so far) you can use atlan s command line interface (cli) to manage some metadata in atlan. currently data contracts and metadata for a limited set of asset types can be managed through the cli. obtain the cli 0.1.0 for now, the cli must be downloaded as a pre built binary: disclaimer closed preview this feature is in closed preview and therefore any download and installation from this link will be subject to the terms applicable to product release stages . contact your atlan customer success manager for your preview today. if your organization is already part of the closed preview, your installation of the feature from this link shall become subject to the terms and scope of preview as agreed with your organization. accordingly, any use of the feature outside the agreed scope may result in revocation of the",
    "source": "https://developer.atlan.com/sdks/cli/"
  },
  {
    "text": "closed preview for your organization. please contact your system administrator before downloading. homebrew macos (m1) macos (intel) linux windows windows recommended when installed via homebrew, you can easily keep things up to date. if you do not use it already, see homebrew s own installation documents for setting up homebrew itself . brew tap atlanhq atlan brew install atlan curl o atlan.tgz l https: github.com atlanhq atlan cli releases releases latest download atlan darwin arm64.tar.gz tar xf atlan.tgz curl o atlan.tgz l https: github.com atlanhq atlan cli releases releases latest download atlan darwin amd64.tar.gz tar xf atlan.tgz curl o atlan.tgz l https: github.com atlanhq atlan cli releases releases latest download atlan linux amd64.tar.gz tar zxf atlan.tgz curl o atlan.zip l https: github.com atlanhq atlan cli releases releases latest download atlan windows amd64.zip unzip atlan.zip configure the cli 0.1.0 you can configure the cli using a config file or in some",
    "source": "https://developer.atlan.com/sdks/cli/"
  },
  {
    "text": "cases environment variables, with the following minimum settings 1 : .atlan config.yaml 1 2 3 4 5 atlan api key : eyzid92... (1) atlan base url : https: tenant.atlan.com (2) log : enabled : false (3) level : info (4) an api token that has access to your assets. the base url of your tenant (including the https: ). (optional) enable logging to produce more details on what the cli is doing. when logging is enabled, specify the level of verbosity. environment variables 1 atlan api key eyzid92... (1) an api token that has access to your assets. define data sources you should also define data sources in the config file: .atlan config.yaml 6 7 8 9 10 11 12 data source snowflake : (1) type : snowflake (2) connection : (3) name : snowflake prod (4) qualified name : default snowflake 1234567890 (5) database : db (6) schema :",
    "source": "https://developer.atlan.com/sdks/cli/"
  },
  {
    "text": "analytics (7) each data source definition must start with data source , followed by a space and a unique reference name for the data source ( snowflake in this example). reference name is your choice the reference name you give in the configuration file is only used here and as a reference in any data contracts you define. it need not match the name of the connection or data source in atlan itself. you must indicate the type of connector for the data source (see connector types for options). details of the connection must also be provided. you must provide the name of the connection, as it appears in atlan. you must provide the unique qualified name of the connection in atlan. (optional) you can also specify the database to use for this connection s assets by default, if none is specified in the data contract. (optional) you can also",
    "source": "https://developer.atlan.com/sdks/cli/"
  },
  {
    "text": "specify the schema to use for this connection s assets by default, if none is specified in the data contract. these ensure the cli can map the details you specify in your data contract to the appropriate corresponding asset in atlan. what s next? with the cli, you can: manage data contracts upload and download files from atlan s backing object store sync metadata to a limited set of asset types integrate data contracts with ci cd processing when both are specified, environment variables will take precedence. 2024 04 25 2024 09 27 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect",
    "source": "https://developer.atlan.com/sdks/cli/"
  },
  {
    "text": "or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/sdks/cli/"
  },
  {
    "text": "dbt developer skip to content dbt atlan university see it in action in our automated enrichment course (45 mins). you can use dbt s meta field to enrich metadata resources from dbt into atlan. atlan will ingest the information from this field and update the assets in atlan accordingly. with this, you have a powerful way to keep the dbt assets documented directly as part of your dbt work. the following is an example: dbt example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 version : 2 models : name : customers description : (1) this table has basic information about a customer, as well as some derived facts based on a customer s orders. meta : (2) atlan : (3)",
    "source": "https://developer.atlan.com/sdks/dbt/"
  },
  {
    "text": "attributes : (4) certificatestatus : draft ownerusers : [ bryan , ashwin ] classifications : (5) typename : ipubxappb0zrcnu1gkjs9b propagate : true removepropagationsonentitydelete : true restrictpropagationthroughlineage : true restrictpropagationthroughhierarchy : false columns : name : customer id description : this is a unique identifier for a customer tests : unique not null name : total order amount description : total value (aud) of a customer s orders. name : customer lifetime value meta : (6) atlan : attributes : description : customer lifetime value. certificatestatus : draft ownerusers : [ ravi ] classifications : typename : ipubxappb0zrcnu1gkjs9b propagate : true removepropagationsonentitydelete : true the description at the top level of an asset defined in dbt will already be mapped to the description field for that asset in atlan. more detailed metadata, however, needs to be specified within the meta field. ... and within the meta field, further within the atlan",
    "source": "https://developer.atlan.com/sdks/dbt/"
  },
  {
    "text": "sub field. for attributes, such as certificates, announcements, or owners these need to be specified within the attributes sub field. classifications need to be specified within a classifications sub field. note that the meta field and its sub structure (including all the detailed attributes) can also be applied to columns within a model. this rich metadata will then be loaded to the corresponding attributes on the asset in atlan. for more details on specific examples, see the dbt tabs in the common asset actions snippets. 2023 12 07 2024 10 01 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store:",
    "source": "https://developer.atlan.com/sdks/dbt/"
  },
  {
    "text": "any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/sdks/dbt/"
  },
  {
    "text": "java sdk developer skip to content java sdk atlan university walk through step by step in our intro to custom integration course (30 mins). obtain the sdk the sdk is available on maven central, ready to be included in your project: gradle maven build.gradle repositories mavencentral () dependencies implementation com.atlan:atlan java: (1) testruntimeonly ch.qos.logback:logback classic:1.2.11 (2) include the latest version of the java sdk in your project as a dependency. you can also give a specific version instead of the , if you d like. the java sdk uses slf4j for logging purposes. you can include logback as a simple binding mechanism to send any logging information out to your console (standard out), at info level or above. pom.xml com.atlan atlan java atlan.version configure the sdk there are two ways to configure the sdk: using environment variables atlan api key should be given your atlan api token , for authentication",
    "source": "https://developer.atlan.com/sdks/java/"
  },
  {
    "text": "( don t forget to assign one or more personas to the api token to give access to existing assets! ) atlan base url should be given your atlan url (for example, https: tenant.atlan.com ) here s an example of setting those environment variables: set environment variables export atlan base url https: tenant.atlan.com export atlan api key ... atlanlivetest.java 1 2 3 4 5 6 7 8 9 import com.atlan.atlanclient ; public class atlanlivetest public static void main ( string [] args ) try ( atlanclient client new atlanclient ()) do something with the client on client creation if you prefer to not use environment variables, you can do the following: atlanlivetest.java 1 2 3 4 5 6 7 8 9 10 11 12 import com.atlan.atlanclient ; public class atlanlivetest public static void main ( string [] args ) try ( atlanclient client new atlanclient ( https: tenant.atlan.com , ...",
    "source": "https://developer.atlan.com/sdks/java/"
  },
  {
    "text": ") ) do something with the client careful not to expose your api token! we generally discourage including your api token directly in your code, in case you accidentally commit it into a (public) version control system. but it s your choice exactly how you manage the api token and including it for use within the client. (note that you can also explicity provide only the tenant url, and the constructor will look for only the api key through an environment variable.) that s it once these are set you can start using your sdk to make live calls against your atlan instance! what s next? delve into more detailed examples: common tasks common operations on assets, that are available across all assets. discover actions asset specific operations that are specific to certain assets. focus on a specific kind of asset governance structures operations dealing with governance structures, rather than",
    "source": "https://developer.atlan.com/sdks/java/"
  },
  {
    "text": "assets. manage governance structures samples real code samples our customers use to solve particular use cases. review live samples searching delve deep into searching and aggregating metadata. learn more about searching events delve deep into the details of the events atlan triggers. learn more about events error handling the sdk defines checked exceptions for the following categories of error: exception description apiconnectionexception errors when the sdk is unable to connect to the api, for example due to a lack of network access or timeouts. authenticationexception errors when the api token configured for the sdk is invalid or expired. conflictexception errors when there is some conflict with an existing asset and the operation cannot be completed as a result. invalidrequestexception errors when the request sent to atlan does not match its expectations. if you are using the built in methods like tocreate() and toupdate() this exception should be treated as a",
    "source": "https://developer.atlan.com/sdks/java/"
  },
  {
    "text": "bug in the sdk. (these operations take responsibility for avoiding this error.) logicexception errors where some assumption made in the sdk s code is proven incorrect. if ever raised, they should be reported as bugs against the sdk. notfoundexception errors when the requested resource or asset does not exist in atlan. permissionexception errors when the api token used by the sdk does not have permission to access a resource or carry out an operation on a specific asset . ratelimitexception errors when the atlan server is being overwhelmed by requests. a given api call could fail due to all of the errors above. so these all extend a generic atlanexception checked exception, and all api operations throw atlanexception . example for example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. so there can be a slight delay between creating the",
    "source": "https://developer.atlan.com/sdks/java/"
  },
  {
    "text": "connection and being permitted to do any operations with the connection. during that delay, any attempt to interact with the connection will result in a permissionexception , even if your api token was used to create connection in the first place. another example you may occasionally hit is some network issue that causes your connection to atlan to be interrupted. in these cases, an apiconnectionexception will be raised. don t worry, the sdk retries automatically while these are useful to know for detecting issues, the sdk automatically retries on such problems. advanced configuration atlan is a distributed, cloud native application, where network problems can arise. these advanced configuration options allow you to optimize how the sdk handles such ephemeral problems. logging the sdk uses slf4j to be logging framework agnostic. you can therefore configure your own preferred logging framework: log4j2 build.gradle dependencies implementation com.atlan:atlan java: implementation org.apache.logging.log4j:log4j core:2.22.0 (1) implementation",
    "source": "https://developer.atlan.com/sdks/java/"
  },
  {
    "text": "org.apache.logging.log4j:log4j slf4j2 impl:2.22.0 replace the ch.qos.logback:logback classic:1.2.11 logback binding with log4j2 bindings. src main resources log4j2.xml d hh:mm:ss.sss [ thread] 5level logger 36 msg n d hh:mm:ss.sss [ thread] 5level logger 36 msg n retries the sdk handles automatically retrying your requests when it detects certain problems: when an apiconnectionexception occurs that is caused by an underlying connectexception or sockettimeoutexception . when there is a 403 response indicating that permission for an operation is not (yet) available. when there is a 500 response indicating that something went wrong on the server side. more details on how they work if any request encounters one of these problems, it will be retried. before each retry, the sdk will apply a delay using: an exponential backoff (starting from 500ms) a jitter (in the range of 75 100 of the backoff delay) each retry will be at least 500ms, and at most 5s. (currently",
    "source": "https://developer.atlan.com/sdks/java/"
  },
  {
    "text": "these values are not configurable.) for each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (this is set to 3 by default.) you can configure the maximum number of retries globally using setmaxnetworkretries() on a client. set this to an integer: configure the maximum number of retries try ( atlanclient client new atlanclient ()) client . setmaxnetworkretries ( 10 ); timeouts the sdk will only wait so long for a response before assuming a network problem has occurred and the request should be timed out. by default, this is set to 80 seconds. you can configure the maximum time the sdk will wait before timing out a request using setreadtimeout() on a client. set this to an integer giving the number of milliseconds before timing out: configure the maximum time to wait before timing out try ( atlanclient client new",
    "source": "https://developer.atlan.com/sdks/java/"
  },
  {
    "text": "atlanclient ()) client . setreadtimeout ( 120 1000 ); (1)! remember this must be given in milliseconds. this example sets the timeout to 2 minutes (120 seconds 1000 milliseconds). multi tenant connectivity since version 0.9.0, the java sdk supports connecting to multiple tenants. from version 4.0.0 onwards you can create any number of clients against any number of different tenants, since every operation that interacts with a tenant now explicitly requires a client to be provided to it: create a client 1 2 3 try ( atlanclient client new atlanclient ( https: tenant.atlan.com )) (1)! client . setapitoken ( ... ); constructing a new client with a different tenant s url is sufficient to create connectivity to that other tenant. you can also (optionally) provide a second argument to directly give the api token for the tenant. use a specific client 1 2 3 4 5 6 7 8 9",
    "source": "https://developer.atlan.com/sdks/java/"
  },
  {
    "text": "10 11 12 try ( atlanclient client new atlanclient ()) glossaryterm term glossaryterm . creator ( (1) example term , 836830be 5a11 4094 8346 002e0320684f , null ) . build (); client . assets . save ( term ); (2) client . assets . save ( term , requestoptions . from ( client ). maxnetworkretries ( 10 ). build ()); (3) term . save ( client ); (4) create an object as usual. you can access the operations for assets directly on the client, under client.assets . these will generally give you the most flexibility they can handle multiple objects at a time and allow overrides. every operation on the client itself has a variant with an (optional) final argument through which you can override settings like retry limits or timeouts for this single request. you can use the from(client) factory method to initialize the request options with all the",
    "source": "https://developer.atlan.com/sdks/java/"
  },
  {
    "text": "settings of your client, and then you only need to chain on those you want to override for this particular request. alternatively, you can pass the client to the operation on the object itself. limit the number of clients to those you must have each client you create maintains its own independent copy of various caches. so the more clients you have, the more resources your code will consume. for this reason, we recommended limiting the number of clients you create to the bare minimum you require ideally just a single client per tenant. using a proxy to use the java sdk with a proxy, you need to send in some additional parameters when running any java ... command. these are described in detail in the java documentation , but are summarized here for simplicity: https socks https.proxyhost should be set to the hostname for your https proxy https.proxyport should",
    "source": "https://developer.atlan.com/sdks/java/"
  },
  {
    "text": "be set to the port for your https proxy (default being 443) run command using an https proxy 1 java dhttps.proxyhost hostname dhttps.proxyport 8080 com.atlan.samples.someclasstorun socksproxyhost should be set to the hostname for your socks proxy socksproxyport should be set to the port for your socks proxy (default being 1080) run command using a socks proxy 1 java dsocksproxyhost hostname dsocksproxyport 8080 com.atlan.samples.someclasstorun providing credentials to the proxy in either case, if you need to authenticate to your proxy, you will need to wrap whatever code you want to run to set up these credentials using something like the following: authenticate to proxy 1 2 3 4 5 6 passwordauthentication pa new passwordauthentication ( (1) username , (2) password . tochararray ()); (3) try ( atlanclient client new atlanclient ()) client . setproxycredential ( pa ); (4) you need to create a built in java passwordauthentication object. provide your username",
    "source": "https://developer.atlan.com/sdks/java/"
  },
  {
    "text": "as the first argument. ...and your password as the second argument, as a char[] . (of course, you should not hard code your password in your code itself, but rather pull it from elsewhere.) then use setproxycredential() to pass this passwordauthentication object to the atlan client, before any of the rest of the code will execute. 2023 04 19 2025 06 10 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/sdks/java/"
  },
  {
    "text": "python sdk developer skip to content python sdk atlan university walk through step by step in our intro to custom integration course (30 mins). obtain the sdk the sdk is currently available on pypi . you can use pip to install it as follows: install the sdk pip install pyatlan configure the sdk there are two ways to configure the sdk: using environment variables atlan api key should be given your atlan api token , for authentication ( don t forget to assign one or more personas to the api token to give access to existing assets! ) atlan base url should be given your atlan url (for example, https: tenant.atlan.com ) here s an example of setting those environment variables: set environment variables export atlan base url https: tenant.atlan.com export atlan api key ... atlan live test.py 1 2 3 from pyatlan.client.atlan import atlanclient client atlanclient () on client",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "creation if you prefer to not use environment variables, you can do the following: atlan live test.py 1 2 3 4 5 6 from pyatlan.client.atlan import atlanclient client atlanclient ( base url https: tenant.atlan.com , api key ... ) careful not to expose your api token! we generally discourage including your api token directly in your code, in case you accidentally commit it into a (public) version control system. but it s your choice exactly how you manage the api token and including it for use within the client. (optional) want to create a client using an api token guid? in some scenarios, you may not want to expose the entire api token or manage environment variables. instead, you can provide the guid of the api token, and the sdk will internally fetch the actual access token. when to use this approach: building apps that use the sdk where token",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "security is a concern when you want to avoid exposing full api tokens in your configuration for containerized applications that need secure token management prerequisites: before using this approach, ensure your argo template is configured with client id and client secret : argo template configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 container : image : ghcr.io atlanhq designation based group provisioning:1.0.2 imagepullpolicy : ifnotpresent env : name : client id valuefrom : secretkeyref : name : argo client creds key : login name : client secret valuefrom : secretkeyref : name : argo client creds key : password python 7.1.4 creating a client with api token guid 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from pyatlan.client.atlan import atlanclient from pyatlan.model.fluent search import fluentsearch from pyatlan.model.query import compoundquery from",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "pyatlan.model.assets import atlasglossary initialize client using api token guid token client atlanclient . from token guid ( guid c5e249d7 abcc 4ad5 87a1 831d7b810df4 (1) ) perform operations with the client (requires appropriate permissions) results ( fluentsearch () . where ( compoundquery . active assets ()) . where ( compoundquery . asset type ( atlasglossary )) . page size ( 100 ) . execute ( client token client ) ) assert results and results . count 1 print ( f found results . count glossary assets ) create client from token guid : use atlanclient.from token guid() to create a client using the guid of an api token. the sdk will automatically fetch the actual access token using the configured client id and client secret . that s it once these are set you can start using your sdk to make live calls against your atlan instance! what s next? delve",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "into more detailed examples: common tasks common operations on assets, that are available across all assets. discover actions asset specific operations that are specific to certain assets. focus on a specific kind of asset governance structures operations dealing with governance structures, rather than assets. manage governance structures samples real code samples our customers use to solve particular use cases. review live samples searching delve deep into searching and aggregating metadata. learn more about searching events delve deep into the details of the events atlan triggers. learn more about events error handling the sdk defines exceptions for the following categories of error: exception description apiconnectionerror errors when the sdk is unable to connect to the api, for example due to a lack of network access or timeouts. authenticationerror errors when the api token configured for the sdk is invalid or expired. conflicterror errors when there is some conflict with an existing",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "asset and the operation cannot be completed as a result. invalidrequesterror errors when the request sent to atlan does not match its expectations. if you are using the built in methods like tocreate() and toupdate() this exception should be treated as a bug in the sdk. (these operations take responsibility for avoiding this error.) logicerror errors where some assumption made in the sdk s code is proven incorrect. if ever raised, they should be reported as bugs against the sdk. notfounderror errors when the requested resource or asset does not exist in atlan. permissionerror errors when the api token used by the sdk does not have permission to access a resource or carry out an operation on a specific asset . ratelimiterror errors when the atlan server is being overwhelmed by requests. a given api call could fail due to all of the errors above. so these all extend a",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "generic atlanerror exception, and all api operations can potentially raise atlanerror . example for example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. so there can be a slight delay between creating the connection and being permitted to do any operations with the connection. during that delay, any attempt to interact with the connection will result in a permissionerror , even if your api token was used to create connection in the first place. another example you may occasionally hit is some network issue that causes your connection to atlan to be interrupted. in these cases, an apiconnectionerror will be raised. don t worry, the sdk retries automatically while these are useful to know for detecting issues, the sdk automatically retries on such problems. advanced configuration atlan is a distributed, cloud native application, where network problems can arise. the sdk",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "therefore automatically attempts to handle ephemeral problems. logging the sdk uses logging module of the standard library that can provide a flexible framework for emitting log messages. python you can enable logging for your sdk script by adding the following lines above your snippets: atlan python sdk test.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import logging from pyatlan.client.atlan import atlanclient from pyatlan.model.assets import atlasglossary logging . basicconfig ( level logging . debug ) (1) logging.config.fileconfig( pyatlan logging.conf ) (2) sdk code snippets client atlanclient () glossary client . asset . get by guid ( asset type atlasglossary , guid b4113341 251b 4adc 81fb 2420501c30e6 ) you can enable logging by using basicconfig with various logging levels: logging.debug : used to give detailed information, typically of interest only when diagnosing problems (mostly used level in sdk). logging.info : used to confirm that",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "things are working as expected. logging.warn : used as an indication that something unexpected happened, or as a warning of some problem in the near future. logging.error : indicates that due to a more serious problem, the sdk has not been able to perform some operation. logging.critical : indicates a serious error, suggesting that the program itself may be unable to continue running (not used in sdk as of now). by default, logs will appear in your console. if you want to use file logging, you can add the following line: logging.config.fileconfig( pyatlan logging.conf ) : this will generate logs according to the configuration defined in pyatlan logging.conf and will generate two log files: tmp pyatlan.log : default log file. tmp pyatlan.json : log file in json format. retries the sdk handles automatically retrying your requests when it detects certain problems: when there is a 403 response indicating that permission",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "for an operation is not (yet) available. when there is a 429 response indicating that the request rate limit has been exceeded, and you need to retry after some time. when there is a 50x response indicating that something went wrong on the server side. more details on how they work if any request encounters one of these problems, it will be retried. before each retry, the sdk will apply a delay using an exponential backoff. (currently the values for the exponential backoff are not configurable.) for each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (this is set to 5 by default.) timeouts by default, the sdk atlanclient() has the following timeout settings: read timeout : 900.0 seconds ( 15 minutes) connect timeout : 30.0 seconds if you need to override these defaults, you can do so as shown",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "in the example below: override sdk client default timeout 1 2 3 4 5 6 7 from pyatlan.client.atlan import atlanclient client atlanclient () timeout values in seconds client . read timeout 1800.0 30 minutes client . connect timeout 60.0 1 minute multi tenant connectivity since version 1.0.0, the python sdk supports connecting to multiple tenants.[ 1] when you use the atlanclient() method you are actually setting a default client. this default client will be used behind the scenes for any operations that need information specific to an atlan tenant. when you want to override that default client you can create a new one and use the set default client() method to change it: create a client 1 2 3 4 5 6 7 from pyatlan.client.atlan import atlanclient client2 atlanclient ( (1) base url https: tenant.atlan.com , api key ... ) atlan . set default client ( client2 ) (2) the",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "atlanclient() method will return a client for the given base url, creating a new client and setting this new client as the default client. if you want to switch between clients that you have already created, you can use atlan.set default client() to change between them. limit the number of clients to those you must have each client you create maintains its own independent copy of various caches. so the more clients you have, the more resources your code will consume. for this reason, we recommended limiting the number of clients you create to the bare minimum you require ideally just a single client per tenant. (and since in the majority of use cases you only need access to a single tenant, this means you can most likely just rely on the default client and the fallback behavior.) proxies pyatlan uses the requests library which supports proxy configuration via environment",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "variables. requests relies on the proxy configuration defined by standard environment variables http proxy, https proxy, no proxy, and all proxy. uppercase variants of these variables are also supported. you can therefore set them to configure pyatlan (only set the ones relevant to your needs): configure a proxy export http proxy http: 10.10.1.10:3128 export https proxy http: 10.10.1.10:1080 export all proxy socks5: 10.10.1.10:3434 to use http basic auth with your proxy, use the http: user:password host syntax in any of the above configuration entries: configure a proxy with authentication export https proxy http: user:pass 10.10.1.10:1080 currently, the way this is implemented limits you to either avoiding multiple threads in your python code (if you need to use multiple clients), or if you want to use multiple threads you should only use a single client. asynchronous sdk operations 8.0.0 starting from version v8.0 , it s possible to run sdk code",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "asynchronously. the async api is designed to mirror the synchronous sdk clients, maintaining familiar client patterns and caching behavior to ensure a smooth developer experience ( see release notes for complete changes ). to get started, you need to initialize an asyncatlanclient : create an async client 1 2 3 4 5 6 from pyatlan.client.aio import asyncatlanclient client asyncatlanclient ( (1) base url https: tenant.atlan.com , api key ... ) create an async client using the same configuration pattern as the synchronous client. basic search example this example demonstrates how to perform an asynchronous search for tables. the api is nearly identical to the synchronous version, with the addition of async await keywords: run an async search 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import asyncio from pyatlan.client.aio",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "import asyncatlanclient from pyatlan.model.fluent search import fluentsearch from pyatlan.model.search import term from pyatlan.model.assets import asset client asyncatlanclient ( base url https: tenant.atlan.com , api key ... ) async def search tables (): search for all active tables results await client . asset . search ( (1) criteria fluentsearch () . where ( term . with state ( active )) . where ( asset . type name . eq ( table )) . to request (), ) process results asynchronously async for table in results : (2) print ( f found table: table . name ) return results . count run the async function total count asyncio . run ( search tables ()) print ( f total tables found: total count ) async search : build search requests using the same fluentsearch pattern as the synchronous client. use await since the async client returns a coroutine object. async iteration : use",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "async for to iterate through results, as the async client returns asyncindexsearchresults which implements aiter . concurrent operations for improved performance the real power of async comes from running multiple operations concurrently . instead of waiting for each operation to complete sequentially, you can execute them in parallel and reduce total execution time: performance comparison: synchronous : total time operation operation ... operation asynchronous : total time max(operation , operation , ..., operation ) run concurrent async searches 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import asyncio from pyatlan.client.aio import asyncatlanclient from pyatlan.model.fluent search import fluentsearch from pyatlan.model.search import term from pyatlan.model.assets import asset",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "client asyncatlanclient ( base url https: tenant.atlan.com , api key ... ) async def search tables (): (1) search for all active tables results await client . asset . search ( criteria fluentsearch () . where ( term . with state ( active )) . where ( asset . type name . eq ( table )) . to request (), ) return results . count async def search columns (): (2) search for all active columns results await client . asset . search ( criteria fluentsearch () . where ( term . with state ( active )) . where ( asset . type name . eq ( column )) . to request (), ) return results . count async def concurrent search (): run table and column searches concurrently execute both searches at the same time table count , column count await asyncio . gather ( (3) search tables (),",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "search columns () ) return tables : table count , columns : column count , total assets : table count column count async def main (): main function to execute concurrent asset searches return await concurrent search () if name main : result asyncio . run ( main ()) print ( f search completed: result [ total assets ] assets found ) print ( f tables: result [ tables ] , columns: result [ columns ] ) tables search function : define an async function to search for tables and return the count. columns search function : define an async function to search for columns and return the count. concurrent execution : use asyncio.gather() to run both searches simultaneously. you can also use asyncio.as completed() if you want to process results as they become available: tasks [ search tables (), search columns ()] for coro in asyncio . as completed",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "( tasks ): result await coro print ( f operation completed with result assets ) when to use async async is most beneficial when you have: multiple independent operations that can run concurrently i o heavy workloads like api calls, database queries, or file operations long running operations where parallelization provides significant time savings for simple, single operations, the synchronous client may be more straightforward to use. 2023 06 28 2025 08 20 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with your consent, you re helping us to",
    "source": "https://developer.atlan.com/sdks/python/"
  },
  {
    "text": "kotlin sdk developer skip to content kotlin sdk obtain the sdk for kotlin, you can reuse the existing java sdk as is. it is available on maven central, ready to be included in your project: gradle build.gradle.kts repositories mavencentral () dependencies implementation ( com.atlan:atlan java: ) (1) implementation ( io.github.microutils:kotlin logging jvm:3.0.5 ) (2) implementation ( org.slf4j:slf4j simple:2.0.7 ) include the latest version of the java sdk in your project as a dependency. you can also give a specific version instead of the , if you d like. the java sdk uses slf4j for logging purposes. you can include slf4j simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin logging jvm microutil. configure the sdk there are two ways to configure the sdk: using environment variables atlan api key should be given your atlan api token , for",
    "source": "https://developer.atlan.com/sdks/kotlin/"
  },
  {
    "text": "authentication ( don t forget to assign one or more personas to the api token to give access to existing assets! ) atlan base url should be given your atlan url (for example, https: tenant.atlan.com ) here s an example of setting those environment variables: set environment variables export atlan base url https: tenant.atlan.com export atlan api key ... atlanlivetest.kt 1 2 3 4 5 6 7 import com.atlan.atlanclient fun main () atlanclient (). use client do something with the client on client creation if you prefer to not use environment variables, you can do the following: atlanlivetest.kt 1 2 3 4 5 6 7 8 9 10 import com.atlan.atlanclient fun main () atlanclient ( https: tenant.atlan.com , ... ) ) do something with the client what s next? delve into more detailed examples: common tasks common operations on assets, that are available across all assets. discover actions asset specific",
    "source": "https://developer.atlan.com/sdks/kotlin/"
  },
  {
    "text": "operations that are specific to certain assets. focus on a specific kind of asset governance structures operations dealing with governance structures, rather than assets. manage governance structures samples real code samples our customers use to solve particular use cases. review live samples searching delve deep into searching and aggregating metadata. learn more about searching events delve deep into the details of the events atlan triggers. learn more about events error handling the sdk defines checked exceptions for the following categories of error: exception description apiconnectionexception errors when the sdk is unable to connect to the api, for example due to a lack of network access or timeouts. authenticationexception errors when the api token configured for the sdk is invalid or expired. conflictexception errors when there is some conflict with an existing asset and the operation cannot be completed as a result. invalidrequestexception errors when the request sent to atlan does",
    "source": "https://developer.atlan.com/sdks/kotlin/"
  },
  {
    "text": "not match its expectations. if you are using the built in methods like tocreate() and toupdate() this exception should be treated as a bug in the sdk. (these operations take responsibility for avoiding this error.) logicexception errors where some assumption made in the sdk s code is proven incorrect. if ever raised, they should be reported as bugs against the sdk. notfoundexception errors when the requested resource or asset does not exist in atlan. permissionexception errors when the api token used by the sdk does not have permission to access a resource or carry out an operation on a specific asset . ratelimitexception errors when the atlan server is being overwhelmed by requests. a given api call could fail due to all of the errors above. so these all extend a generic atlanexception checked exception, and all api operations throw atlanexception . example for example, when creating a connection there",
    "source": "https://developer.atlan.com/sdks/kotlin/"
  },
  {
    "text": "is an asynchronous process that grants permissions to the admins of that connection. so there can be a slight delay between creating the connection and being permitted to do any operations with the connection. during that delay, any attempt to interact with the connection will result in a permissionexception , even if your api token was used to create connection in the first place. another example you may occasionally hit is some network issue that causes your connection to atlan to be interrupted. in these cases, an apiconnectionexception will be raised. don t worry, the sdk retries automatically while these are useful to know for detecting issues, the sdk automatically retries on such problems. advanced configuration atlan is a distributed, cloud native application, where network problems can arise. these advanced configuration options allow you to optimize how the sdk handles such ephemeral problems. logging the sdk uses slf4j to be",
    "source": "https://developer.atlan.com/sdks/kotlin/"
  },
  {
    "text": "logging framework agnostic. you can therefore configure your own preferred logging framework: log4j2 build.gradle.kts dependencies implementation ( com.atlan:atlan java: ) implementation ( io.github.microutils:kotlin logging jvm:3.0.5 ) implementation ( org.apache.logging.log4j:log4j core:2.22.0 ) (1) implementation ( org.apache.logging.log4j:log4j slf4j2 impl:2.22.0 ) replace the org.slf4j:slf4j simple:2.0.7 binding with log4j2 bindings. src main resources log4j2.xml d hh:mm:ss.sss [ thread] 5level logger 36 msg n d hh:mm:ss.sss [ thread] 5level logger 36 msg n retries the sdk handles automatically retrying your requests when it detects certain problems: when an apiconnectionexception occurs that is caused by an underlying connectexception or sockettimeoutexception . when there is a 403 response indicating that permission for an operation is not (yet) available. when there is a 500 response indicating that something went wrong on the server side. more details on how they work if any request encounters one of these problems, it will be retried. before each retry, the sdk will",
    "source": "https://developer.atlan.com/sdks/kotlin/"
  },
  {
    "text": "apply a delay using: an exponential backoff (starting from 500ms) a jitter (in the range of 75 100 of the backoff delay) each retry will be at least 500ms, and at most 5s. (currently these values are not configurable.) for each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (this is set to 3 by default.) you can configure the maximum number of retries globally using setmaxnetworkretries() on a client. set this to an integer: configure the maximum number of retries atlanclient (). use client client . setmaxnetworkretries ( 10 ) timeouts the sdk will only wait so long for a response before assuming a network problem has occurred and the request should be timed out. by default, this is set to 80 seconds. you can configure the maximum time the sdk will wait before timing out a request using",
    "source": "https://developer.atlan.com/sdks/kotlin/"
  },
  {
    "text": "setreadtimeout() on a client. set this to an integer giving the number of milliseconds before timing out: configure the maximum time to wait before timing out atlanclient (). use client client . setreadtimeout ( 120 1000 ) (1)! remember this must be given in milliseconds. this example sets the timeout to 2 minutes (120 seconds 1000 milliseconds). multi tenant connectivity since version 0.9.0, the java sdk supports connecting to multiple tenants. from version 4.0.0 onwards you can create any number of clients against any number of different tenants, since every operation that interacts with a tenant now explicitly requires a client to be provided to it: create a client 1 2 3 atlanclient ( https: tenant.atlan.com ). use client (1)! client . setapitoken ( ... ) constructing a new client with a different tenant s url is sufficient to create connectivity to that other tenant. you can also (optionally) provide",
    "source": "https://developer.atlan.com/sdks/kotlin/"
  },
  {
    "text": "a second argument to directly give the api token for the tenant. use a specific client 1 2 3 4 5 6 7 8 9 10 11 12 atlanclient (). use client val term glossaryterm . creator ( (1) example term , 836830be 5a11 4094 8346 002e0320684f , null ) . build () client . assets . save ( term ) (2) client . assets . save ( term , requestoptions . from ( client ). maxnetworkretries ( 10 ). build ()) (3) term . save ( client ) (4) create an object as usual. you can access the operations for assets directly on the client, under client.assets . these will generally give you the most flexibility they can handle multiple objects at a time and allow overrides. every operation on the client itself has a variant with an (optional) final argument through which you can override settings like retry",
    "source": "https://developer.atlan.com/sdks/kotlin/"
  },
  {
    "text": "limits or timeouts for this single request. you can use the from(client) factory method to initialize the request options with all the settings of your client, and then you only need to chain on those you want to override for this particular request. alternatively, you can pass the client to the operation on the object itself. limit the number of clients to those you must have each client you create maintains its own independent copy of various caches. so the more clients you have, the more resources your code will consume. for this reason, we recommended limiting the number of clients you create to the bare minimum you require ideally just a single client per tenant. using a proxy to use the java sdk with a proxy, you need to send in some additional parameters when running any java ... command. these are described in detail in the java documentation",
    "source": "https://developer.atlan.com/sdks/kotlin/"
  },
  {
    "text": ", but are summarized here for simplicity: https socks https.proxyhost should be set to the hostname for your https proxy https.proxyport should be set to the port for your https proxy (default being 443) run command using an https proxy 1 java dhttps.proxyhost hostname dhttps.proxyport 8080 com.atlan.samples.someclasstorun socksproxyhost should be set to the hostname for your socks proxy socksproxyport should be set to the port for your socks proxy (default being 1080) run command using a socks proxy 1 java dsocksproxyhost hostname dsocksproxyport 8080 com.atlan.samples.someclasstorun providing credentials to the proxy in either case, if you need to authenticate to your proxy, you will need to wrap whatever code you want to run to set up these credentials using something like the following: authenticate to proxy 1 2 3 4 5 6 val pa passwordauthentication ( (1) username , (2) password . tochararray ()) (3) atlanclient (). use client client .",
    "source": "https://developer.atlan.com/sdks/kotlin/"
  },
  {
    "text": "setproxycredential ( pa ) (4) you need to create a built in java passwordauthentication object. provide your username as the first argument. ...and your password as the second argument, as a char[] . (of course, you should not hard code your password in your code itself, but rather pull it from elsewhere.) then use setproxycredential() to pass this passwordauthentication object to the atlan client, before any of the rest of the code will execute. 2023 04 19 2025 06 10 was this page helpful? thanks for your feedback! thanks for your feedback! help us improve this page by using our feedback form to provide us with more information. back to top cookie consent we use cookies to: anonymously measure page views, and allow you to give us one click feedback on any page. we do not collect or store: any personally identifiable information. any information for any (re)marketing purposes. with",
    "source": "https://developer.atlan.com/sdks/kotlin/"
  },
  {
    "text": "your consent, you re helping us to make our documentation better google analytics accept reject manage settings",
    "source": "https://developer.atlan.com/sdks/kotlin/"
  }
]